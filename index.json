
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a researcher in mathematical optimization with interests in nonlinear and discrete problems, and an emphasis on formulations and computational approaches.\nTL;DR: I write mathematical models that represent decision-making processes or equilibria within systems, and build abstract algorithms and concrete computer programs that provide guaranteed solutions of these models.\nI joined the Inria Institute in Grenoble in January 2024 as a Tenured Associate Researcher (Chargé de Recherche) working with the Polaris group at the Grenoble Computer Science Lab. Broadly speaking, we work on decision and learning in uncertain, unknown, dynamic contexts, potentially with multiple agents. If that sounds like fun, feel free to contact me or check some example topics. I am also a guest researcher at the Zuse Institute Berlin in the IOL group, where I worked up to 2023.\nMy research interests span the theory, methods, and algorithms for several flavours of mathematical optimization. More specifically, I have been interested in exact solution methods for constrained optimization with constraint structures we can exploit. Those include solution methods, computational models, and software in mixed-integer (non-)linear and convex optimization and in particular around the SCIP framework and Frank-Wolfe related approaches. I have been exploring applications of these classes of problems in power systems, quantum information, systems biology, networks and infrastructure, and data science \u0026amp; machine learning.\nI graduated with a double PhD (cotutelle) from Polytechnique Montréal, at the GERAD lab and Centrale Lille, at INRIA \u0026amp; the Cristal lab, in mathematical optimization. My thesis focused on bilevel optimization, an extension coined near-optimality robustness, and pricing for demand response in smart grids. It was co-supervised by Luce Brotcorne (Inria) \u0026amp; Miguel F. Anjos (University of Edinburgh).\nI am involved in several open-source projects around optimization and scientific computing in the Julia programming language and around JuMP. I worked with and in various industries, from a hardware startup to steel manufacturing. I did my joint Bachelor-Master in Process Engineering at the UTC in France with a semester at the TUBS in Germany and Polytechnique Montreal.\nOn a personal note, I read both fiction (mostly history, detective, thrillers and fantasy) and non-fiction books (on economic policy, education, transportation systems, the energy transition); a more detailed readling list can be found on my goodread. I also enjoy games in various formats (tabletop, video, board, card) and cooking (from fermentation attempts to pasta recipes and coffee brewing).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1703357026,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a researcher in mathematical optimization with interests in nonlinear and discrete problems, and an emphasis on formulations and computational approaches.\nTL;DR: I write mathematical models that represent decision-making processes or equilibria within systems, and build abstract algorithms and concrete computer programs that provide guaranteed solutions of these models.","tags":null,"title":"Mathieu Besançon","type":"authors"},{"authors":null,"categories":null,"content":"This is an informal post summarizing our recent paper Probabilistic Lookahead Strong Branching via a Stochastic Abstract Branching Model together with Gioni Mexi from the Zuse Institute Berlin and Somayeh Shamsi and Pierre Le Bodic from Monash University.\nI’ll try to remain approachable but will assume that the reader is slightly familiar with Branch-and-Bound, and in general with Computational Mixed-Integer Optimization.\nTable of Contents Abstract Models for Branch-and-Bound Trees Strong Branching and Lookahead Stopping Pandora’s Multi-Variable Branching: An Abstract Branching Tree with Strong Branching Probabilistic Lookahead Improving Strong Branching in SCIP Abstract Models for Branch-and-Bound Trees One characteristic of modern frameworks for mixed-integer optimization is their complexity, in the sense of the number of moving parts in the solvers. Many algorithms run with different purposes and are influenced by each other’s result. The algorithms are exact, but their convergence to an optimal solution and proof of optimality can vary wildly from one instance to the next, and is very far from the worst-case analysis. This may seem obvious but is far from the case in many fields. In smooth convex optimization, it is more often the case that the theoretical rates are also those observed in practice.\nBecause of this gap between theoretical and observed performance, it can be hard to reason on what branch-and-cut-based solvers are doing, how different decisions in the sub-algorithms influence them.\nSome papers proposed simplified models of branch-and-bound algorithms to enable researchers to establish and compare theoretical properties, and study the influence on these simplified models of certain algorithmic decisions. Sounds vague? We will see concrete examples.\nAn abstract model for branching and its application to mixed integer programming, P. Le Bodic, G. Nemhauser (2017): defines the problem of building a branch-and-bound tree from variables defined from fixed dual gains. The model is then used to define a scoring criterion from dual gains.\nAn abstract model for branch and cut, P. le Bodic \u0026amp; A. Kazachkov (2023), extends this paper to branch-and-cut, modelling the relaxation with a set of cuts as the unique child of the previous relaxation.\nBranch-and-Bound versus Lift-and-Project relaxations in combinatorial cptimization, G. Cornuéjols, Y. Dubey (2023) compares the relaxation obtained from Branch-and-Bound against the one obtained from a lift-and-project hierarchy (lift-and-project cuts applied recursively).\nIn many cases, the goal of the article is to establish properties of the constructed simplified model, for instance to show some trends and compare them to the behaviour of real instances / solvers. In few cases, these models are used to extract key take-aways that can be exploited for actually solving hard problems. The abstract model for branching paper for instance derives from the abstract branch-and-bound trees some rules to score variables based on their left and right dual gains. Our paper sets the same goal: can we build an abstract model from which to draw actionable insight for algorithm design?\nStrong Branching and Lookahead Stopping At any node of a branch-and-bound tree, the algorithm branches on one variable that has a fractional value and should take an integer one (we will spare ourselves constraint branching and keep it simple for now). This partitions the space into two disjoint polytopes for which we continue solving the linear relaxations, branching, etc. Any choice of fractional variable at all nodes will make the algorithm terminate in finite time with the optimal solution, but this random choice typically produces an extremely large tree.\nOn the other side of the spectrum, one could produce the best tree by… searching for the best variable. This would be akin to a clairvoyant branching rule that solves the tree in order to solve the tree. Instead of fully expanding the branch-and-bound tree in this idealized branching, we could only explore the children of the nodes and use the obtained dual bound improvement as a metric to evaluate branching candidates, and this is how we obtain Strong Branching (SB). Strong branching is a limited idealized oracle, which uses a depth-one lookup in the branch-and-bound tree. Despite being “only” depth one, it is still:\nexpensive, because it requires solving two linear problems per candidate. This is much more expensive than many other branching rules, which only require a constant or linear amount of computations (in terms of problem size) per candidate. powerful in terms of predictive power. SB empirically produces very small trees, and has been shown to produce theoretically small trees in Cite Dey paper. Because of these two characteristics, SB is typically used a lot at the beginning of the tree, where branching decisions matter a lot, and then controlled with working limits on the budget of simplex iterations used for SB, on the …","date":1702771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702771200,"objectID":"2cd488ecc4b782df3e2964839b55501c","permalink":"https://matbesancon.xyz/post/2023-12-abstract-branching/","publishdate":"2023-12-17T00:00:00Z","relpermalink":"/post/2023-12-abstract-branching/","section":"post","summary":"An informal recap of our recent paper.\n","tags":["optimization","scip","integer-optimization"],"title":"Branch-And-Bound Models and Strong Branching","type":"post"},{"authors":null,"categories":null,"content":"I have been using Obsidian for about a month now and have been truly impressed with the application. After some experimentations back and forth, the hype, the control and the stationary regime, I wanted to gather some notes on my usage as a researcher in applied maths / computer science.\nI tried multiple notetaking / productivity applications before for research, including Trello, Evernote, Google Keep, and plain notes scattered around (the latter being my previous default solution, with github issues and TODOs in latex papers directly).\nFile organization Unlike some people, I tend to like folders (at a moderate depth) and not solely rely on search. My vault has roughly the following struture:\n├── _assets │ └── templates │ ├── paper_review.md │ ├── research_note.md │ ├── t_weekly.md │ └── talk_abstract.md ├── abstracts │ └── Summer_school_Einstein_Opt_ML.md ├── paper_reviews │ └── warmstart_conic.md ├── preamble.sty ├── random │ └── Spivak_notation.md ├── reading_notes │ ├── Concepts │ │ ├── Benders.md │ │ └── lift_n_project.md │ ├── Papers │ │ ├── mirror_descent_frankwolfe.md │ │ └── Rens_heuristc.md │ └── Projects │ ├── Strong_branching.md │ └── V_polyhedral_cuts.md └── weekly └── 2023-W41.md Let’s walk through the main folders:\n_assets contains the images attached to notes, PDFs, and note templates. abstracts contains my talk abstracts (I used to have them written as a one-off thing and have to scavenge my emails to gather them afterwards) paper_reviews contains the peer reviews I wrote preamble.sty I’ll mention in plugins random for notes that have no other place, out of topic for instance reading_notes is the core of my Obsidian usage, with notes related to research including: Concepts for general optimization concepts for which I want an overview note: what is Benders decomposition, etc. I also use these to group several papers on the topic while keeping a unified notation Papers are a note on a single paper Projects are running notes for ongoing research projects, including notes from meetings, diagrams, todos weekly for weekly running notes. Plugins Those I use I kept it pretty simple so far. I am using the reference map to access and reference papers quickly in notes.\nIn useful things for mathematics: I use Quick LaTeX for Obsidian and Extended MathJax with a bunch of commands in the preamble.sty file and shortcuts. Commands I include are also the same I would use in a lot of papers to be able to copy content from one to the other.\nFinally, I am using the git plugin to manage my vault as a simple github repository.\nThose I dropped In plugins I ended up removing: the calendar can be useful for some poeple but it ends up being redundant with my actual calendar app, and redundancy either creates friction, duplication, or losses.\nI also dropped the daily notes, I don’t find research to work at the scale of a day, and switched to weekly notes instead. In these, I add things to do for the current week, random small thoughts that don’t deserve their full note yet, and things I am doing to be able to look back later.\nNote-taking is a means As a final note, I would say that I spent some time setting all this up, but not an indecent amount. For anyone setting up any productivity system or app, some things should stick:\nIf you spend more time optimizing your productivity app than using it, you are probably doing it wrong The benefits of using these apps only kicks in with consistency. We got very used to immediate rewards for anything we do, and any app who offers this is probably hacking your brain into feeling satisfied Some “gurus” for these productivity apps tend to show you how to do everything in there, calendar, slides for presentations, your grocery list. Explore things that can work out, but apps are tools, and tools serve a purpose, they are not a lifestyle. Point 1 is especially vicious, it is linked to a fake productivity feeling, we all know someone who spent too much time in research organizing their literature review, their Zotero, their bullet journal system or their note-taking system. Taking notes should remain a fairly minor activity, one that we perform without thinking about it and that is there to support the actual work: developing new methods, designing and implementing the algorithms, preparing and running experiments, writing that paper that has been taking dust for months.\nStill to improve In the things that I still haven’t mastered: making internal links useful. Sure I can link notes to each other. What I don’t see yet is the usefulness of it in my research notes, probably because the number of notes where the benefit kicks in is not there yet.\nI also haven’t managed to synchronize with the git repo system with my phone, work in progress. I also rarely needed so far to access or edit my notes on mobile so far.\n","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698162499,"objectID":"a4a3d8daea05a3dab740a647d06218bf","permalink":"https://matbesancon.xyz/post/2023-10-obsidian/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/post/2023-10-obsidian/","section":"post","summary":"A one-month impression.\n","tags":["academia","productivity"],"title":"Obsidian for research","type":"post"},{"authors":null,"categories":null,"content":"In January 2024, I will be joining Inria Grenoble1 as an Associate Researcher (my home-brewed translation of Chargé de Recherche) in the POLARIS team and wanted to make a quick note about it after exchanging about several points and clearing out confusions from several people.2\nWhat is Inria, and that position? Inria is a fairly large institution in size and scope. Broadly speaking, research topics vary across domains in computer science and (mostly applied) mathematics. The institution also places an emphasis on technology transfer and impact beyond one’s research domain. Some projects you may have heard of that started at Inria include the OCaml programming language and the scikit-learn library.\nThe position As a rough projection if you are in the German system, think of this position as a W{1/2} permanent position at a Max-Planck Institute (as a researcher without teaching duties). Unlike Max-Planck, Inria has centers within university campuses with research groups that are mostly (always?) composed of permanent and temporary researchers and staff from research institutes (Inria, CNRS) and surrounding universities / grandes écoles.\nOne can make a parallel between the career steps as a researcher at Inria and CNRS and the ones at a university:\nInria University Chargé de Recherche (CR) / Associate Researcher Maître de Conférences (MCF) / Associate Professor Directeur de Recherche (DR) / Senior Researcher Professeur des Universités (PU) / Full Professor I translated DR as Senior Researcher and not Research Director, which would be the most obvious translation, because unlike what the title implies, Research Directors do not necessarily have management-heavy positions and conduct their researcher at a smaller scale. This is different from a principal investigator (PI) position involving having one’s own group (more on that in the next section).\nOne thing to note on the type of position is that this is a public servant position, with a salary range regulated at the national level. Being hired at a national level also means it can be easier for instance, to move to other Inria centers, which wouldn’t be the case with contracts tied to the local university.\nFinally, Inria opened in the last years a second type of permanent Junior position, Inria Starting Faculty Positions to recruit at the same level as CR but with different hiring, working and promotion conditions.\nThe project-team organization One specificity of Inria is its organization in project-teams (équipes-projet), a group of tenured researchers and faculty at different levels of seniority organized together towards a specific goal or set of research questions. Project-teams can vary in size, from about 4 to 15 tenured researchers and faculty (numbers from the top of my head and empirical evidence, there might be examples stretching this bound). Think of it as blending a static organization (research departments and divisions in other places) with the temporary nature of a research project. A project-team can be reconducted to continue in similar or extended research directions, but is not expected to be set in stone. Project-teams are created, terminated, merged, split as researchers within them feel the need to reorganize the way they work.\nAnother key aspects, highlighted from the beginning of the excellent Advice Booklet for Inria Applicants, is that they are not “research chairs” or “groups” from the German or US models that would be centered around one senior Principal Investigator hiring and being in charge for everyone according to their vision. Multiple members are permanent researchers, collaborate and hire their Ph.D. students, postdocs, students, or engineers, and choose to collaborate within the team in various ways.\nHow does one get that position? This question could fill a whole series of blog posts and I will not delve into too much detail. As an interesting fact to understand, the hiring process is a national competition regulated by a standardized open call with a fixed number of places per Inria center. Most applicants provide a research proposal with a heavy emphasis on its integration within one of the team-projects. I’ll refer to the Guide for Inria Applicants again for a more complete overview.\nA last point on the matter, I would consider that “just applying” to these positions when they open is nearly impossible: contact the teams beforehand, several months before the call for applications. I am highly indebted to the people from different groups who went out of their way to give me thorough reviews and critics on the application, the audition presentation and my research statements.\nComing steps in the near future: settling and building The first obvious thing on my side will be to manage to move all the random items I accumulated in Berlin to Grenoble, and then move myself there and somehow settle. I am still “amazed” at the sole quantity of books that I collected here and already filled a huge bag.\nMore interestingly, I will …","date":1697846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698162499,"objectID":"1bb3f37d257fb35a908738c9c5fa493f","permalink":"https://matbesancon.xyz/post/2023-10-crcn-inria/","publishdate":"2023-10-21T00:00:00Z","relpermalink":"/post/2023-10-crcn-inria/","section":"post","summary":"In January 2024, I will be joining Inria Grenoble1 as an Associate Researcher (my home-brewed translation of Chargé de Recherche) in the POLARIS team and wanted to make a quick note about it after exchanging about several points and clearing out confusions from several people.","tags":["career","academia"],"title":"I am joining Inria Grenoble in 2024 as a researcher","type":"post"},{"authors":["Sébastien Designolle","Gabriele Iommazzo","Mathieu Besançon","Sebastian Knebel","Patrick Gelß","Sebastian Pokutta"],"categories":[],"content":"","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"edf24183de2c9ad697ea960f9f035413","permalink":"https://matbesancon.xyz/publication/journal/phys-rev-research-5-043059/","publishdate":"2023-12-22T16:16:30.519841Z","relpermalink":"/publication/journal/phys-rev-research-5-043059/","section":"publication","summary":"","tags":[],"title":"Improved local models and new Bell inequalities via Frank-Wolfe algorithms","type":"publication"},{"authors":null,"categories":null,"content":"This post collects a partial list of winter and summer schools for 2023/2024, directly inspired by the ones compiled by Thiago Serra.\nTable of Contents Winter 2023 Summer 2023 Autumn 2023 Winter 2024 Winter 2023 Winter School of AMSI, 9th January, Melbourne, Australia Summer 2023 Spring School on Complexity, 12th June, Oléron, France Summer School on Automatic Algorithm Design 2023, 12th June, Villeneuve d’Ascq, France Summer School of NUMERICAL COMPUTATIONS: THEORY AND ALGORITHMS, 14th June, Calabria, Italy Summer School of IPCO, 19th June, Madison, Wisconsin, US Summer School of PhD Summer School on Sustainable Supply Chains, 29th June, Hagen, Germany Gene Golub SIAM Summer School on Quantum Computing and Optimization , 30th July, Lehigh, PA, US Summer School of Machine Learning for Constraint Programming 10th July, Leuven, Belgium Summer School of the Bilevel Optimization conference, 8th August, Southampton, UK Autumn 2023 Autumn School on Constrained Optimization in Machine Learning, ALOP, University of Trier Autumn School of NATCOR on Combinatorial Optimization, 11th September, University of Southampton, UK If I missed your favourite school for 2023 and first quarter 2024, open a pull request and modify this file. Thanks to Mark Turner and Joao Dionisio for pointing out some of these!\nWinter 2024 Winter School on Network Optimization, Estoril, Portugal ","date":1675036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690269871,"objectID":"623238ad00bcf8af3c1651f2fceadbde","permalink":"https://matbesancon.xyz/post/2023-01-schools23/","publishdate":"2023-01-30T00:00:00Z","relpermalink":"/post/2023-01-schools23/","section":"post","summary":"A partial list of graduate winter and summer schools in optimization.\n","tags":["optimization","phd","academia"],"title":"Graduate Winter/Summer Schools in Optimization - 2023","type":"post"},{"authors":["Mark Turner","Timo Berthold","Mathieu Besançon"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"43c40cb2b8c6c879fd2dba1f25934f67","permalink":"https://matbesancon.xyz/publication/preprint/turner-2023-contextaware/","publishdate":"2023-12-22T16:16:32.224277Z","relpermalink":"/publication/preprint/turner-2023-contextaware/","section":"publication","summary":"","tags":[],"title":"A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming","type":"publication"},{"authors":["Mark Turner","Timo Berthold","Mathieu Besançon","Thorsten Koch"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"8962286e93c071527e492f7b75847ea9","permalink":"https://matbesancon.xyz/publication/conference/turner-2023-cutting/","publishdate":"2023-12-22T16:16:31.266745Z","relpermalink":"/publication/conference/turner-2023-cutting/","section":"publication","summary":"","tags":[],"title":"Cutting plane selection with analytic centers and multiregression","type":"publication"},{"authors":["Ksenia Bestuzheva","Mathieu Besançon","Wei-Kun Chen","Antonia Chmiela","Tim Donkiewicz","Jasper van Doornmalen","Leon Eifler","Oliver Gaul","Gerald Gamrath","Ambros Gleixner"," others"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"3a324bd591754e319ef28857f1ee733e","permalink":"https://matbesancon.xyz/publication/journal/bestuzheva-2023-enabling/","publishdate":"2023-12-22T16:16:30.403592Z","relpermalink":"/publication/journal/bestuzheva-2023-enabling/","section":"publication","summary":"","tags":[],"title":"Enabling research through the SCIP optimization suite 8.0","type":"publication"},{"authors":["Akshay Sharma","Mathieu Besançon","Joaquim Dias Garcia","Benoı̂t Legat"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"b040a09955fbad87cb815e53eebcae54","permalink":"https://matbesancon.xyz/publication/journal/sharma-2022-flexible/","publishdate":"2023-12-22T16:16:30.637783Z","relpermalink":"/publication/journal/sharma-2022-flexible/","section":"publication","summary":"","tags":[],"title":"Flexible Differentiable Optimization via Model Transformations","type":"publication"},{"authors":["Gennesaret Tjusila","Mathieu Besançon","Mark Turner","Thorsten Koch"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"cc7f6abde76651d249552bcd971353eb","permalink":"https://matbesancon.xyz/publication/preprint/tjusila-2023-clues/","publishdate":"2023-12-22T16:16:31.87193Z","relpermalink":"/publication/preprint/tjusila-2023-clues/","section":"publication","summary":"","tags":[],"title":"How Many Clues To Give? A Bilevel Formulation For The Minimum Sudoku Clue Problem","type":"publication"},{"authors":["Gioni Mexi","Somayeh Shamsi","Mathieu Besançon","Pierre Le Bodic"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"37bb4bcc5de527d2c1142d80e25bd0ee","permalink":"https://matbesancon.xyz/publication/preprint/mexi-2023-probabilistic/","publishdate":"2023-12-22T16:16:32.339404Z","relpermalink":"/publication/preprint/mexi-2023-probabilistic/","section":"publication","summary":"","tags":[],"title":"Probabilistic Lookahead Strong Branching via a Stochastic Abstract Branching Model","type":"publication"},{"authors":["Gioni Mexi","Mathieu Besançon","Suresh Bolusani","Antonia Chmiela","Ambros Gleixner","Alexander Hoen"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"913925b4dd751691bd6baeb327c98171","permalink":"https://matbesancon.xyz/publication/preprint/mexi-2023-scylla/","publishdate":"2023-12-22T16:16:31.987832Z","relpermalink":"/publication/preprint/mexi-2023-scylla/","section":"publication","summary":"","tags":[],"title":"Scylla: a matrix-free fix-propagate-and-project heuristic for mixed-integer optimization","type":"publication"},{"authors":["Deborah Hendrych","Mathieu Besançon","Sebastian Pokutta"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"5e4c91ce5685f5fb76722a733c535bca","permalink":"https://matbesancon.xyz/publication/preprint/hendrych-2023-solving/","publishdate":"2023-12-22T16:16:32.454377Z","relpermalink":"/publication/preprint/hendrych-2023-solving/","section":"publication","summary":"","tags":[],"title":"Solving the Optimal Experiment Design Problem with Mixed-Integer Convex Methods","type":"publication"},{"authors":["Suresh Bolusani","Mathieu Besançon","Ambros Gleixner","Timo Berthold","Claudia D'Ambrosio","Gonzalo Muñoz","Joseph Paat","Dimitri Thomopulos"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"0ec5ddaa22b226a0cd30c27806c907a6","permalink":"https://matbesancon.xyz/publication/preprint/bolusani-2023-mip/","publishdate":"2023-12-22T16:16:32.104359Z","relpermalink":"/publication/preprint/bolusani-2023-mip/","section":"publication","summary":"","tags":[],"title":"The MIP Workshop 2023 Computational Competition on Reoptimization","type":"publication"},{"authors":null,"categories":null,"content":"This is a short post on the cut selection mechanism in the mixed-integer optimization solver SCIP and things I used for its implementation in the SCIP.jl Julia wrapper. You can check out the corresponding pull request for completeness.\nTable of Contents Callbacks? SCIP plugins Cut selection Cut selector interface Some C-Julia magic Callbacks? The space of mixed-integer optimization solvers is mostly divided between commercial, closed-source solvers and academic solvers open in source code. In the second cluster, SCIP stands out for the tunability of the solving process, like all solvers through some parameters but more importantly through callbacks.\nCallbacks are functions that are passed to a solver (or another function more generally) by the user with an expected behavior. Conceptually, they are the most elementary building block for Inversion of Control, letting the user define part of the behaviour of the solver through their own code and not only through fixed parameters.\nA basic callback system implemented in many solvers is a printing or logging callback, the user function is called at every iteration of a solving process with some iteration-specific information to print or log, here is a Julia example with gradient descent:\nfunction my_solver(x0::AbstractVector{T}, gradient_function::Function, callback::Function) x = x0 while !terminated g = gradient_function(x) stepsize = compute_stepsize(x) callback(x, g, stepsize) x = x - gamma * g terminated = ... end return x end In this example, the callback is not expected to modify the solving process but contains all the information about the current state and can record or print data.\nThe C version of it would be something like:\n#include \u0026lt;stdbool.h\u0026gt; // defining the function types typedef void (*Gradient)(double* gradient , double* x); typedef void (*Callback)(double* gradient , double* x, double stepsize); void my_solver(double* x, Gradient gradient_function, Callback callback) { double* gradient = initialize_gradient(x); double stepsize; bool terminated = false; while (!terminated) { gradient_function(gradient, x); stepsize = compute_stepsize(gradient, x); callback(x, gradient, stepsize); update_iterate(x, gradient, stepsize); terminated = ...; } } SCIP plugins SCIP plugins are generic interfaces for certain components of the solver such as cutting plane generators (also called separators), heuristics, lazy constraints. Think of plugins as a bundle of functions that have a grouped logic. Compared to callbacks, they are another level in Inversion of Control often referred to as Dependency Injection. Since C does not have a native mechanism for such a concept (think C++ abstract classes, Haskell data classes, Rust traits, Java interfaces, Scala traits), the SCIP developers just cooked up their own with macros for the sugar of an interface.\nSCIP plugins are listed on the page for how to add them.\nCut selection A cut is a linear inequality $\\alpha^T x \\leq \\beta$ such that:\nat least one optimal solution remains feasible with that cut (in general, cuts will not remove optimal solutions), a part of the feasible region of the convex relaxation is cut off (otherwise, the cut is trivial and useless). In SCIP 8, a cut selector plugin was added, see the description in the SCIP 8 release report. It was originally motivated by this paper including a subset of the SCIP 8 authors on adaptive cut selection, showing that a fixed selection rule could perform poorly.\nThere is ongoing research on cut selection at ZIB and other places, having seen that smarter rules do make a difference.\nThe selection problem can be stated as follows: given a set of previously generated cuts (some might be locally valid at the current node only), which ones should be added to the linear relaxation before continuing the branching process?\nInstinctively, a cut should be added only if it improves the current relaxation. If the current linear programming relaxation solution is not cut off by a cut, that cut is probably not relevant at the moment, even though it might cut off another part of the polytope. Example of criteria currently used to determine whether a cut should be added are:\nefficacy: how far is the current LP relaxation from the new hyperplane, sparsity: how many non-zeros coefficients does the cut have orthogonality (to other constraints), a cut that is parallel to another cut means that one of them is redundant. Instead of trying to come up with fixed metrics and a fixed rule, the cut selector allows users to define their own rule by examining all cuts and the current state of the solver.\nCut selector interface I will focus here on the Julia interface, some parts are very similar to what would be implemented by a C or C++ user, except for memory management that is done automatically here.\nThe cut selector interface is pretty simple, it consists on the Julia side of\na structure that needs to be a subtype of AbstractCutSelector, one key function that has to be implemented. The …","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664870389,"objectID":"e6f874c2422fe3895e325265dfb9301b","permalink":"https://matbesancon.xyz/post/2022-10-03-cutselection/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/post/2022-10-03-cutselection/","section":"post","summary":"This is a short post on the cut selection mechanism in the mixed-integer optimization solver SCIP and things I used for its implementation in the SCIP.jl Julia wrapper. You can check out the corresponding pull request for completeness.","tags":["julia","optimization","scip","integer-optimization"],"title":"SCIP plugins and the cut selection interface","type":"post"},{"authors":null,"categories":null,"content":"Today was the release of SCIP.jl v0.11, the first release switching to SCIP 8. The major change in this (massive) release was the rewrite of the nonlinear optimization part, using a so-called expression framework. The rewrite of the wrapper had some fairly tedious parts, debugging C shared libraries is quickly a mess with cryptic error messages. But the nonlinear rewrite gave me the opportunity to tweak the way Julia expressions are passed to SCIP in a minor way.\nTable of Contents SCIP expressions The Julia wrapper initial framework A lazified expression declaration SCIP expressions I will not go in depth into the new expression framework and will instead reference these slides but more importantly the SCIP 8 release report\nThe key part is that in a nonlinear expression, each operand is defined as an expression handler, and new ones can be introduced by users. Several specialized constraint types or constraint handlers in SCIP terminology were also removed, using the expression framework with a generic nonlinear constraint instead.\nThe Julia wrapper initial framework As a Lisp-inspired language, (some would even a Lisp dialect), Julia is a homoiconic language: valid Julia code can always be represented and stored in a primitive data structure. In this case, the tree-like structure is Expr with fields head and args:\njulia\u0026gt; expr = :(3 + 1/x) :(3 + 1 / x) julia\u0026gt; expr.head :call julia\u0026gt; expr.args 3-element Vector{Any}: :+ 3 :(1 / x) The SCIP.jl wrapper recursively destructures the Julia expression and builds up corresponding SCIP expressions, a SCIP data structure defined either as a leaf (a simple value or a variable) or as an operand and a number of subexpressions. This is done through a push_expr! function which either:\nCreates and returns a single variable expression if the expression is a variable Creates and returns a single value expression if the expression is a constant If the expression is a function f(arg1, arg2...), calls push_expr! on all arguments, and then creates and returns the SCIP expression corresponding to f. One part remains problematic, imagine an expression like 3 * exp(x) + 0.5 * f(4.3), where f is not a primitive supported by SCIP. It should not have to be indeed, because that part of the expression could be evaluated at expression compile-time. But if one is walking down the expression sub-parts, there was no way to know that a given part is a pure value, the expression-constructing procedure would first create a SCIP expression for 4.3 and then try to find a function for f to apply with this expression pointer as argument. This was the use case initially reported in this issue at a time when SCIP did not support trigonometric functions yet.\nAnother motivation for solving this issue is on the computational and memory burden. Imagine your expression is now 3 * exp(x) + 0.1 * cos(0.1) + 0.2 * cos(0.2) + ... + 100.0 * cos(100.0). This will require producing 2 * 1000 expressions for a constant, declared, allocated and passed down to SCIP. The solver will then likely preprocess all constant expressions to reduce them down, so it ends up being a lot of work done on one end to undo immediately on the other.\nA lazified expression declaration Make push_expr! return two values (scip_expr, pure_value), with the second being a Boolean for whether the expression is a pure value or not. At any leaf computing f(arg1, arg2...).\nIf the expression of all arguments are pure_value, do not compute the expression and just return a null pointer, pure_value is true for this expression.\nIf at least one of the arguments is not a pure_value, we need to compute the actual expression. None of the pure_value arguments were declared as SCIP expressions yet, we create a leaf value expression for them with Meta.eval(arg_i). The non-pure value arguments already have a correct corresponding SCIP expression pointer. pure_value is false for this expression.\nNote here that we are traversing some sub-expressions twice, once when walking down the tree and once more hidden with Meta.eval(arg_i) which computes the value for said expression, where we delegate the expression value computation to Julia. An alternative would be to return a triplet from every push_expr! call (expr_pointer, pure_value, val) and evaluate at each pure_value node the value of f(args...), with the value of the arguments already computed. This would however complexity the code in the wrapper with no advantage of the runtime, the expression evaluation is not a bottleneck for expressions that can realistically be tackled by a global optimization solver like SCIP.\n","date":1651190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664835685,"objectID":"f3da301bc4023f907c19319263bdf268","permalink":"https://matbesancon.xyz/post/2022-04-29-expression-trees/","publishdate":"2022-04-29T00:00:00Z","relpermalink":"/post/2022-04-29-expression-trees/","section":"post","summary":"Today was the release of SCIP.jl v0.11, the first release switching to SCIP 8. The major change in this (massive) release was the rewrite of the nonlinear optimization part, using a so-called expression framework.","tags":["julia","optimization","scip"],"title":"Pruning the expression tree with recursive value identification","type":"post"},{"authors":["Deborah Hendrych","Hannah Troppens","Mathieu Besançon","Sebastian Pokutta"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"fc60862dc8250648f837e8bd02ea2c34","permalink":"https://matbesancon.xyz/publication/preprint/convexmip/","publishdate":"2023-12-22T16:16:31.641714Z","relpermalink":"/publication/preprint/convexmip/","section":"publication","summary":"","tags":[],"title":"Convex mixed-integer optimization with Frank-Wolfe algorithms","type":"publication"},{"authors":["Mathieu Besançon","Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"f87f37703a4159c11e37ed3fa5c98a68","permalink":"https://matbesancon.xyz/publication/journal/besanccon-2022-frankwolfe/","publishdate":"2023-12-22T16:16:29.903678Z","relpermalink":"/publication/journal/besanccon-2022-frankwolfe/","section":"publication","summary":"","tags":[],"title":"FrankWolfe.jl: A High-Performance and Flexible Toolbox for Frank-Wolfe Algorithms and Conditional Gradients","type":"publication"},{"authors":["Jan Macdonald","Mathieu Besançon","Sebastian Pokutta"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"84664da4301903d477bb1c5dc7cba9c1","permalink":"https://matbesancon.xyz/publication/conference/macdonald-2021-interpretable/","publishdate":"2023-12-22T16:16:31.150353Z","relpermalink":"/publication/conference/macdonald-2021-interpretable/","section":"publication","summary":"","tags":[],"title":"Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings","type":"publication"},{"authors":["St Elmo Wilken","Mathieu Besançon","Miroslav Kratochvı́l","Chilperic Armel Foko Kuate","Christophe Trefois","Wei Gu","Oliver Ebenhöh"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"fe576cef6fc004c425c928425bacc7a6","permalink":"https://matbesancon.xyz/publication/journal/wilken-2022-07-11-499575/","publishdate":"2023-12-22T16:16:30.286169Z","relpermalink":"/publication/journal/wilken-2022-07-11-499575/","section":"publication","summary":"","tags":[],"title":"Interrogating the effect of enzyme kinetics on metabolism using differentiable constraint-based models","type":"publication"},{"authors":null,"categories":null,"content":"It has been about a year since I joined the Zuse Institute to work on optimization methods and computation. One of the key projects of the first half of 2021 has been on building up FrankWolfe.jl, a framework for nonlinear optimization in Julia using Frank-Wolfe methods. You can find a paper introducing the package here. This was an opportunity to experiment with different design choices for efficient, scalable, and flexible optimization tools while keeping the code simple to read and close to the algorithms.\nI will list down a few roads we went on, experimenting what reads and works best to achieve these goals.\nTable of Contents No mutability Passing containers Dedicated workspace Functors No mutability Probably the simplest pattern to follow. It is also a good one when the created objects are light, ideally stack-allocated.\nThis is typically the code you would write to get the leanest version of the Frank-Wolfe algorithm:\nx = initialize(feasible_set) while !terminated direction = grad(x) v = compute_extreme_point(feasible_set, direction) γ = find_stepsize(stepsize_strategy, x, v) x = x + γ * (v - x) terminated = check_termination(x) end This program is highly generic, users can define their own grad function, and typically implement compute_extreme_point and find_stepsize methods for their custom feasible set and step size strategy types. If you push it further, you can use a custom abstract vector type for x and v. Not a vector in the programming sense, you can use weird vector spaces as long as addition and scaling are defined.\nWhat would be a problem then? If you have seen high-performance code before, you are probably screaming at the allocations happening all over the place. Every line is allocating a new object in memory, first this direction, then the extreme point v. The worst might be the x update step which allocates three vectors because of intermediate expressions. If you come from another performance-enabling programming environment (Fortran, C, C++, Rust), what I am saying is probably obvious. If you come from interpreted languages like Python or R, you may wonder why bothering about these? If you do not need performance, indeed maybe you shouldn’t bother but when developing a library, users will probably expect not to have to rewrite your code for a larger-scale use case. Also, these interpreted languages are typically slow across the board when performing operations in the language itself and not moving them to external kernels written in a compiled language (or being lucky with Numba). In Julia, operations will typically be as fast as they can get if you pay attention to minor things, so the bottleneck quickly becomes the allocations of large objects. The other thing people may oppose is that it is the role of the compiler to take high-level expressions and reformulate them to avoid allocations. This is a common argument among some functional programming circles, everything is immutable because the compiler will figure everything out. To some extent, this is true of course but pushing too much program transformation to the compiler introduces some complexity on all users, not just the ones focusing on performance. You may typically get bitten by iterators methods (filter, map) in Rust yielding a result of a custom type which changes if a type annotation is given first. Without this type annotation, when expecting a consistent type to be inferred, one can get an error complaining about a weird type generated by the chaining of all operations. Finally, pushing this on the compiler means that you expect it to optimize your code consistently and always in the way you would do it, because in most cases ‘‘overriding’’ the compiler behaviour is far from trivial and even verifying the decisions the compiler took will require inspecting lowered emitted code (down to LLVM IR, assembly).\nFinally, worrying about performance of the inner loop is also a consequence of the nature of the algorithm itself: Frank-Wolfe, as typical for first-order methods, will perform a lot of iterations that are relatively cheap, as opposed to, say Interior Point Methods which will typically converge in few iterations but with each one of them doing significant work. In the latter case, allocating a few vectors might be fine because linear algebra will dominate runtime, but not in FW where each individual operation is relatively cheap compared to allocations.\nPassing containers This would be the typical signature of C functions, receiving almost all heap-allocated containers as arguments. A typical example would be replacing the gradient computation with:\ngrad!(storage, x) which would compute the gradient at x in-place in the storage container. Note the ! which is just a Julia idiom to indicate a function that mutates one of its arguments. Adding storage arguments to function calls is also used in Optim.jl for the definition of a gradient or Hessian or in DifferentialEquations.jl to pass the function describing a dynamic. …","date":1639180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"f3a0daa1fb7f895e477b84c5ad7a8741","permalink":"https://matbesancon.xyz/post/2021-12-11-mutability-library/","publishdate":"2021-12-11T00:00:00Z","relpermalink":"/post/2021-12-11-mutability-library/","section":"post","summary":"It has been about a year since I joined the Zuse Institute to work on optimization methods and computation. One of the key projects of the first half of 2021 has been on building up FrankWolfe.","tags":["julia","optimization","frank-wolfe"],"title":"Mutability, scope, and separation of concerns in library code","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents Position of postdocs and PhDs in labs A quick perspective on a postdoc career In January 2021, I joined the Zuse Institute as a postdoctoral researcher. After six months, most of which were in home office or remote, I wanted to put down a couple of simple observations.\nPosition of postdocs and PhDs in labs The completion of a PhD is tied to carrying out a full well-scoped research project with a high degree of autonomy. The well-scoped aspect depends on the country and domain one works in. Stopping the work on a project requires defining another one and does not go lightly. The doctoral experience is about making your work revolve around a research project for which the PhD candidate has a high degree of ownership. Collaborations are mostly about bringing other people around this project, and the research one conducts is also their status to some extend.\nA postdoc can conduct a single or multiple research projects which can be evolved, paused, or regrouped without changing one’s status. A project can also be continued while the person’s status changes (for example switching to another position). In my case, research as a postdoc is massively parallelized compared to what my PhD was (probably too much so). This implies accepting not to be the main actor on every aspect of each of these projects. It also meant playing wider on research domains and accepting not to enjoy the same level of depth, thus often asking “why does this work” or “where is this coming from” instead of silently taking notes and then spending a solid day looking up references.\nIn some sense, a PhD can look like a planet with its surrounding moons and transient asteroids. Its motion is explained in a larger system but it is in itself the center of some activity. In contrast, day-to-day work as a postdoc is akin to making your way in an N-body dynamics problem. There is still some individual focus around larger spheres, but there can be multiple centers driving motion.\nThese observations are very situational: not all postdocs have to or choose to wrestle with multiple research projects upfront. Some funding requires more or less one’s full time and attention, without the expectation to complete other projects or paper simultaneously.\nA quick perspective on a postdoc career I’ll wrap up this post on a more personal note. Pursuing a career as a postdoc is not an obvious choice, nor is it a neutral one.\nI applied for doctoral programs for the experience of the PhD in itself, not as a gate to something else like an academic career. I started without strong priors on whether I wanted to pursue a career in academia, public research, an industrial job or getting back to an entrepreneurial environment. As time went on, I realized I did enjoy not only the activity of a researcher but also the environment, or at least a part of it significant enough. This does not mean the environment is great in all aspects, it means the set of tradeoffs it implies were ones that correspond well to my expectations. This is why I would consider it as not a neutral choice.\nSome people get away with starting a PhD in places they know or have links with. This was my case as I had lived and had connections in both cities where my PhD was planned. However, it’s often not a path one can follow that easily for a postdoc (at least in STEM) as researchers are expected to move to other labs and institutions.\nPersonal relations were hard to maintain during the beginning of the pandemic but were at least equally hard to maintain regardless of one’s location. Being people’s remote relation when things are opening up is a more complex position to keep these bonds and staying on the other side of locked borders weighed more on some of my relationships than I could have anticipated. In this sense is a career path which can structurally pull you away from things and people that matter not an obvious choice.\n","date":1626220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"c40a82c8f975ff01e88655d1f686813c","permalink":"https://matbesancon.xyz/post/2021-07-14-6months-postdoc/","publishdate":"2021-07-14T00:00:00Z","relpermalink":"/post/2021-07-14-6months-postdoc/","section":"post","summary":"Table of Contents Position of postdocs and PhDs in labs A quick perspective on a postdoc career In January 2021, I joined the Zuse Institute as a postdoctoral researcher. After six months, most of which were in home office or remote, I wanted to put down a couple of simple observations.","tags":["academia","career"],"title":"6 months on the other side: life as a postdoc","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents Universities Engineering schools Classes Préparatoires IUT The Bologna reform brought some homogeneity to higher education in Europe, in particular the Bachelor-Master-Doctorate levels recognized across countries and the European Credit Transfer and Accumulation System (ECTS) defining an accounting system for academic programs, with 60 credits being the equivalent of a full-time year.\nEven though we all refer to the same level with a Bachelor’s or Master’s, the reality of programs at each level can differ within and between countries. The goal of this post is to provide some opinionated explanations on the main curricula followed in STEM in France. I will not compare it to the systems of other countries which would be trickier, just provide few pointers. In applied sciences, technology and engineering domains, many people will not have been through universities but alternative degree-awarding institutions called Grandes Écoles, an alternative system. The descriptions I give will represent overall trends, there will be institutions that do not fit well in one of the boxes and individuals within these institutions that have profiles quite different from the typical ones.\nUniversities Universities are the easiest model to understand since they fit into the Bachelor-Master-Doctorate Bologna model and work similarly to universities abroad.\nUniversity students are usually more exposed to research throughout their curriculum, being taught by professors and researchers for most of the course work from the Bachelor’s on and being trained specifically for research in many Master’s programs. A distinctive element of the French higher education program is selection at least as much as course content, with tougher selection being perceived as more prestigious.\nUniversities have a specific aspect with that regard: there is almost no selection for admission, as long as you graduated from high school. This means 1st-year cohorts are composed of lots of uncertain people here as a fallback plan or because they needed anything. This also means faculties cannot regulate the number of admissions in the 1st year and thus guarantee the quality of the curriculum. Selection only happens between the Bachelor’s years and between Bachelor’s and Master’s programs.\nThe good part of a system that is not selective for admissions is that a lot of people can sample a Bachelor program and become extremely good in their domain even though they did not seem to have a suitable profile based on their high-school record. A drawback is that 1st year experiences are usually awful because of under-staffing and the overall low cohort motivation.\nAnother issue appearing later in curricula and that was reported is that a pure university Bachelor’s and Master’s track leaves a feeling of unawareness of industry opportunities with most courses being taught by university professors who themselves have seldom worked outside of academia.\nEngineering schools These institutions (écoles d’ingénieurs) are public or private institutions delivering an engineering degree at a Master’s level. They are usually much smaller than universities, it is not shocking to hear of schools with about 300-500 students and a single building. The initial motivation (some several centuries old) for these schools was to prepare technical managers working for the state and companies. The term “ingénieur” in France is perceived mostly as a title first, and a function secondarily. People will consider themselves ingénieurs even if they do not actually work in engineering (some schools are fairly famous for training managers more than technical specialists). Engineering schools are usually well-funded, even more when looking at the budget per capita. A larger part of the curriculum is preparing industry professionals and the institutions to develop stronger partnerships with companies (for better and worse).\nEngineering schools also have more freedom over the curriculum from the start than universities. There is however one institution controlling the programs delivering a diplôme d’ingénieur called the Commission des titres d’ingénieurs (CTI). They evaluate engineering programs periodically and assert whether an institution can continue delivering the degree. The requirements are much more precise than ones for a Master’s degree, with examples such as:\na minimum level in English validated by an external examination a minimum time abroad some humanities and social sciences. The CTI has always been a topic of debate, whether it should intervene less or more in the programs, which direction and changes they should give to engineering title requirements.\nThe last distinction between engineering schools is the way they recruit: after high school or after a preparation program. Schools recruiting after high school offer a 5-year joint Bachelor-Master’s program up to the engineer title. There might be a cut-off after the Bachelor’ completion but it is expected to complete …","date":1618099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"834199c5237f29bdeeadf766d8093a26","permalink":"https://matbesancon.xyz/post/2021-04-11-french-highered/","publishdate":"2021-04-11T00:00:00Z","relpermalink":"/post/2021-04-11-french-highered/","section":"post","summary":"An opinionated guide for outsiders.\n","tags":["academia"],"title":"French Higher education in STEM","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents Plan, organize, prioritize ruthlessly Mind the change blindness on your research Thesis ownership Last December, I defended my doctoral dissertation and finished the last steps towards the completion of my double PhD. I am only writing about it now because the experience left me drained but also extremely busy with things I had been putting off for a while. Now that a quarter of 2021 has vanished before me, we get to the point where putting it off for longer means forgetting about it. ▓▓▓▓░░░░░░░░░░░ 25%\n— Year Progress (@year_progress) April 2, 2021 The major reasons for not doing this earlier are banality and messiness. Banality because many have written great pieces about different stages of the PhD process, even better when written by people from awarding institutions who see cohorts succeed and fail every year. Still, I believe there are few angles that I would have liked to see stressed more and earlier, for me or others. Messiness because several thoughts had been in the back of my mind, discussed with others, and were still looming around without much structure.\nThe end of a PhD is notoriously hard and you are mostly fighting against yourself, the excitement from the beginning is gone and any trick could be taken to smoothen this. Here are some things that worked not only well, but more than I would have expected.\nPlan, organize, prioritize ruthlessly This is a general line of advice for the whole doctoral journey, even more important for the end. The finishing process by the French institutions jointly awarding my degree was ridiculously complex with many steps involving back-and-forth with multiple documents and actors. I would have loved to illustrate this with the diagram for the procedure at my institution but that might be needlessly targeting them.\nWith such a mess, it is easy to lose focus of the scientific part and waste your time on paperwork. Remember this needs to be done but is low-priority. Do it when you are low on energy and focus, when you are tired or finish a work sprint, do not waste the valuable hours of your day on papers. Check the hard deadlines, is it in quite some time? Put it in a calendar and forget about it until you have time or no choice.\nMy point here is: no PhD is awarded for being good at filling the paperwork and it should not be on the top of your mind.\nWe were notoriously joking in the French research team about the time it took me to get properly enrolled in the first year. It could have been faster if I had spent more time and attention on it, in that case, go several times to the graduate administration offices. In retrospect, considering it low-priority and focusing on research before working the admin into my schedule was perfectly fine and let me focus on research much faster.\nTowards the end, prioritizing means constantly asking “Can I put this off until after my defence / my dissertation hand-out?”. If not, can you squeeze it in a time frame where it is not penalizing your research? If yes, put it in a calendar and forget about it. Don’t hesitate to consider this approach even for personal stuff you consider or know represents efforts. Seeing family, looking for a place for your next position, etc.\nThe last sprint is fairly stressful, being ruthless on your priorities helps not being too hard on yourself. Setting reasonable short-term objectives is a great way to make progress. One of the most productive decisions I took was accepting to go on vacation in southern France for a week with friends. I had writing hours set every day (rarely more than two) and activities the rest of the time. I probably got at least as much done as I would have had stuck in my apartment.\nMind the change blindness on your research I will refer throughout this post to the introduction, context, literature review and conclusion of the dissertation as the outer parts and the core contribution chapters as the inner parts of the thesis.\nWhile writing the outer parts of your dissertation, mind the change blindness, this perception “bug” that lets us unaware of things that changed throughout a period. In this case, you have become an expert on your research topic, despite what the imposter syndrome might be saying. This also implies that you always have a mental picture of facts and ideas related to your research that have become part of your everyday work. The reader does not have the same years of work on the research topic, even if they are in your domain. Highlight the contributions, more than you would have in an informal chat. Do not hesitate to write something down in the outer parts even if it seems obvious. For readers who have not looked at the problem, show why it is interesting. In some cultures, boasting about your results or achievements is quickly frowned upon; in the dissertation though, resist the urge to minimize them. It is not unscientific to highlight that your work is new, important, or opening new lines of research.\nThe feeling I often had …","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"0c34723acec960839e81b561be6d0ec9","permalink":"https://matbesancon.xyz/post/2021-04-02-phdone/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/post/2021-04-02-phdone/","section":"post","summary":"Random thoughts I've been willing to write since the defence.\n","tags":["academia","phd"],"title":"Bringing a PhD to the finishing line","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents Academia and the culture of implicit knowledge Back to manuscript changes Difference highlight options Going to the root: why are we encouraging inefficiency Last week finished with great news, a paper accepted with minor revisions. With this response came the review of two anonymous scientists, invited by the editor to assess the manuscript and provide feedback and suggestions.\nAside\nThe number of reviewers can fluctuate depending on your field, on the journal, on the nature of the communication (conference proceeding, short paper, article).\nMy personal “highest score” is 8 reviewers on a paper. The experience was terrible, a huge toll on everyone’s time, in my opinion showing a lack of peer-reviewing process on the editor’s side.\nWhether the required revisions are major or minor, the editor will expect a response from the authors containing:\nthe modified manuscript some sort of response with how they addressed the reviewers’ comments. Some sort of response is where differences start to appear between different disciplines and even sub-disciplines.\nAcademia and the culture of implicit knowledge We had a discussion with the co-authors on how to convey the changes, with a disagreement on which would be best. Specifically, I was asked “why don’t you use method X like everyone?”.\nWho is everyone? Are we sure that it is the case, even in our sub-field?\nThe question raises the very interesting point of implicit expectations in the academic culture. Technical know-how is transmitted informally within research groups, from one researcher to the next.\nWhat is expected in a response to reviewers? Some specific points to raise in the letter to the editor? Even one step before, what journal would be a good fit for this manuscript? What are the unsaid characteristics of that journal? There is little, if anything, to find in journals’ guides to authors, which would definitely be an appropriate place for it.\nThis one-to-one transmission creates very “localized” practices and habits because no one documents these practices but transmits them informally in informal chats or group meetings.\nWhy? First documenting beliefs is hard and unnatural in academic writing. We are used to structuring our written productions so that readers can follow the logical links. Writing in terms of gut feelings and beliefs is against those principles. Another reason is that some of this implicit knowledge is not something people would want to be recorded with their name on it.\n“That journal has an awful review process” or “this conference has proceedings of varying quality” is something people will happily tell you within a research group but not write in a public note.\nSome implicit knowledge is not that controversial but is not a scientific contribution either. Some examples are field-dependent best practices for writing and answering or any content in the sweet spot between graduate-level courses and new research: too advanced to be teachable, but not new to be publishable. In optimization, this kind of content was until recently only covered by few blogs, like Paul Rubin’s or Yet another Math Programming consultant. A new addition is the OR stackexchange Q\u0026amp;A forum and we see from the intense activity that it is covering an existing gap.\n“Let’s tear down the implicit on writing practices” has been the motivation for writing this blog post.\nBack to manuscript changes In order to take a broader view and partially remove the implicit aspect, I asked my direct circles how they present the response:\nAcademic \u0026amp; research twitter, when you send a manuscript after a round of review, you\n- just send the new manuscript\n- add an automatically generated diff (latex diff)\n- manually edit the tex source to add colors where things changed?\n— Mathieu Besançon (@matbesancon) February 9, 2021 My Twitter circle is biased towards applied maths and computer science at large and more specifically towards discrete \u0026amp; constrained optimization, applied probabilities and some other areas of computational sciences.\nI asked a similar question on a private server with PhD students from more diverse disciplines, received fewer answers but with detailed responses.\nI always assumed there is at least a written response to the editor and reviewers with a summary of the changes in the new version of the manuscript. The question is whether there is a version of the manuscript highlighting differences, and how it is produced.\nDifference highlight options I will list below several options. Some of them are in the initial options we discussed with my co-authors. Some are\nColourizing your diff The first option that was presented to me the first time I went through the process is to set a different colour for changed paragraphs so that reviewers and editors can browse through the new version and look at the new paragraphs.\nMost article editing software will have some way to do it.\n\\usepackage{color} \\RequirePackage[normalem]{ulem} \\definecolor{BLUE}{rgb}{0,0,1} …","date":1613174400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617378386,"objectID":"1dfb92e358fcb52bcb956b9881f548b1","permalink":"https://matbesancon.xyz/post/2021-2-13-diffreview/","publishdate":"2021-02-13T00:00:00Z","relpermalink":"/post/2021-2-13-diffreview/","section":"post","summary":"Make it useful for reviewers, not a hassle for authors.\n","tags":["academia","latex"],"title":"Peer review \u0026 change highlight","type":"post"},{"authors":["Mathieu Besançon","Miguel F Anjos","Luce Brotcorne"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"dbdd7a1bc0a38501096ba7866ca5cbf0","permalink":"https://matbesancon.xyz/publication/journal/besanccon-2021-complexity/","publishdate":"2023-12-22T16:16:30.162945Z","relpermalink":"/publication/journal/besanccon-2021-complexity/","section":"publication","summary":"","tags":[],"title":"Complexity of near-optimal robust versions of multilevel optimization problems","type":"publication"},{"authors":["Mathieu Besançon","Theodore Papamarkou","David Anthoff","Alex Arslan","Simon Byrne","Dahua Lin","John Pearson"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"02fc43203086c2eb1a28fcfc090f8283","permalink":"https://matbesancon.xyz/publication/journal/js-sv-098-i-16/","publishdate":"2023-12-22T16:16:29.507097Z","relpermalink":"/publication/journal/js-sv-098-i-16/","section":"publication","summary":"","tags":[],"title":"Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem","type":"publication"},{"authors":["Alejandro Carderera","Mathieu Besançon","Sebastian Pokutta"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"033b30195af160f3376bc110d7ccfd59","permalink":"https://matbesancon.xyz/publication/conference/carderera-2021-simple/","publishdate":"2023-12-22T16:16:31.03462Z","relpermalink":"/publication/conference/carderera-2021-simple/","section":"publication","summary":"","tags":[],"title":"Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions","type":"publication"},{"authors":["Ksenia Bestuzheva","Mathieu Besançon","Wei-Kun Chen","Antonia Chmiela","Tim Donkiewicz","Jasper van Doornmalen","Leon Eifler","Oliver Gaul","Gerald Gamrath","Ambros Gleixner"," others"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"fd3f997ddcc89dafc048da9f225c43e3","permalink":"https://matbesancon.xyz/publication/preprint/scipreport/","publishdate":"2023-12-22T16:16:31.757319Z","relpermalink":"/publication/preprint/scipreport/","section":"publication","summary":"","tags":[],"title":"The SCIP Optimization Suite 8.0","type":"publication"},{"authors":null,"categories":null,"content":"In a previous post, I detailed some of the features of MathOptSetDistances.jl and the evolution of the idea behind it. This is part II focusing on derivatives.\nTable of Contents Meet ChainRules.jl Projection derivative Example on the nonnegative orthant Forward rule Reverse rules Conclusion Notes The most interesting part of the packages is the projection onto a set. For some applications, what we need is not only the projection but also the derivative of this projection.\nOne answer here would be to let Automatic Differentiation (AD) do the work. However:\nJust like there are closed-form expressions for the projection, many sets admit closed-form projection derivatives that can be computed cheaply, Some projections may require to perform steps impossible or expensive with AD, as a root-finding procedure1 or an eigendecomposition2; Some functions might make calls into deeper water. JuMP for instance supports a lot of optimization solvers implemented in C and called as shared libraries. AD will not propagate through these calls. For these reasons, AD systems often let users implement some derivatives themselves, but as a library developer, I do not want to depend on a full AD package (and force downstream users to do so).\nMeet ChainRules.jl ChainRules.jl is a Julia package addressing exactly the issue mentioned above: it defines a set of primitives to talk about derivatives in Julia. Library developers can implement custom derivatives for their own functions and types. Finally, AD library developers can leverage ChainRules.jl to obtain derivatives from functions when available, and otherwise use AD mechanisms to obtain them from more elementary functions.\nThe logic and motivation is explained in more details in Frame’s talk at JuliaCon 2020 and the package documentation which is very instructive on AD in general.\nProjection derivative We are interested in computing $D\\Pi_{\\mathcal{S}}(v)$, the derivative of the projection with respect to the initial point. As a refresher, if $\\Pi_s(\\cdot)$ is a function from $V$ onto itself, and if $V$ then the derivative $D\\Pi$ maps a point in $V$ onto a linear map from the tangent space of $V$ onto itself. The tangent space of $V$ is roughly speaking the space where differences of values in $V$ live. If $V$ corresponds to real numbers, then the tangent space will also be real numbers, but if $V$ is a space of time/dates, then the tangent space is a duration/time period. See here3 for more references. Again, roughly speaking, this linear map takes perturbations of the input $\\Delta v$ and maps them to perturbation of the projected point $\\Delta v_p$.\nAs an example warm-up:\n$S$ is the whole domain of $v$ $\\Rightarrow$ the projection is $v$ itself, $D\\Pi_{\\mathcal{S}}(v)$ is the identity operator. $S$ is $\\{0\\}^n$ $\\Rightarrow$ the projection is always $\\{0\\}^n$, $D\\Pi_{\\mathcal{S}}(v)$ maps every $Δv$ to a zero vector: perturbations in the input do not change the output. $D\\Pi_{\\mathcal{S}}(v)$ is a linear map from $\\mathcal{V}$ to $\\mathcal{V}$. If $v \\in \\mathbb{R}^n$, it can be represented as a $n\\times n$ matrix. There are several ways of representing linear maps, see the LinearOperators.jl package for some insight. Two approaches (for now) are implemented for set distances:\nMatrix approach: given $v \\in \\mathbb{R}^n$, return the linear operator as an $n\\times n$ matrix. Forward mode: given $v$ and a direction $\\Delta v$, provide the directional derivative $D\\Pi_{\\mathcal{S}}(v) \\Delta v$. Reverse mode: given $v$, provide a closure corresponding to the adjoint of the derivative. (1) has been implemented by Akshay for many sets during his GSoC this summer, along with the projections themselves.\n(1) corresponds to computing the derivative eagerly as a full matrix, thus paying storage and computation cost upfront. The advantage is the simplicity for standard vectors, take v, s, build and return the matrix. (2) is the building block for forward-mode differentiation: given a point $v$ and an input perturbation $\\Delta v$, compute the output perturbation. (3) corresponds to a building block for reverse-mode differentiation. An aspect of the matrix approach is that it works well for 1-D arrays but gets complex quite quickly for other structures, including multi-argument functions or matrices. Concatenating everything into a vector is too rigid.\nExample on the nonnegative orthant The nonnegative orthant cone is the set $\\mathbb{R}^n_+$; it is represented in MOI as MOI.Nonnegatives(n) with n the dimension. The projection is simple because it can be done elementwise: $$ (\\Pi_S(v))_i = max(v_i, 0) \\,\\,\\forall i. $$\nIn other terms, any non-diagonal term of the gradient matrix is 0 for any $v$. Here is a visualization made with haste for $n=2$ using the very promising Javis.jl:\nThe red circle is a vector in the plane and the blue square its projection.4\nThe Julia implementation follows the same idea, here in a simplified version:\nfunction projection_on_set(v::AbstractVector{T}, …","date":1608768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651250551,"objectID":"af1fca7a2fd7b549324bff047bfbad7d","permalink":"https://matbesancon.xyz/post/2020-12-24-chains_sets2/","publishdate":"2020-12-24T00:00:00Z","relpermalink":"/post/2020-12-24-chains_sets2/","section":"post","summary":"Differentiating set projections.\n","tags":["julia","optimization","jump","automatic-differentiation"],"title":"Sets, chains and rules - part II","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents MathOptInterface and the motivation Examples Set projections User-defined distance notions Bonus In this post, I will develop the process through which the MathOptSetDistances.jl package has been created and evolved. In the second one, I will go over the differentiation part.\nMathOptInterface and the motivation MathOptInterface.jl or MOI for short is a Julia package to unify structured constrained optimization problems. The abstract representation of problems MOI addresses is as follows:\n$$ \\begin{align} \\min_{x}\\,\\, \u0026amp; F(x) \\\\\\\\ \\text{s.t.}\\,\\, \u0026amp; G_k(x) \\in \\mathcal{S}_k \\,\\, \\forall k \\\\\\\\ \u0026amp; x \\in \\mathcal{X}. \\end{align} $$\n$\\mathcal{X}$ is the domain of the decision variables, $F$ is the objective function, mapping values of the variables to the real line. The constrained aspect comes from the constraints $G_k(x) \\in \\mathcal{S}_k$, some mappings of the variables $G_k$ have to belong to a certain set $\\mathcal{S}_k$. See this recent paper on MOI for more information on this representation.\nThe structured aspect comes from the fact that a specific form of $F$, $G$ and $\\mathcal{S}$ is known in advance by the modeller. In other words, MOI does not deal with arbitrary unknown functions or black-box sets. For such cases, other tools are more adapted.\nFrom a given problem in this representation, two operations can be of interest within a solution algorithm or from a user perspective:\nGiven a value for $x$, evaluating a function $F(x)$ or $G(x)$, Given a value $v$ in the co-domain of $G_k$, asserting whether $v \\in S_k$. The first point is addressed by the function eval_variables in the MOI.Utilities submodule (documentation).\nThe second point appears as simple (or at least it did to me) but is trickier. What tolerance should be set? Most solvers include a numerical tolerance on constraint violations, should this be propagated from user choices, and how?\nThe deceivingly simple feature ended up opening one of the longest discussions in the MOI repository.\nFairly straightforward[…]\nOptimistic me, beginning of the PR, February 2020\nA more meaningful query for solvers is, given a value $v$, what is the distance from $v$ to the set $\\mathcal{S}$:\n$$ \\begin{align} (\\text{δ(v, s)})\\,\\,\\min_{v_p}\\,\\, \u0026amp; \\text{dist}(v_p, v) \\\\\\\\ \\text{s.t.}\\,\\, \u0026amp; v_p \\in \\mathcal{S}. \\end{align} $$\nThe optimal value of the problem above noted $δ(v, s)$ depends on the notion of the distance taken between two values in the domain $\\mathcal{V}$, noted $dist(\\cdot,\\cdot)$ here. In terms of implementation, the signature is roughly:\ndistance_to_set(v::V, s::S) -\u0026gt; Real Aside: this is an example where multiple dispatch brings great value to the design: the implementation of distance_to_set depends on both the value type V and the type of set S. See why it’s useful in the Bonus section.\nIf $\\mathcal{S}$ was a generic set, computing this distance would be as hard as solving an optimization problem with constraints $v \\in \\mathcal{S}$ but since we are dealing with structured optimization, many particular sets have closed-form solutions for the problem above.\nExamples $\\|\\cdot\\|$ will denote the $l_2-$norm if not specified.\nThe distance computation problem defined by the following data:\n$$ \\begin{align} \u0026amp; v \\in \\mathcal{V} = \\mathbb{R}^n,\\\\ \u0026amp; \\mathcal{S} = \\mathbb{Z}^n,\\\\ \u0026amp; dist(a, b) = \\|a - b\\| \\end{align} $$\nconsists of rounding element-wise to the closest integer.\nThe following data:\n$$ \\begin{align} \u0026amp; v \\in \\mathcal{V} = \\mathbb{R}^n,\\\\ \u0026amp; \\mathcal{S} = \\mathbb{R}^n_+,\\\\ \u0026amp; dist(a, b) = \\|a - b\\| \\end{align} $$\nfind the closest point in the positive orthant, with a result:\n$$ v_{p}\\left[i\\right] = \\text{max}(v\\left[i\\right], 0) \\,\\, \\forall i \\in \\{1..n\\}. $$\nSet projections The distance from a point to a set tells us how far a given candidate is from respecting a constraint. But for many algorithms, the quantity of interest is the projection itself:\n$$ \\Pi_{\\mathcal{S}}(v) \\equiv \\text{arg} \\min_{v_p \\in \\mathcal{S}} \\text{dist}(v, v_p). $$\nLike the optimal distance, the best projection onto a set can often be defined in closed form i.e. without using generic optimization methods.\nWe also keep the convention that the projection of a point already in the set is always itself: $$ δ(v, \\mathcal{S}) = 0 \\,\\, \\Leftrightarrow \\,\\, v \\in \\mathcal{S} \\,\\, \\Leftrightarrow \\,\\, \\Pi_{\\mathcal{S}}(v) = v. $$\nThe interesting thing about projections is that once obtained, a distance can be computed easily, although only computing the distance can be slightly more efficient, since we do not need to allocate the projected point.\nUser-defined distance notions Imagine a set defined using two functions: $$ \\begin{align} \\mathcal{S} = \\{v \\in \\mathcal{V}\\,|\\, f(v) \\leq 0, g(v)\\leq 0 \\}. \\end{align} $$\nThe distance must be evaluated with respect to two values: $$ (max(f(v), 0), max(g(v), 0)). $$\nHere, the choice boils down to a norm, but hard-coding it seems harsh and rigid for users. Even if we plan correctly and add most norms people would …","date":1608681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608892714,"objectID":"e50e14e89d79888840e9eec101bbf986","permalink":"https://matbesancon.xyz/post/2020-12-23-chains_sets/","publishdate":"2020-12-23T00:00:00Z","relpermalink":"/post/2020-12-23-chains_sets/","section":"post","summary":"The Pandora box from simple set membership.\n","tags":["julia","optimization","jump","automatic-differentiation"],"title":"Sets, chains and rules - part I","type":"post"},{"authors":null,"categories":null,"content":" Image source 1.\nTable of Contents Communicating vessels and optimization formulation Vessel equilibrium as an optimization problem Computing a direction Projecting on the manifold Putting it all together Conclusion and perspective Acknowledgment Bonus Sources Fluid mechanics was one of my favourite topics in the Process Engineering program I followed (some people will quit reading at this point and never talk to me again) so without surprise, I could not resist diving into this new blog post on SIAM News. This is the second time a post from Mark Levi caught my attention, the last was on heat exchangers, on which I also wrote a post, toying with parallel and counter-current heat exchangers.\nThis new post from Mark Levi illustrates a key concept in constrained optimization: Lagrange multipliers and a nice interpretation in a problem of communicating vessels.\nCommunicating vessels and optimization formulation If you are familiar with fluid mechanics, feel free to skip this section. Imagine $N$ vessels filled with water, all connected through a pipe at the bottom as shown on the top figure. The problem statement is, given initial levels of water $x_k$ in each $k-th$ vessel:\nhow does the state evolve? what equilibrium, if any, is eventually reached? Otherwise, consider the weight of water creates pressure within it. The lower a point in the water, the higher the pressure, since there is more water above which exercises its weight. A difference in pressure between two points will create a motion of the water, until the pressure equalizes. Put differently, some fluid moves from the full part of the vessel (with more pressure) to empty parts (with less pressure) until the pressure equalizes.\n2\nSince the pressure at a point depends on the height of the fluid above this point, two points have equal pressure when the height of water above them is equal. This is a phenomenon we often experience, with a watering can for instance.\nVessel equilibrium as an optimization problem A system reaches an equilibrium at the minimum of its potential energy. Feel free to skip this part if you read the blog post by Mark Levi, we basically go over the problem formulation once again. An equilibrium state (where the state does not evolve anymore) can be found by solving the optimization problem minimizing the potential energy, subject to the respect of the laws of physics. These laws state two things:\nNo water loss: the mass of liquid is preserved, and since we are working with an incompressible liquid, the total volume too is constant. No negative volume: the different vessels exchange water, their volume increasing or decreasing with time, but at no point can a vessel reach a negative volume. Each vessel $k$ will be described by a profile, an area as function of the height $f_k(x)$. We assume that these functions $f_k$ are all continuous. The state at any point in time is the height in each vessel $x_k$. The total volume of water in the vessel is given by: $$V_k(x_k) = \\int_0^{x_k} f_k(h) dh.$$\nThe conservation of volume can be expressed as:\n$$V_{0} = \\sum_{k=1}^N V_k(x_k) = \\sum_{k=1}^N \\int_0^{x_k} f_k(h) dh$$\nwhere $V_{0}$ is the initial total volume water. The nonnegativity of water volume in each vessel can be expressed as: $$\\int_0^{x_k} f_k(h) dh \\geq 0,,, \\forall k \\in \\{1..N\\} $$\nThe area at any height $f_k(x)$ is positive or null, so this constraint can be simplified as: $$x_k \\geq 0 ,,, \\forall k \\in \\{1..N\\} $$\nThe potential function, the objective minimized by the problem, is the last thing we miss. It consists of the total potential function of the water in the vessels, caused by gravity only. Each infinitesimal slice of water from $x$ to $x + dx$ exercises its weight, which is proportional to its volume $f_k(x) dx$ times height $x$. By integrating over a whole vessel $k$, this gives a potential of: $$ \\int_0^{x_k} h f_k(h) M dh$$ with M a constant of appropriate dimension. Since we are minimizing the sum of these functions, we will get rid of the constant (sorry for shocking physicists), yielding an objective:\n$$ F(x) = \\sum_{k=1}^N \\int_0^{x_k} h f_k(h)dh.$$\nTo sum it all, the optimization problem finding an equilibrium is:\n$$ \\begin{align} \\min_{x} \u0026amp; \\sum_{k=1}^N \\int_0^{x_k} h f_k(h)dh \\\\\\\\\n\u0026amp; \\text{subject to:} \\\\\\ \u0026amp; G(x) = \\sum_{k=1}^N \\int_0^{x_k} f_k(h) dh - V_0 = 0\\\\\\\\ \u0026amp; x_k \\geq 0 ,,, \\forall k \\in \\{1..N\\} \\end{align} $$\nIf you read the blog post, you saw the best way to solve this problem is by relaxing the positivity constraints and write the first-order Karush-Kuhn-Tucker (KKT) conditions:\n$$ \\begin{align} \u0026amp; \\nabla F(x) = \\lambda \\nabla G(x) \u0026amp; \\Leftrightarrow\\\\\\ \u0026amp; x_k f_k(x_k) = \\lambda f_k(x_k) ,,,\\forall k \\in \\{1..N\\} \u0026amp; \\Leftrightarrow \\\\\\ \u0026amp; x_k = \\lambda ,,,\\forall k \\in \\{1..N\\} \\end{align} $$\nSo the multiplier $\\lambda$ ends up being the height of water across all vessels, the equations come back to the intuitive result. Between the second and third line, we implicitly eliminate the case $f_k(x_k) = …","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"5aca70310fd50348c5b4968275471fcb","permalink":"https://matbesancon.xyz/post/2020-05-09-volumes/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/2020-05-09-volumes/","section":"post","summary":"Constrained optimization on a fundamental engineering problem\n","tags":["julia","optimization","jump"],"title":"Experiments on communicating vessels, constrained optimization and manifolds","type":"post"},{"authors":null,"categories":null,"content":"One of the luxuries of a PhD in applied maths / computer science is the possibility to work from home. By possibility, I imply both the technical feasibility (my project does not require special equipment), and social acceptability (I never worked in labs enforcing presenteeism).\nOf course, working from home or from my usual corner coffee shop once every other week is very different from these exceptional circumstances. I should also highlight I do not have to take care of children or sick family members, which comes as a priority, big congrats to those who manage it. This post should not be seen as how things should be done, simply how I cope and as a list of ideas to pick from if you are in a similar situation. The context is also very special for me as I just moved to the UK for an academic visit at the University of Edinburgh, I had just settled at a new city, flat and office when things started to escalate.\n🎉🏴󠁧󠁢󠁳󠁣󠁴󠁿\nWith an apartment found and tickets bought, this is now official, I\u0026#39;ll be in Edinburgh in March \u0026amp; April for an academic visit at @UoEMaths ERGO\n— Mathieu Besançon (@matbesancon) February 23, 2020 I decided to stay in Edinburgh instead of coming back to France because the trip itself would be irresponsible, travelling through the UK, sleeping in London and then crossing the border with continental Europe, but also because I was settled in comfortably enough to spend the rough months of the epidemic here. However, this meant adjusting in different ways.\nTable of Contents Overall routine Social media and work breaks Broadening work activities Setup and spacial separation Virtual conferences and social aspects Keeping your locals afloat We got this Overall routine Being in Europe with many friends \u0026amp; contacts in North America, I used the circumstances to slightly shift my day, waking up between 8.30 and 9.15, and going to bed a bit later, sharing more hours of the day with them. This would normally be more complicated with things to attend to where I am, but allow me to attend European late morning and afternoon remote events, North-American morning and early-afternoon events. I take a quick breakfast in the kitchen (no hot drink), make a batch of tea or coffee to keep while at my desk. I use the morning to catch up on emails, work on research, writing code and prose (with breaks, see below). I do not go back to eat while not hungry, and not before 1.30PM. The afternoon is a blend of research work and semi-work-related activities. I usually have figured out something to cook by the end of the day, depending on how much I estimate this will take I start late-afternoon or early evening. Depending on the mood and tasks, I work a bit after dinner, before closing all work tabs and windows (yes, all) to switch to leisure time, including films, calling people, etc. I keep at least one hour at the end of the day for reading, these days fiction (the Poppy War, the Alienist and Death in the East at the moment), and only in paper versions, since I don’t have a reader and I get enough screen time in the day.\nSocial media and work breaks First things first, these things suck up your time and attention the rest of the year, now they also build up your anxiety like never before. I’m not telling you to resist (I don’t), just to separate the time where you work from the social media time in small blocks; pomodoro is your friend there. With this setting, being distracted is okay, when you suddenly find yourself on non-work related things, just stop the work timer, take your break, and restart it when you are ready to restart. My breaks vary between 5 and 30 minutes, either texting friends, reading non-work news articles, scrolling or playing mines: Well how is YOUR discovery of gravity going? pic.twitter.com/Q8jaDPq6Tg\n— Mathieu Besançon (@matbesancon) March 26, 2020 The rest of the time, I close distracting browser tabs (including emails) and toss my phone out of reach.\nBroadening work activities Staying focused on work is clearly harder; one way to cope with it is to broaden work activities and even if it does not serve you in the short term, consider it work, no need to drown in guilt. My semi-work activities include open-source software, reading papers and books in my domain but not directly relevant to my research, following MOOCs, even on some things I assume I already know to see a different perspective. You may have noticed lots of university seminars are maintained in an online format. Since you do not need to run to the building on the other side of the campus, it can be a good opportunity to spend 45 minutes on other topics for your general scientific culture.\nSetup and spacial separation This section is to take with a pinch of salt, it assumes some financial comfort and a big enough flat. Early in the epidemic when it felt like we were going for gradual home isolation, I started setting up my office in a way that felt good, not for two hours in the evening any more, but for 8+ hours in the day.\nFirst, …","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"da50879797d14d3aa7110f06301464aa","permalink":"https://matbesancon.xyz/post/2020-03-26-confined-phd/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/2020-03-26-confined-phd/","section":"post","summary":"Calling it a guide would imply I figured it out, this is merely documenting what works... okay?\n","tags":["phd","academia"],"title":"Coping with a confined PhD, a naive report","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents Automatic Differentiation Automatic differentiation on a pure-Julia solver Example problem: weighted independent set Optimization model of the weighted independent set A Julia implementation Why not reverse-mode? Giving reverse with Zygote a shot How could this be improved? Conclusion, speculation, prospect Special mentions In continuous convex optimization, duality is often the theoretical foundation for computing the sensibility of the optimal value of a problem to one of its parameters. In the non-linear domain, it is fairly standard to assume one can compute at any point of the domain the function $f(x)$ and gradient $\\nabla f(x)$.\nWhat about discrete optimization?\nThe first thought would be that differentiating the resolution of a discrete problem does not make sense, the information it yields since infinitesimal variations in the domain of the variables do not make sense.\nHowever, three cases come to mind for which asking for gradients makes perfect sense:\nIn mixed-integer linear problems, some variables take continuous values. All linear expressions are differentiable, and every constraint coefficient, right-hand-side and objective coefficient can have an attached partial derivative.\nEven in pure-integer problems, the objective value will be a continuous function of the coefficients, possibly locally smooth, for which one can get the partial derivative associated with each weight.\nWe might be interested in computing the derivative of some expression of the variables with respect to some parameters, without this expression being the objective.\nFor these points, some duality-based techniques and reformulations can be used, sometimes very expensive when the input size grows. One common approach is to first solve the problem, then fixing the integer variables and re-solving the continuous part of the problem to compute the dual values associated with each constraint, and the reduced cost coefficients. This leads to solving a NP-hard problem, followed by a second solution from scratch of a linear optimization problem, still, it somehow works.\nMore than just solving the model and computing results, one major use case is embarking the result of an optimization problem into another more complete program. The tricks developed above cannot be integrated with an automated way of computing derivatives.\nAutomatic Differentiation Automatic Differentiation is far from new, but has known a gain in attention in the last decade with its used in ML, increasing the usability of the available libraries. It consists in getting an augmented information out of a function.\nIf a function has a type signature f: a -\u0026gt; b, the goal is, without modifying the function, to compute a derivative, which is also a function, which to every point in the domain, yields a linear map from domain to co-domain df: a -\u0026gt; (a -o b), where a -o b denotes a linear map, regardless of underlying representation (matrix, function, …). See the talk and paper1 for a type-based formalism of AD if you are ok with programming language formalism.\nAutomatic differentiation on a pure-Julia solver ConstraintSolver.jl is a recent project by Wikunia. As the name indicates, it is a constraint programming solver, a more Computer-Science-flavoured approach to integer optimization. As a Julia solver, it can leverage both multiple dispatch and the type system to benefit from some features for free. One example of such feature is automatic differentiation: if your function is generic enough (not relying on a specific implementation of number types, such as Float64), gradients with respect to some parameters can be computed by calling the function just once (forward-mode automatic differentiation).\nExample problem: weighted independent set Let us consider a classical problem in combinatorial optimization, given an undirected graph $G = (V, E)$, finding a subset of the vertices, such that no two vertices in the subset are connected by an edge, and that the total weight of the chosen vertices is maximized.\nOptimization model of the weighted independent set Formulated as an optimization problem, it looks as follows:\n$$\\begin{align} (\\mathcal{P}): \\max_{x} \u0026amp; \\sum_{i \\in V} w_i x_i \\\\\\\\ \\text{s.t.} \\\\\\\\ \u0026amp; x_i + x_j \\leq 1 \\,\\, \\forall (i,j) \\in E \\\\\\\\ \u0026amp; x \\in \\mathbb{B}^{|V|} \\end{align} $$\nTranslated to English, this would be maximizing the weighted sum of picked vertices, which are decisions living in the $|V|$-th dimensional binary space, such that for each edge, no two vertices can be chosen. The differentiable function here is the objective value of such optimization problem, and the parameters we differentiate with respect to are the weights attached to each vertex $w_i$. We will denote it $f(w) = \\max_x (\\mathcal{P}_w)$.\nIf a vertex $i$ is not chosen in a solution, there are two cases:\nthe vertex has the same weight as at least one other, say $j$, such that swapping $i$ and $j$ in the selected subset does not change the optimal value. of …","date":1579737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"d3cd0d097b8f6111c7d33839ee6ada63","permalink":"https://matbesancon.xyz/post/2020-01-23-discrete-diff/","publishdate":"2020-01-23T00:00:00Z","relpermalink":"/post/2020-01-23-discrete-diff/","section":"post","summary":"What can automated gradient computations bring to mathematical optimizers, what does it take to compute?\n","tags":["julia","automatic-differentiation","optimization","integer-optimization","jump"],"title":"Differentiating the discrete: Automatic Differentiation meets Integer Optimization","type":"post"},{"authors":["Mathieu Besançon","Miguel F Anjos","Luce Brotcorne","Juan A Gomez-Herrera"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"1dd28f555d24ffb56c9ec54852cf7a2d","permalink":"https://matbesancon.xyz/publication/journal/besanccon-2020-bilevel/","publishdate":"2023-12-22T16:16:29.772457Z","relpermalink":"/publication/journal/besanccon-2020-bilevel/","section":"publication","summary":"","tags":[],"title":"A bilevel approach for optimal price-setting of time-and-level-of-use tariffs","type":"publication"},{"authors":null,"categories":null,"content":"Unlike other ecosystems in the scientific programming world, scientists and engineers working with Julia usually prefer a whole stack in Julia for many reasons. The compiler is doing way better when able to infer what is going on in a piece of code; when an error is thrown, the stack trace looks much nicer when only pure Julia code is involved, functions and types can be defined as generic as wanted without hard-coded container or number types for instance.\nSometimes however, inter-operability with native code is needed to use some external native libraries. By that I mean natively built libraries (*.so files on Linux systems, *.dylib on OSX, *.dll on Windows). In this post, we will explore some tools to work with native libraries in Julia. In the last couple weeks, I tinkered a bit with the HiGHS solver developed at the University of Edinburgh, which I will use as an example throughout this post. It is still work in progress, but has nice promises as the next-generation linear optimization solver in the COIN-OR suite.\nWhat does a native lib look like? Looking at the repository, it is a pretty standard CMake-based C++ project producing both an executable and library which can be called through a C interface. The two initial components are:\nThe source code producing the library, this can be written in any language producing native code (C, C++, Rust) The header file defining the C API to call the library from other programs. This interface is defined in a single header file src/interfaces/highs_c_api.h, header files may define a bunch of types (structs, unions, enums) but most importantly they define function prototypes looking like:\nint preprocess_variables(int* values, double offset, float coefficient); When using the function from Julia, the call to the native library looks like the following:\nccall( (my_library_name, :preprocess_variables), CInt, # return type (Ptr{Cint}, Cdouble, Cfloat), # tuple of argument types (pointer(my_array), 3.5, 4.5f) # tuple of arguments ) Let us dive in.\nSolution 1: build and link For this approach, the first step is to build the HiGHS library and have the library available. Following the documentation, the instructions are:\ncd HiGHS # where HiGHS is installed mkdir build cd build cmake .. # generate makefiles make # build everything here in the build directory Like often with native packages, some dependencies might be implicitly assumed, here is a Dockerfile building the project on an alpine machine, you should be able to reproduce this with Docker installed.\nFROM alpine:3.7 RUN apk add git cmake g++ gcc clang make WORKDIR /optpreprocess_variables RUN git clone https://github.com/ERGO-Code/HiGHS.git RUN mkdir -p HiGHS/build WORKDIR /opt/HiGHS/build RUN cmake .. \u0026amp;\u0026amp; make RUN make test RUN make install # optional Now back to the Julia side, say we assume the library is available at a given path, one can write the Julia functions corresponding to the interface. It is preferable not to expose error-prone C calls to the user. In the example of the preprocess_variables function defined above, a Julia wrapper would look like:\nfunction preprocess_variables(my_array::Vector{Cint}, offset::Cdouble, coefficient::Cfloat) result = ccall( (:preprocess_variables, my_library_name), Cint, (Ptr{Cint}, Cdouble, Cfloat), (pointer(my_array), 3.5, 4.5f) ) return result end Once these wrapper functions are defined, users can convert their values to the corresponding expected argument types and call them. The last thing needed is my_library_name, which must be the path to the library object. Hard-coding or assuming paths should be avoided, it makes software harder to install on some systems. One thing that can be done is asking the user to pass the library path as an environment variable:\nENV[\u0026#34;HIGHS_DIR\u0026#34;] # should contain the path to the HIGHS directory joinpath(ENV[\u0026#34;HIGHS_DIR\u0026#34;], \u0026#34;build\u0026#34;, \u0026#34;lib\u0026#34;, \u0026#34;libhighs.so\u0026#34;) Doing this every time is however not convenient. Since library paths are not changing at every call, one can check for this path at the installation of the package. For this purpose, a file deps/build.jl can be added in every package and will be run at the installation of the package or when the Pkg.build command is called. A build.jl for our purpose could look like:\nconst highs_location = ENV[\u0026#34;HIGHS_DIR\u0026#34;] const libhighs = joinpath(highs_location, \u0026#34;build\u0026#34;, \u0026#34;lib\u0026#34;, \u0026#34;libhighs.so\u0026#34;) const depsfile = joinpath(@__DIR__, \u0026#34;deps.jl\u0026#34;) open(depsfile, \u0026#34;w\u0026#34;) do f print(f, \u0026#34;const libhighs = \u0026#34;) print(f, libhighs) println(f) end The snippet above looks for the libhighs.so library, using the environment variable as location of the base directory of HiGHS. Placed in build.jl, the script will create a deps.jl file in the deps folder of the Julia package, and write const libhighs = \u0026#34;/my/path/to/highs/lib/libhighs.so\u0026#34;. This is more or less what happens with the SCIP.jl wrapper v0.9. Once the build step succeeds, one can add in the main module in /src:\nmodule HiGHS const deps_file = …","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"89650771389e61b39211727f99876766","permalink":"https://matbesancon.xyz/post/2019-11-04-binary-julia/","publishdate":"2019-11-04T00:00:00Z","relpermalink":"/post/2019-11-04-binary-julia/","section":"post","summary":"When going native is the only option, at least do it once and well.\n","tags":["julia"],"title":"Working with binary libraries for optimization in Julia","type":"post"},{"authors":null,"categories":null,"content":" A couple weeks ago, I had a wonderful evening thanks to the Skype a Scientist program, a 4th grade class from the US (think 9-10 years old if like me, you have no idea what grades stand for) and their super-dedicated teacher. It was a fun time but the most surprising part was discovering the questions they had prepared. I thought it would be worth it to list them and record some answers as I remember them. I grouped the questions in four arbitrary categories. The list also does not reflect the order in which the questions were asked.\nTable of Contents 👨‍🔬 So… you’re a scientist? Why did you decide to be a scientist? How did you become a scientist? What school did you go to? What kind of place do you work in? What science experiments do you do? How many days a week do you work? What research are you working on? 🏢 Life in a lab Have you seen a chemical reaction? How much workspace do you have? Do you any experiments with animals? What do you do in your lab? (if you work in a lab?) 📈 Maths, again? Do you make models of anything? How did the language of math get created? Why is math so hard? How are there so many different strategies for math? 💻 Working with computers Have you ever seen the inside of a computer? Do you help make computers? Have you ever seen a super computer? Have you ever created an app? 👨‍🔬 So… you’re a scientist? Why did you decide to be a scientist? Curiosity mostly, but also I wanted to challenge myself on open questions.\nHow did you become a scientist? What school did you go to? Anyone can become a scientist by doing science, the schools we go matters less than the will to explore science. I guess I became a full-time scientist when I started my PhD two years ago.\nWhat kind of place do you work in? I work in a “lab”, but in my case this is an office (no white lab coat, no smoking tubes) with two other people. The office is located in a research institute, where other scientists and professors work on their research.\nWhat science experiments do you do? I write mathematical models to make better decisions in complex environments, for example in power grids. With specific models, the computers are really good at finding the best decisions.\nHow many days a week do you work? I work from Monday to Friday, so 5 days. Sometimes I take vacations off, sometimes I work a bit more, depending on the emergency of what I am doing.\nWhat research are you working on? I am working on models for better decisions in what is called the power grid. The power grid is the network connecting everything to electricity sources. Whether you are in your class or your kitchen at home, when you turn on the light, electricity is flowing all the way from places where it is produced (like the water network). These days, there are more and more renewable sources like solar panels and wind farms, but sometimes there is no sun or wind, so we have to anticipate better what is happening and make our consumption flexible.\n🏢 Life in a lab Have you seen a chemical reaction? Yes, and so have you! Cooking food on a frying pan for example will start chemical reactions, if you leave it for too long, it’s getting brown and burned.\nHow much workspace do you have? If we talk about physical workspace, I use a full table, where I have my laptop, a keyboard, and a mess of papers, draft notes, books. I’m a messy scientist.\nDo you any experiments with animals? Nope!\nWhat do you do in your lab? (if you work in a lab?) Once I have developed models and obtained interesting answers from computers, I write articles for other scientists to read. Other than that, I discuss with other people to find ideas on the models we write or how to write better computer programs, I drink coffee and eat cookies even when I shouldn’t.\n📈 Maths, again? Do you make models of anything? I haven’t found counter-examples yet so I’ll go with yes, you can make models of anything. A model is a way we represent something in a way that is easier to grasp, either for us humans, or for a computer.\nHow did the language of math get created? I am not an expert in the history of science, but from what I remember of old mathematical papers I saw, the language of mathematics was created piece by piece, by iterations. First, concepts would be created, like adding two numbers together into a bigger number. Then, some scientist, not necessarily the person who developed the concept, would find way to represent this abstract concept, for example with a cross symbol: $+$.\nSometimes, several ways to represent the same thing would exist in parallel, and people would only agree later on which one should be kept.\nSee this timeline when different modern symbols were introduced: Source.\nWhy is math so hard? Because it’s both:\nA new language to learn (how to say or write things) New concepts (new things we are able to say) Things get easier with practice once you can relate the concept to things you already know or visualize. That’s also why you are learning multiplications with different …","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"56a06bab223c098f329ddba10eeaf26f","permalink":"https://matbesancon.xyz/post/2019-10-21-skype-a-scientist/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/post/2019-10-21-skype-a-scientist/","section":"post","summary":"\n","tags":[],"title":"Questions and answers from 4th graders","type":"post"},{"authors":null,"categories":null,"content":" Constructors are a basic building block of object-oriented programming (OOP). They expose ways to build specific types of objects consistently, using arbitrary rules to validate properties. Still, constructors are odd beasts in the OOP world. In Java, this is usually the first case of function overloading that learning programmers meet, often without knowing the term. An overloaded constructor is shown in the following example:\nclass Car { private Motor motor; public Car(Motor m) { this.motor = m; } public Car() { this.motor = new Motor(); } } Scala and Kotlin, which are both languages on the Java Virtual Machine designed after and learning from Java, made the design choice of imposing a primary constructor, which all other constructors have to call. Constructors are weird beasts because they act partly as a function, partly as a method. Moreover, they expose a special use of this as a method call instead of being a pointer to the current object:\nclass Car { private Motor motor; public Car(Motor m) { // \u0026#39;this\u0026#39; as an object reference this.motor = m; } public Car(int power) { Motor m = new Motor(power); // this as a method this(m); } } This has been in my experience confusing and harder to teach on my side because it forces the learner to get a grasp of many specific tricks at the same time. Another hard-to-grasp point is this(motor), which has never been defined has such. The definition it corresponds to is Car(Motor m), the required mental load here is just unnecessary. This is why I appreciate Kotlin and Scala having made constructors more restrictive, removing the need for hand-wavy explanations for bad design. This great blog post gives an overview of constructors in different mainstream languages and compare them with the trait-based system of Rust.\nConstructors outside class-based OOP I will focus here on composite types or struct. There is a whole section of the Julia docs on constructors, but I would summarize things as:\nThere is a primary constructor which must provide values for all fields. All other constructors are just functions, no magic is involved, and constructors are just multiple methods in the context of multiple dispatch. This way of building objects as simple structures holding data in different fields is not new, Kotlin and Scala have a similar pattern as we mentioned above. Languages like Rust and Go take a different path by having structures being plain structures, initialized by providing all fields directly: // rust example struct Motor { pub power: u8, } struct Car { pub motor : Motor } // let m = Motor{power : 33}; // go example type Motor struct { Power uint } // m := Motor{33} Both languages have conventions for calling a standard constructing function, namely fn new(args) -\u0026gt; T and func NewT(args) for Rust and Go respectively, but those are not special and remain a simple convention without additional language complexity.\nTwo lessons learned Two interesting Pull Requests are about to be merged in Distributions.jl, which is the main package for working with probability distributions in Julia. Both revolve around a revision of the work of constructors. I will use them to make a point which I believe generalizes well to other systems. No probability theory should be needed here, it is merely a motivating example.\nLesson 1: product distributions and constructor promises Given multiple random variables: $ X_{i}, i = 1..n $ we define a product distribution as the vector random variable built by stacking the different $ X_i $:\n$$ X = [ X_i | i \\in 1..n ] $$\nThey arrived in Distributions.jl in this PR if you are curious. One thing to be careful about is that the term “product distribution” does not correspond with the eponymous Wikipedia entry. What we refer to here is the product type in the sense of tuple construction and not the arithmetic product. EDIT: the correct corresponding Wikipedia entry is the one on Product measure, thanks Chad for pointing it out. One important property is that the entries of the product type are independent distributions, which helps a great deal deducing properties of the product distribution.\nAn example product type could be the product of two univariate Gaussian distributions:\n$$ X_1 \\sim \\mathcal{N}(0, 1)$$ $$ X_2 \\sim \\mathcal{N}(0, 2)$$ $$ X = [X_1, X_2]$$\nThe implementation of the Product type stores the vector of univariate distributions, sampling and computing the PDF/CDF is done on a per-entry basis. The corresponding code would look like this: using Distributions: Normal, Product, pdf Xs = [Normal(0, 1), Normal(0, 2)] p = Product(Xs) # sample from p rand(p) # compute PDF at (x1 = 0, x2 = 1) pdf(p, [0.0, 1.0])\nOne problem we have here is that we know some specialized, faster techniques can be used in specific cases. Our product here for example, is nothing more than a multivariate Gaussian distribution with independent components: $$ X \\sim \\mathcal{N}([0, 0], diag([1, 2]))$$ with $diag(\\cdot)$ constructing a diagonal matrix from a vector. …","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"d5e5daacba605742e8192c6381fc973d","permalink":"https://matbesancon.xyz/post/2019-09-23-constructors/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/post/2019-09-23-constructors/","section":"post","summary":"Constructors are a basic building block of object-oriented programming (OOP). They expose ways to build specific types of objects consistently, using arbitrary rules to validate properties. Still, constructors are odd beasts in the OOP world.","tags":["julia","rust","java"],"title":"Lessons learned on object constructors","type":"post"},{"authors":null,"categories":null,"content":" The progress of mathematical optimization as a domain has been tightly coupled with the development and improvement of computational methods and their implementations as computer programs. As observed in the recent MIPLIB compilation 1, the quantification of method performance in optimization cannot really be split from the experimental settings, solver performance is far from a theoretical science.\nDifferent methods and implementations manipulate different data structures to represent the same optimization problem. Reformulating optimization models has often been the role and responsibility of the practitioner, transforming the application problem at hand to fit a standard form that a given solver accepts as input for a solution method. Interested readers may find work on formal representation of optimization problems as data structures by Liberti et al23. Mapping a user-facing representation of an object into a semantically equivalent internal representation is the role of compilers. For mathematical optimization specifically, Algebraic Modelling Languages (AML) are domain-specific languages (and often an associated compiler and runtime) turning a user-specified code into data structures passed to solvers. Examples of such languages are JuMP, Pyomo, GAMS or AMPL; the first two being embedded in a host language (Julia and Python respectively), while the two last are stand-alone with their own compiler and runtime.\nWe will focus in this post on MathOptInterface.jl (MOI) which acts as a second layer of the compilation phase of an AML. The main direct user-facing language for this is JuMP, which has already been covered in other resources45. When passed to MOI, the problem has been read from the user code but not reformulated yet. In compiler terms, MOI appears after the parsing phase: the user code has been recognized and transformed into corresponding internal structures.\nTable of Contents Re-formulating problems using multiple dispatch The example of linear constraints Unique dispatch and multiple solvers The bridge system Bridge implementation Problem reformulation heuristics Perspective \u0026amp; conclusion Further resources Re-formulating problems using multiple dispatch Multiple dispatch is the specialization of code depending on the arity and type of arguments. When multiple definitions (methods) exist for a function, the types of the different arguments are used to determine which definition is compatible. If several definitions are compatible, the most specific with respect to the position in the type hierarchy is selected. If several definitions are compatible without a total ordering by specificity, the method call is ambiguous, which raises an error. More information on the dispatch system in Julia can be found in the seminal article and the recent talk on multiple dispatch. See the following examples for the basic syntax:\nf(x) = 3 # same as f(x::Any) = 3 f(x::Int) = 2x # dispatch on arity f(x, y) = 2 # defining and dispatching on a custom type struct X value::Float64 end f(x::X) = 3 * x.value In this section, we will consider the reformulation of problems using multiple dispatch. In a generic form, an optimization problem can be written as:\n$$\\begin{align} \\min_{x} ,,\u0026amp; f(x) \\\\ \\text{s.t.}\\\\ \u0026amp; F_i(x) \\in S_i \u0026amp; \\forall i \\end{align} $$\nThe example of linear constraints We will build a reformulation system leveraging multiple dispatch. Assuming the user code is already parsed, the problem input can be represented as function-set pairs $(F_i, S_i)$. If we restrict this to individual linear constraints, all functions are of the form: $$ F_i(x) = a_i^T x $$\nThe three types of sets are:\nLessThan(b): $ y \\in S_i \\Leftrightarrow y \\leq b $ GreaterThan(b): $ y \\in S_i \\Leftrightarrow y \\geq b $ EqualTo(b): $ y \\in S_i \\Leftrightarrow y = b $ abstract type ConstraintSet end struct LessThan{T} \u0026lt;: ConstraintSet b::T end struct GreaterThan{T} \u0026lt;: ConstraintSet b::T end struct EqualTo{T} \u0026lt;: ConstraintSet b::T end abstract type ScalarFunction end struct ScalarAffineFunction{T} \u0026lt;: ScalarFunction a::Vector{T} x::Vector{VariableIndex} end Now that the fundamental structures are there, let us think of a solver based on the simplex method, accepting only less-or-equal linear constraints. We will assume a Model type has been defined, which supports a function add_constraint!(m::Model, f::F, s::S), which adds a constraint of type F in S.\nfunction add_constraint!(m::Model, f::ScalarAffineFunction, s::LessThan) pass_to_solver(m.solver_pointer, f, s) end function add_constraint!(m::Model, f::ScalarAffineFunction{T}, s::GreaterThan{T}) where {T} # a^T x \u0026gt;= b \u0026lt;=\u0026gt; -a^T x \u0026lt;= b leq_set = LessThan{T}(-s.b) leq_function = ScalarAffineFunction(-f.a, f.x) add_constraint!(m, leq_function, leq_set) end function add_constraint!(m::Model, f::ScalarAffineFunction, s::EqualTo) # a^T x == b \u0026lt;=\u0026gt; a^T x \u0026lt;= b \u0026amp;\u0026amp; a^T x \u0026gt;= b leq_set = LessThan(s.b) geq_set = LessThan(s.b) leq_function = copy(f) geq_function = copy(f) add_constraint!(m, leq_function, …","date":1568246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"406fa142b0191f91da7f2b94f9ed344c","permalink":"https://matbesancon.xyz/post/2019-09-12-bridging-indicator/","publishdate":"2019-09-12T00:00:00Z","relpermalink":"/post/2019-09-12-bridging-indicator/","section":"post","summary":"Compiling mathematical optimization problems in a multiple-dispatch context.\n","tags":["julia","jump","optimization","graphs"],"title":"Bridges as an extended dispatch system","type":"post"},{"authors":[],"categories":[],"content":"In a previous post, we pushed the boundaries of the Graphs.jl abstraction to see how conforming the algorithms are to the declared interface, noticing some implied assumptions that were not stated. This has led to the development of VertexSafeGraphs.jl and soon to some work on Graphs.jl itself.\nAnother way to push the abstraction came out of the JuliaNantes workshop: leveraging some special structure of graphs to optimize some specific operations. A good parallel can be established be with the LinearAlgebra package from Julia Base, which defines special matrices such as Diagonal and Symmetric and Adjoint, implementing the AbstractMatrix interface but without storing all the entries.\nA basic example Suppose you have a path graph or chain, this means any vertex is connected to its predecessor and successor only, except the first and last vertices. Such graph can be represented by a Graphs.SimpleGraph: import Graphs const LG = Graphs g = LG.path_graph(10) for v in 1:9 @assert LG.has_edge(g, v, v+1) # should not explode end\nThis is all fine, but we are encoding in an adjacency list some structure that we are aware of from the beginning. If you are used to thinking in such way, “knowing it from the beginning” can be a hint that it can be encoded in terms of types and made zero-cost abstractions. The real only runtime information of a path graph (which is not available before receiving the actual graph) is its size $n$. The only thing to do is implement the handful of methods from the Graphs interface.\nstruct PathGraph{T \u0026lt;: Integer} \u0026lt;: LG.AbstractGraph{T} nv::Int end LG.edgetype(::PathGraph) = LG.Edge{Int} LG.is_directed(::Type{\u0026lt;:PathGraph}) = false LG.nv(g::PathGraph) = g.nv LG.ne(g::PathGraph) = LG.nv(g) - 1 LG.vertices(g::PathGraph) = 1:LG.nv(g) LG.edges(g::PathGraph) = [LG.Edge(i, i+1) for i in 1:LG.nv(g)-1] LG.has_vertex(g::PathGraph, v) = 1 \u0026lt;= v \u0026lt;= LG.nv(g) function LG.outneighbors(g::PathGraph, v) LG.has_vertex(g, v) || return Int[] LG.nv(g) \u0026gt; 1 || return Int[] if v == 1 return [2] end if v == LG.nv(g) return [LG.nv(g)-1] end return [v-1, v+1] end LG.inneighbors(g::PathGraph, v) = outneighbors(g, v) function LG.has_edge(g::PathGraph, v1, v2) if !has_vertex(g, v1) || !has_vertex(g, v2) return false end return abs(v1-v2) == 1 end A more striking example PathGraph may leave you skeptical as to the necessity of such machinery, and you are right. A more interesting example might be complete graphs. Again for these, the only required piece of information is the number of vertices, which is a lot lighter than storing all the possible edges. We can make a parallel with FillArrays.jl, implicitly representing the entries of a matrix.\nUse cases The question of when to use a special-encoded graph is quite open. This type can be used with all functions assuming a graph-like behaviour, but is immutable, it is therefore not the most useful when you construct these special graphs as a starting point for an algorithm mutating them.\nPerformance As of now, simple benchmarks will show that the construction of special graphs is cheaper than the creation of the adjacency lists for Graphs.SimpleGraph. Actually using them for “global” algorithms is another story:\nfunction f(G, nv) g = G(nv) pr = pagerank(g) km = kruskal_mst(g) return (g, pr, km) end Trying to benchmark this function on PathGraph shows it is way worse than the corresponding SimpleGraph structure, the CompleteGraph implementation is about the same order of allocations and runtime as its list-y counterpart.\nThe suspect for the lack of speedup is the edges operation, optimized with a custom edge iterator in Graphs and returning a heap-allocated Array in SpecialGraphs for now. Taking performance seriously will requiring tackling this before anything else. Other opportunities for optimization may include returning StaticArrays and re-implementing optional methods such as Graphs.adjacency_matrix using specialized matrix types.\nConclusion and further reading The work on these graph structures is happening in SpecialGraphs.jl, feel free to file issues and submit pull requests. Also check out the matrix-based graph prototype in this post.\n","date":1564071283,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697889712,"objectID":"c408ebcd62bd8e71a54688d5696b823b","permalink":"https://matbesancon.xyz/post/2019-07-25-special-graphs/","publishdate":"2019-07-25T18:14:43+02:00","relpermalink":"/post/2019-07-25-special-graphs/","section":"post","summary":"In a previous post, we pushed the boundaries of the Graphs.jl abstraction to see how conforming the algorithms are to the declared interface, noticing some implied assumptions that were not stated.","tags":["julia","graphs","interface"],"title":"Leveraging special graph shapes in Graphs","type":"post"},{"authors":[],"categories":[],"content":"In various graph-related algorithms, a graph is modified through successive operations, merging, creating and deleting vertices. That’s the case for the Blossom algorithm finding a best matching in a graph and using contractions of nodes. In such cases, it can be useful to remove only the vertex being contracted, and maintain the number of all other vertices.\nLightGraphs.jl offers a set of abstractions, types and algorithms to get started with graphs. The claim of the abstraction is simple: whatever the underlying structure representing your graph, if it implements the AbstractGraph interface, it can be used out of the box with all algorithms built on LightGraphs.jl. The main concrete type presented by LightGraphs.jl is SimpleGraph and its directed counterpart SimpleDiGraph, only storing edges as adjacency lists, meaning vertices are just the integers from 1 to the length of the list. This means that in a graph with 6 vertices, deleting vertex 4 will re-label vertex 6 as 4. Hopefully, the interface should allow us to build a graph type on top of another graph, re-implementing only vertex removal.\nA simple vertex-safe implementation First things first, we will build it as a struct, using LightGraphs:\nimport LightGraphs const LG = LightGraphs struct VSafeGraph{T, G\u0026lt;:LG.AbstractGraph{T}, V\u0026lt;:AbstractVector{Int}} \u0026lt;: LG.AbstractGraph{T} g::G deleted_vertices::V VSafeGraph(g::G, v::V) where {T, G\u0026lt;:LG.AbstractGraph{T}, V\u0026lt;:AbstractVector{Int}} = new{T, G, V}(g, v) end VSafeGraph(g::G) where {G\u0026lt;:LG.AbstractGraph} = VSafeGraph(g, Vector{Int}()) VSafeGraph(nv::Integer) = VSafeGraph(LG.SimpleGraph(nv)) We added simple default constructors for convenience. The structure holds two elements:\nAn inner abstract graph g A list of vertices already deleted: deleted_vertices. The interface can now be implemented for our type, starting with the trivial parts:\nLG.edges(g::VSafeGraph) = LG.edges(g.g) LG.edgetype(g::VSafeGraph) = LG.edgetype(g.g) LG.is_directed(g::VSafeGraph) = LG.is_directed(g.g) LG.is_directed(::Type{\u0026lt;:VSafeGraph{T,G}}) where {T,G} = LG.is_directed(G) LG.ne(g::VSafeGraph) = LG.ne(g.g) LG.nv(g::VSafeGraph) = LG.nv(g.g) - length(g.deleted_vertices) LG.vertices(g::VSafeGraph) = (v for v in LG.vertices(g.g) if !(v in g.deleted_vertices)) LG.outneighbors(g::VSafeGraph, v) = LG.outneighbors(g.g, v) LG.inneighbors(g::VSafeGraph, v) = LG.inneighbors(g.g, v) LG.has_vertex(g::VSafeGraph, v) = LG.has_vertex(g.g, v) \u0026amp;\u0026amp; !(v in g.deleted_vertices) LG.has_edge(g::VSafeGraph, e) = LG.has_edge(g.g, e) LG.add_vertex!(g::VSafeGraph) = LG.add_vertex!(g.g) LG.rem_edge!(g::VSafeGraph, v1, v2) = LG.rem_edge!(g.g, v1, v2) Base.copy(g::VSafeGraph) = VSafeGraph(copy(g.g), copy(g.deleteed_vertices)) For most of these, we only re-call the method on the inner graph type. Only for LG.nv, which computes the number of vertices in the inner graph, minus the number of vertices in our removed list. Now the tricky parts, adding an edge and removing a vertex, which require a bit more verifications:\nfunction LG.add_edge!(g::VSafeGraph, v1, v2) if !LG.has_vertex(g, v1) || !LG.has_vertex(g, v2) return false end LG.add_edge!(g.g, v1, v2) end function LG.rem_vertex!(g::VSafeGraph, v1) if !LG.has_vertex(g, v1) || v1 in g.deleted_vertices return false end for v2 in LG.outneighbors(g, v1) LG.rem_edge!(g, v1, v2) end for v2 in LG.inneighbors(g, v1) LG.rem_edge!(g, v2, v1) end push!(g.deleted_vertices, v1) return true end Instead of removing the vertex v1 from the inner graph, the function removes all edges pointing to and from v1, and then adds it to the removed list.\nSpecific and generic tests So far so good, we can add some basic tests to check our type behaves as expected:\n@testset \u0026#34;Graph construction and basic interface\u0026#34; begin nv = 20 g1 = VSafeGraph(nv) @test LG.nv(g1) == nv @test LG.nv(g1.g) == nv g2_inner = LG.CompleteGraph(nv) g2 = VSafeGraph(g2_inner) @test LG.nv(g2) == LG.nv(g2_inner) @test LG.ne(g2) == LG.ne(g2_inner) @test all(sort(collect(LG.vertices(g2))) .== sort(collect(LG.vertices(g2_inner)))) g3 = VSafeGraph(LG.CompleteDiGraph(30)) @test LG.is_directed(g3) @test !LG.is_directed(g2) end @testset \u0026#34;Vertex deletion\u0026#34; begin Random.seed!(33) nv = 45 inner = LG.CompleteGraph(nv) g = VSafeGraph(inner) @test LG.ne(inner) == LG.ne(g) @test LG.nv(inner) == LG.nv(g) nrm = 0 for _ in 1:15 removed_ok = LG.rem_vertex!(g, rand(1:nv)) if !removed_ok continue end nrm += 1 @test LG.nv(inner) == nv @test LG.nv(g) == nv - nrm @test length(g.deleted_vertices) == nrm @test LG.ne(inner) == LG.ne(g) end end So far so good. Now, with the promise of generic graphs and the AbstractGraph interface, we should be able to use any algorithm in LightGraphs.jl, let us try to compute a page rank and a Kruskal minimum spanning tree:\nnv = 45 inner = LG.CompleteGraph(nv) g = VSafeGraph(inner) removed_ok = LG.rem_vertex!(g, rand(1:nv)) @test removed_ok # LG broken here @test_throws BoundsError LG.pagerank(g) @test_throws BoundsError LG.kruskal_mst(g) Yikes, what’s happening here? …","date":1559207683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578325251,"objectID":"732b9b8a8f3f03fb4cee321f67ba5ca4","permalink":"https://matbesancon.xyz/post/2019-05-30-vertex-safe-removal/","publishdate":"2019-05-30T11:14:43+02:00","relpermalink":"/post/2019-05-30-vertex-safe-removal/","section":"post","summary":"In various graph-related algorithms, a graph is modified through successive operations, merging, creating and deleting vertices. That’s the case for the Blossom algorithm finding a best matching in a graph and using contractions of nodes.","tags":["julia","graphs","interface"],"title":"Vertex removal in LightGraphs","type":"post"},{"authors":null,"categories":null,"content":"Last Friday was a great seminar of the Combinatorial Optimization group in Paris, celebrating the 85th birthday of Jack Edmonds, one of the founding researchers of combinatorial optimization, with the notable Blossom matching algorithm. .@SoniaVanier opened the workshop and organized a great party at Sorbonne for Jack Edmonds. I had a great honor to be one of the speakers at this event #orms pic.twitter.com/oHwKvg43Zm\n— Ivana Ljubic (@ILjubic) May 3, 2019 Laurence Wolsey and Ivana Ljubic were both giving talks on applications and developments in Benders decompositions. It also made me want to refresh my knowledge of the subject and play a bit with a simple implementation.\nLaurence Wolsey talks about Benders decomposition at the Jack Edmonds birthday workshop at Sorbonne #orms pic.twitter.com/K8hjdqKmwQ\n— Ivana Ljubic (@ILjubic) May 3, 2019 Table of Contents High-level idea Digging into the structure A JuMP implementation High-level idea Problem decompositions are used on large-scale optimization problems with a particular structure. The decomposition turns a compact, hard-to-solve formulation into an easier one but of great size. In the case of Benders, great size means a number of constraints growing exponentially with the size of the input problem. Adding all constraints upfront would be too costly. Furthermore, in general, only a small fraction of these constraints will be active in a final solution, the associated algorithm is to generate them incrementally, re-solve the problem with the new constraint until no relevant constraint can be found anymore.\nWe can establish a more general pattern of on-the-fly addition of information to an optimization problem, which entails two components:\nAn incrementally-built problem, called Restricted Master Problem (RMP) in decomposition. An oracle or sub-problem, taking the problem state and building the new required structure (here a new constraint). Sounds familiar? Benders can be seen as the “dual twin” of the Dantzig-Wolfe decomposition I had played with in a previous post.\nDigging into the structure Now that we have a general idea of the problem at hand, let’s see the specifics. Consider a problem such as: $$ \\min_{x,y} f(y) + c^\\top x $$ s.t. $$ G(y) \\in \\mathcal{S}$$ $$ A x + D y \\geq b $$ $$ x \\in \\mathbb{R}^{n_1}_{+}, y \\in \\mathcal{Y} $$\nWe will not consider the constraints specific to $y$ (the first row) nor the $y$-component of the objective. The key assumption of Benders is that if the $y$ are fixed, the problem on the $x$ variables is fast to solve. Lots of heuristics use this idea of “fix-and-optimize” to avoid incorporating the “hard” variables in the problem, Benders leverages several properties to bring the idea to exact methods (exact in the sense of proven optimality).\nTaking the problem above, we can simplify the structure by abstracting away (i.e. projecting out) the $x$ part: $$ \\min_{y} f(y) + \\phi(y) $$ s.t. $$ G(y) \\in \\mathcal{S}$$ $$ y \\in \\mathcal{Y} $$\nWhere: $$ \\phi(y) = \\min_{x} \\{c^\\top x, Ax \\geq b - Dy, x \\geq 0 \\} $$\n$\\phi(y)$ is a non-smooth function, with $, dom\\ \\phi ,$ the feasible domain of the problem. If you are familiar with bilevel optimization, this could remind you of the optimal value function used to describe lower-level problems. We will call $SP$ the sub-problem defined in the function $\\phi$.\nThe essence of Benders is to start from an outer-approximation (overly optimistic) by replacing $\\phi$ with a variable $\\eta$ which might be lower than the true min value, and then add cuts which progressively constrain the problem. The initial outer-approximation is:\n$$ \\min_{y,\\eta} f(y) + \\eta $$ s.t. $$ G(y) \\in \\mathcal{S}$$ $$ y \\in \\mathcal{Y} $$\nOf course since $\\eta$ is unconstrained, the problem will start unbounded. What are valid cuts for this? Let us define the dual of the sub-problem $SP$, which we will name $DSP$: $$ \\max_{\\alpha} (b - Dy)^\\top \\alpha $$ s.t. $$ A^\\top \\alpha \\leq c $$ $$ \\alpha \\geq 0 $$\nGiven that $\\eta \\geq min SP$, by duality, $\\eta \\geq max DSP$. Furthermore, by strong duality of linear problems, if $\\eta = \\min \\max_{y} DSP$, it is exactly equal to the minimum of $\\phi(y)$ and yields the optimal solution.\nOne thing to note about the feasible domain of $DSP$, it does not depend on the value of $y$. This means $z$ feasible for all values of the dual is equivalent to being feasible for all extreme points and rays of the dual polyhedron. Each of these can yield a new cut to add to the relaxed problem. For the sake of conciseness, I will not go into details on the case when the sub-problem is not feasible for a $y$ solution. Briefly, this is equivalent to the dual being unbounded, it thus defines an extreme ray which must be cut out. For more details, you can check these lecture notes.\nA JuMP implementation We will define a simple implementation using JuMP, a generic optimization modeling library on top of Julia, usable with various solvers. Since the master and sub-problem resolutions are …","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"ae82b16499db373e4a45c725fcd1b8b9","permalink":"https://matbesancon.xyz/post/2019-05-08-simple-benders/","publishdate":"2019-05-08T00:00:00Z","relpermalink":"/post/2019-05-08-simple-benders/","section":"post","summary":"Cracking Benders decomposition, one cut at a time.\n","tags":["optimization","jump","integer-optimization","julia"],"title":"A take on Benders decomposition in JuMP","type":"post"},{"authors":null,"categories":null,"content":"This week, I came across Richard Oberdieck’s post, “Why ’evaluate’ is the feature I am missing the most from commercial MIP solvers”. It would indeed be practical to have for the reasons listed by the author, but some barriers stand to have it as it is expressed in the snippets presented.\nTable of Contents Initial problem statement A terminology problem Encoding possibilities as sum-types A typed solution for describing mathematical problems Stealing a solution elsewhere Further resources Initial problem statement The author first tests the optimization of a non-linear function through scipy as such:\nfunc = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x func(5) # 25.001603108415402 So far so good, we are defining a scalar function, passing it a scalar value at which it evaluates and returns the value, which is what it is supposed to do.\nNow the real gripe comes when moving on to developing against a black box solver (often commercial, closed-source), commonly used for linear, mixed-integer problems: import xpress as xp # Define the model and variables Model = xp.problem() x = xp.var(lb=0, ub=10) Model.addVariable(x) # Define the objective and solve test_objective = 5*x Model.setObjective(test_objective) Model.solve() # test_objective(5) does not work\nOne first problem to notice here is that test_objective is at best an expression, not a function, meaning it does not depend on an input argument but on decision variables declared globally. That is one point why it cannot be called.\nNow, the rest of this article will be some thoughts on how optimization problems could be structured and represented in a programming language.\nOne hack that could be used is being able to set the values of x, but this needs to be done at the global level: x = xp.var(lb=0, ub=10) Model.addVariable(x) # Define the objective test_objective = 5*x x.set(5) # evaluates test_objective with the set value of x xp.evaluale(test_objective)\nHaving to use the global scope, with an action on one object (the variable x) modifying another (the test_objective expression) is called a side-effect and quickly makes things confusing as your program grows in complexity. You have to contain the state in some way and keep track. Keeping track of value changes is more or less fine, but the hardest part is keeping track of value definitions. Consider the following example: x = xp.var(lb=0, ub=10) Model.addVariable(x) y = xp.var(lb=0, ub=10) Model.addVariable(y) # Define the objective and solve test_objective = 5*x + 2*y xp.evaluale(test_objective) # no variable set, what should this return? x.set(5) xp.evaluale(test_objective) # y is not set, what should this return?\nA terminology problem We are touching a more fundamental problem here, variables are not values and cannot be considered as such. Merging the term “variable” for variables of your Python/Julia/other program with the decision variables from an optimization problem creates a great confusion. Just like variables, the term function is confusing here: most optimization techniques exploit the problem structure, think linear, disciplined convex, semi-definite; anything beyond non-linear differentiable or black-box optimization will use the specific structure in a specialized algorithm. If standard functions from your programming language are used, no structure can be leveraged by the solver, which only sees a function pointer it can pass values to. So working with mathematical optimization forces you to re-think what you call “variables” and what you call “functions”.\nThere is something we can do for the function part, which is defining arithmetic rules over variables and expressions, which is for instance what the JuMP modelling framework does: using JuMP m = Model() @variable(m, x1 \u0026gt;= 0) @variable(m, x2 \u0026gt;= 0) # random affine function f(a, b) = π + 3a + 2b f(x1, x2) # returns a JuMP.GenericAffExpr{Float64,VariableRef} @variable(m, y) f(x1, x2) + y # also builds a JuMP.GenericAffExpr{Float64,VariableRef}\nThis works especially well with affine functions because composing affine expressions builds other affine expressions but gets more complex any time other types of constraints are added. For some great resource on types and functions for mathematical optimization, watch Prof. Madeleine Udell’s talk at JuliaCon17 (the Julia syntax is from a pre-1.0 version, it may look funny).\nEncoding possibilities as sum-types Getting back to evaluation, to make this work, you need to know what values variables hold. What if the model hasn’t been optimized yet? You could take:\nA numerical approach and return NaN (floating point value for Not-A-Number) An imperative approach and throw an error when we evaluate an expression without values set or the model optimized A typed functional approach and describe the possibility of presence/absence of a value through types The first approach was JuMP 0.18 and prior, the second is JuMP 0.19 and onward, the third is the one of interest to us, if we want to describe what is …","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"43ed916a7a94f76a3d197360f50c5d35","permalink":"https://matbesancon.xyz/post/2019-04-14-optimization-function-evaluation/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/post/2019-04-14-optimization-function-evaluation/","section":"post","summary":"Some digging in representations for optimization modelling\n","tags":["optimization","jump","functional","python","julia"],"title":"Variables are not values: types and expressions in mathematical optimization","type":"post"},{"authors":null,"categories":null,"content":" I must admit I am not always the most talented at social events. One point I am especially bad at is remembering names, and it gets even harder when lots of people have similar or similar-sounding names. What if we could select a list of people with names as different from each other as possible?\nFirst some definitions, different here is meant with respect to the Hamming distance of any two names. This is far from ideal since Ekaterina would be quite far from Katerina, but it will do the trick for now.\nGraph-based mental model This sounds like a problem representable as a complete graph. The names are the vertices, and the weight associated with each edge $(i,j)$ is the distance between the names of the nodes. We want to take a subset of $k$ nodes, such that the sum of edge weights for the induced sub-graph is maximum. This is therefore a particular case of maximum (edge) weight clique problem over a complete graph, which has been investigated in [1, 2] among others.\nA mathematical optimization approach This model can be expressed in a pretty compact way:\n$$ \\max_{x,y} \\sum_{(i,j)\\in E} c_{ij} \\cdot y_{ij} $$ subject to: $$ 2y_{ij} \\leq x_i + x_j ,, \\forall (i,j) \\in E$$ $$ \\sum_{i} x_i \\leq k $$ $$x_i, y_{ij} \\in \\mathbb{B} $$\nThe graph is complete and undirected, so the set of edges is:\n$ E = $ {$ (i,j) | i \\in $ {$ 1..|V| $}$, j \\in ${$ 1..i-1 $}}\nIt’s an integer problem with a quadratic number of variables and constraints. Some other formulations have been proposed, and there may be a specific structure to exploit given that we have a complete graph. For the moment though, this generic formulation will do.\nA Julia implementation What we want is a function taking a collection of names and returning which are selected. The first thing to do is build this distance matrix. We will be using the StringDistances.jl package not to have to re-implement the Hamming distance.\nimport StringDistances hamming(s1, s2) = StringDistances.evaluate(StringDistances.Hamming(), s1, s2) function build_dist(vstr, dist = hamming) return [dist(vstr[i], vstr[j]) for i in eachindex(vstr), j in eachindex(vstr)] end We keep the option to change the distance function with something else later. The optimization model can now be built, using the distance function and $k$, the maximum number of nodes to take.\nusing JuMP import SCIP function max_clique(dist, k) m = Model(with_optimizer(SCIP.Optimizer)) n = size(dist)[1] @variable(m, x[1:n], Bin) @variable(m, y[i=1:n,j=1:i-1], Bin) @constraint(m, sum(x) \u0026lt;= k) @constraint(m, [i=1:n,j=1:i-1], 2y[i,j] \u0026lt;= x[i] + x[j]) @objective(m, Max, sum(y[i,j] * dist[i,j] for i=1:n,j=1:i-1)) return (m, x, y) end I’m using SCIP as an integer solver to avoid proprietary software, feel free to switch it for your favourite one. Note that we don’t optimize the model yet but simply build it. It is a useful pattern when working with JuMP, allowing users to inspect the build model or add constraints to it before starting the resolution. The last steps are straightforward:\ndist = build_dist(vstr) (m, x, y) = max_clique(dist, k) optimize!(m) # solve the problem # get the subset of interest diverse_names = [vstr[i] for i in eachindex(vstr) if JuMP.value(x[i]) ≈ 1.] And voilà.\nTrying out the model I will use 50 real names taken from the list of random names website, which you can find here. The problem becomes large enough to be interesting, but reasonable enough for a decent laptop. If you want to invite 4 of these people and get the most different names, Christian, Elizbeth, Beulah and Wilhelmina are the ones you are looking for.\nBonus and random ideas It is computationally too demanding for now, but it would be interesting to see how the total sum of distances evolves as you add more people.\nAlso, we are using the sum of distances as an objective to maximize. One interesting alternative would be to maximize the smallest distance between any two nodes in the subset. This changes the model, since we need to encode the smallest distance using constraints. We will use an indicator constraint to represent this:\n$$\\max_{x,y} d $$ subject to: $$ y_{ij} \\Rightarrow d \\leq c_{ij} ,, \\forall (i,j) \\in E$$ $$ 2y_{ij} \\leq x_i + x_j \\forall (i,j) \\in E $$ $$ \\sum_{(i,j) \\in E} y_{ij} = k\\cdot (k-1) $$\nDepending on the solver support, the indicator constraint can be modelled directly, with big M or SOS1 constraints. This remains harder than the initial model.\nSpecial thanks to Yuan for bringing out the discussion which led to this post, and to BYP for the feedback.\nSources [1] Alidaee, Bahram, et al. “Solving the maximum edge weight clique problem via unconstrained quadratic programming.” European Journal of Operational Research 181.2 (2007): 592-597.\n[2] Park, Kyungchul, Kyungsik Lee, and Sungsoo Park. “An extended formulation approach to the edge-weighted maximal clique problem.” European Journal of Operational Research 95.3 (1996): 671-682.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"01a2c2b9f10659f8bb42c5548961c487","permalink":"https://matbesancon.xyz/post/2019-04-07-name_distances/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/post/2019-04-07-name_distances/","section":"post","summary":"Making social events easier as a graph problem.\n","tags":["julia","optimization","jump","graph","integer-optimization"],"title":"Picking different names with integer optimization","type":"post"},{"authors":null,"categories":null,"content":" This post explores the possibility to build static lists in Julia, meaning lists for which the size is known at compile-time. This is inspired by a post on a Scala equivalent but will take different roads to see more than a plain port. Of course, this implementation is not that handy nor efficient but is mostly meant to push the limits of the type system, especially a trick of using recursive types as values (replacing a dependent type system). Some other references:\nThe list operations are inspired by the implementation in DataStructures.jl StaticArrays.jl is a good inspiration for static data structures in Julia Table of Contents First thoughts: value type parameter Recursive natural numbers Implementing a list-y behaviour Special-valued lists Multi-typed lists Conclusion Sources First thoughts: value type parameter Julia allows developers to define type parameters. In the case of a list, the most obvious one may be the type of data it contains: abstract type MyList{T} end\nSome types are however parametrized on other things, if we look at the definition of AbstractArray for example:\nAbstractArray{T,N} Supertype for N-dimensional arrays (or array-like types) with elements of type T.\nThe two type parameters are another type T and integer N for the dimensionality (tensor rank). The only constraint for a value to be an acceptable type parameter is to be composed of plain bits, complying with isbitstype.\nThis looks great, we could define our StaticList directly using integers.\n\u0026#34;\u0026#34;\u0026#34; A static list of type `T` and length `L` \u0026#34;\u0026#34;\u0026#34; abstract type StaticList{T,L} end struct Nil{T} \u0026lt;: StaticList{T,0} end StaticList{T}() where T = Nil{T}() StaticList(v::T) where T = Cons(v, Nil{T}()) struct Cons{T,L} \u0026lt;: StaticList{T,L+1} h::T t::StaticList{T,L} function Cons(v::T, t::StaticList{T,L}) where {T,L} new{T,L}(v,t) end end # Usage: # Cons(3, Nil{Int}()) is of type StaticList{Int,1} # Cons(4, Cons(3, Nil{Int}())) is of type StaticList{Int,2} If you try to evaluate this code, you will get an error: ERROR: MethodError: no method matching +(::TypeVar, ::Int64)\nPretty explicit, you cannot perform any computation on values used as type parameters. With more complex operations, this could make the compiler hang, crash or at least perform poorly (we would be forcing the compiler to execute this code at compile-time).\nOne way there might be around this is macros or replacing sub-typing with another mechanism. For the macro-based approach, ComputedFieldTypes.jl does exactly that. More discussion on computed type parameters in [1] and [2].\nEdit: using integer type parameters can be achieved using ComputedFieldTypes.jl as such:\njulia\u0026gt; using ComputedFieldTypes julia\u0026gt; abstract type StaticList{T,L} end julia\u0026gt; struct Nil{T} \u0026lt;: StaticList{T,0} end julia\u0026gt; @computed struct Cons{T,L} \u0026lt;: StaticList{T,L} h::T t::StaticList{T,L-1} function Cons(v::T, t::StaticList{T,L0}) where {T,L0} L = L0+1 new{T,L}(v,t) end end julia\u0026gt; Cons(3, Nil{Int}()) Cons{Int64,1,0}(3, Nil{Int64}()) julia\u0026gt; Cons(4, Cons(3, Nil{Int}())) Cons{Int64,2,1}(4, Cons{Int64,1,0}(3, Nil{Int64}())) This might be the neatest option for building the StaticList.\nRecursive natural numbers We can use the same technique as in the Scala post, representing natural number using recursive types.\nZeroLength is a special singleton type Next{L} represents the number following the one represented by L We can modify our previous example: \u0026#34;\u0026#34;\u0026#34; A type parameter for List length, the numerical length can be retrieved using `length(l::Length)` \u0026#34;\u0026#34;\u0026#34; abstract type Length end struct ZeroLength \u0026lt;: Length end struct Next{L\u0026lt;:Length} \u0026lt;: Length end \u0026#34;\u0026#34;\u0026#34; A linked list of size known at compile-time \u0026#34;\u0026#34;\u0026#34; abstract type StaticList{T,L\u0026lt;:Length} end struct Nil{T} \u0026lt;: StaticList{T,ZeroLength} end StaticList{T}() where T = Nil{T}() StaticList(v::T) where T = Cons(v, Nil{T}()) struct Cons{T,L\u0026lt;:Length} \u0026lt;: StaticList{T,Next{L}} h::T t::StaticList{T,L} function Cons(v::T, t::StaticList{T,L}) where {T,L\u0026lt;:Length} new{T,L}(v,t) end end \u0026#34;\u0026#34;\u0026#34; By default, the type of the Nil is ignored if different from the type of first value \u0026#34;\u0026#34;\u0026#34; Cons(v::T,::Type{Nil{T1}}) where {T,T1} = Cons(v, Nil{T}())\nWe can then define basic information for a list, its length:\nBase.length(::Type{ZeroLength}) = 0 Base.length(::Type{Next{L}}) where {L} = 1 + length(L) Base.eltype(::StaticList{T,L}) where {T,L} = T Base.length(l::StaticList{T,L}) where {T,L} = length(L) One thing should catch your attention in this block, we use a recursive definition of length for the Length type, which means we can blow our compiler. However, both of the definitions are static, in the sense that they don’t use type information, so the final call should reduce to spitting out the length cached at compile-time. You can confirm this is the case by checking the produced assembly instructions with @code_native. We respected our contract of a list with size known at compile-time.\nImplementing a list-y behaviour This part is heavily inspired by the DataStructures.jl list implementation, as …","date":1553904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"d04262380247a1ac96d69d0161a612e5","permalink":"https://matbesancon.xyz/post/2019-03-30-static-list/","publishdate":"2019-03-30T00:00:00Z","relpermalink":"/post/2019-03-30-static-list/","section":"post","summary":"Pushing the type system for more compile-time information\n","tags":["julia"],"title":"Static lists in Julia","type":"post"},{"authors":null,"categories":null,"content":" While reading books, lots of thoughts come and go, especially if the subject resonates and is connected to topics of interest. If the book is a novel or other fiction, I try to keep these thoughts away to remain in the universe and story. For non-fiction though, some of these thoughts constitute valuable elements to put on a larger frame, with other publications, slowly building context for the topics at hand. The Entrepreneurial State, by Mariana Mazzucato, definitely has some elements to put on a larger frame of economic policy, economics, on the perceived and actual role of the state and its institutions.\nChanging the narrative The main point and argument of the book is brilliant: re-trace the facts about the development of some technologies, companies, industries to challenge the established, implicit or explicit narrative about the state’s role. In that case, the narrative is the sacred effect of the market and individual entrepreneurs for building today’s greatest achievements.\nThe unexpected HR argument One point I never thought about before reading the book is the talent pool each side is taking from. While keeping the sexy part of the narrative, the private firms will always attract the best talents. For sure, some people will join public services for the greater good, but some necessary talents might not join because they have major criteria on what to achieve.\nIf the narrative is that public institutions are there simply for controlling and punctually fixing the economy, lots of talents will be driven by private firms able to offer them to actually accomplish things and move forward. If the State is now seen as the voice setting the direction for the coming years and the rules to get there before letting the children out on the playground, people working in the “development department” as Mazzucato calls it play a part in a strategic role they would not have in the firms themselves.\nIf you think about it, the only public servants glorified and pictured with cool jobs in today’s representation are linked to the military or police. Think of recent corporate series you’ve seen, public servants are always those envious ones who didn’t have the courage to take the risky path.\nOh no, Apple again To add some context, the first edition of the book was out in 2013. If I try to remember the ambient perception, tech was not yet the evil eating the world, building a startup was still freaking your parents out (at least if you lived in France), Bitcoin was still nerds’ or drugs money. And Apple was still in the general opinion the cool company building slick products. A chapter of the book is specifically dedicated to the company and how it has been helped by the US federal and state governments at various stages and for various steps of its rise. The narrative the author sets is that even the most innovative, “entrepreneurial”, garage-born companies got helped by the government all the way through, whatever version of it is told by them, the media or VCs. Still, this is a personal touch, but Apple has never been fascinating, nor have I admired the firm more than others, or had this “wow” effect to friends getting jobs there. It’s still a consumer firm building expensive toys, not solving the world’s problem. The problem with this chapter is that I agreed with the author’s underlying point before even starting it, so the content I read felt mostly like a bunch of historical facts on a company I am not that interested in.\nChanging energy systems Currently involved in a PhD involving thinking new decision processes for power grids, I naturally got thrilled to see a chapter on renewable generation and green business. The author mentions the parallel and applications of IT to these new challenges, with the nice mention of “throwing software at the problem” [1], referring to designing algorithms to cope with solar and wind power intermittency. In that case, it is indeed necessary but not sufficient. Software will at most bring information within reach of the agents needing it at the right time. This availability is only a prerequisite for enabling better decision-making in power systems, creating value shared between the different levels contributing to the various improvements. See recent work in journals such as IEEE Transactions on Power Systems / on Smart Grids, the hard problems are not data collection or transmission but making decision under various types of constraints. Nonetheless, it was a nice surprise to read an economist’s view, summary and prediction for smart grids, with mentions of programs such as demand response.\nThe danger of the strawman This might be one central critic to the book, from an argument construction perspective. The whole stream of thought is built around a de- and reconstruction of the role of the State for major innovations, past and present. There is however the continuous danger of a strawman argument, the detractors of the role of the states are often described as “they”, and …","date":1552867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"9ff4ca764329a6e65747f91ea9ec2731","permalink":"https://matbesancon.xyz/post/2019-01-24-entrepreneurial-state-review/","publishdate":"2019-03-18T00:00:00Z","relpermalink":"/post/2019-01-24-entrepreneurial-state-review/","section":"post","summary":"Some thoughts and notes on an industrial policy book.\n","tags":["book","economics"],"title":"Book review: the Entrepreneurial State","type":"post"},{"authors":null,"categories":null,"content":"In a recent pull request on a personal project, I spent some time designing an intuitive API for a specific problem. After reaching a satisfying result, I realized this would never have been possible without one of the central mechanisms of the Julia language: multiple dispatch. Feel free to read the Julia docs on the topic or what Wikipedia has to say about it.\nThis post is a walkthrough for multiple dispatch for a case in mathematical optimization. The first part will introduce the problem context and requires some notion in mathematical optimization, if this stuff is scary, feel free to skip to the rest directly.\nTable of Contents Refresher on if-then-else constraints Modeling if-then-else constraints Handling big M in an elegant way Polishing our design: enriched types Conclusion: avoiding a clarity-flexibility trade-off Refresher on if-then-else constraints I promised an example oriented towards mathematical optimization, here it is: it is common to model constraints with two variables $(x, y)$, $x$ continuous and $y$ binary stating:\n$y = 0 \\Rightarrow x = 0$ If $y = 1$, there is no specific constraint on $x$ Some examples of models with such constraint:\nFacility location: if a wharehouse is not opened, $y = 0$, then the quantity served by this point has to be $x = 0$, otherwise, the quantity can go up to the wharehouse capacity. Unit commitment (a classic problem for power systems): if a power plant has not been activated for a given hour, then it cannot supply any power, otherwise, it can supply up to its capacity. Complementarity constraints: if a dual variable $\\lambda$ is 0, then the corresponding constraint is not active (in non-degenerate cases, the slack variable is non-zero) Logical constraints with such if-then-else structure cannot be handled by established optimization solvers, at least not in an efficient way. There are two usual ways to implement this, “big-M” type constraints and special-ordered sets of type 1 SOS1.\nA SOS1 constraint specifies that out of a set of variables or expressions, at most one of them can be non-zero. In our case, the if-then-else constraint can be modeled as: $$SOS1(x,, 1-y)$$\nMost solvers handling integer variables can use these $SOS1$ constraints within a branch-and-bound procedure.\nThe other formulation is using an upper-bound on the $x$ variable, usually written $M$, hence the name:\n$$x \\leq M \\cdot y $$\nIf $y=0$, $x$ can be at most 0, otherwise it is bounded by $M$. If $M$ is sufficiently big, the constraint becomes inactive. However, smaller $M$ values yield tighter formulations, solved more efficiently. See Paul Rubin’s detailed blog post on the subject. If we want bounds as tight as possible, it is always preferable to choose one bound per constraint, instead of one unique $M$ for them all, which means we need a majorant of all individual $M$.\nAs a rule of thumb, big-M constraints are pretty efficient if $M$ is tight, but if we have no idea about it, SOS1 constraints may be more interesting, see [1] for recent numerical experiments applied to bilevel problems.\nModeling if-then-else constraints Now that the context is set, our task is to model if-then-else constraints in the best possible way, in a modeling package for instance. We want the user to specify something as:\nfunction handle_ifthenelse(x, y, method, params) # build the constraint with method using params end Without a dispatch feature baked within the language, we will end up doing it ourselves, for instance in:\nfunction handle_ifthenelse(x, y, method, params) if typeof(method) == SOS1Method # model as SOS1Method elseif typeof(method) == BigMMethod # handle as big M with params else throw(MethodError(\u0026#34;Method unknown\u0026#34;)) end end NB: if you have to do that in Julia, there is a isa(x, T) function verifying if x is a T in a more concise way, this is verifying sub-typing instead of type equality, which is much more flexible.\nThe function is way longer than necessary, and will have to be modified every time. In a more idiomatic way, what we can do is:\nstruct SOS1Method end struct BigMMethod end function handle_ifthenelse(x, y, ::SOS1Method) # model as SOS1Method end function handle_ifthenelse(x, y, ::BigMMethod, params) # handle as big M with params end Much better here, three things to notice:\nThis may look similar to pattern matching in function arguments if you are familiar with languages as Elixir. However, the method to use can be determined using static dispatch, i.e. at compile-time. We don’t need to carry around params in the case of the SOS1 method, since we don’t use them, so we can adapt the method signature to pass only what is needed. This code is much easier to document, each method can be documented on its own type, and the reader can refer to the method directly. Cherry on top, any user can define their own technique by importing our function and defining a new behavior: import OtherPackage # where the function is defined struct MyNewMethod end function handle_ifthenelse(x, y, …","date":1550966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"a89225f4619c744c2d52fcb80cecae6c","permalink":"https://matbesancon.xyz/post/2019-02-24-multiple-dispatch-optimizers/","publishdate":"2019-02-24T00:00:00Z","relpermalink":"/post/2019-02-24-multiple-dispatch-optimizers/","section":"post","summary":"Leveraging one of Julia central features for clearer formulation of an optimization problem.\n","tags":["optimization","julia"],"title":"Multiple dispatch - an example for mathematical optimizers","type":"post"},{"authors":null,"categories":null,"content":" After the first submissions to journals, most researchers will be contacted by editors for reviewing articles others have written. It may seem like a daunting task, evaluating the work someone else put several months to prepare, code, write, correct and submit.\nDisclaimer: to preserve the anonymity of the reviews I made and am making, all examples I give below are made up.\nThe main phases of my reviewing process are:\nWhat is this about? Can I review it? Is the paper in the scope of the journal? Are there some topics I might struggle to understand? Diving in, a first pass to get the story right Thematic passes \u0026amp; writing the recommendations What is this about? Can I review it? After receiving the invitation and getting the manuscript, my screening phase consists in reading only these three elements:\nTitle Abstract Keywords At that point, I roughly know if it is relevant for both the journal and me that I review it. If I feel way out of scope, I’ll reach out to the editor. I will also quickly check the name of the authors to make sure I do not have a conflict of interests with any of them, without looking them up on the internet of course, the goal is to avoid bias if I know them at a personal level.\nNote: Since this only took a quick screening, it can be done in a day or two, letting the editor know too late that you will not review increases the time to publication which is bad for the author, the journal and scientific publication in general.\nIs the paper in the scope of the journal? At that point, I re-read the journal’s aim and scope and keep in mind the main ideas. If I am not that familiar with it, I will also check titles and abstracts of random papers in the last issues. This will help during the review if there are some doubts on the manuscript being at the right spot.\nAre there some topics I might struggle to understand? If I have doubts on some parts of the method or context and can identify them, I’ll search for foundational articles and reference text books on the subject.\nIn any case, it is predictable that not all reviewers of the paper cover the same area of expertise, especially for multi-disciplinary journals. Still, it is always better to be comfortable with all components. Take a case in mathematical optimization, for instance a manuscript tackling a problem in power systems, with a game theoretical aspect and formulating a Semi-Definite Positive model solved using a bundle method. I might be familiar with the application (power systems) and game-theoretical considerations in such domain, but without being an expert in SDP and even less bundle methods. This is not a reason to refuse the role of reviewer.\nHowever, not being proficient on a component can introduce a bias in the review by putting the reviewer on the defensive:\n“why do the authors need all this fuss with this thing I’ve never heard of, why not the good all techniques like what I do”.\nI’ve seen read different comments in reviews which looked a lot like this. This is why it can be valuable to take some time to get more familiar with shadow areas. Plus this makes reviewing a challenge and an excuse to learn something new and connected to my area.\nDiving in, a first pass to get the story right At that point, I book at least two hours for a first read of the paper, with a pen, a printed version and a notebook. I should eventually get a tablet to take notes on the PDF instead of print-outs but for the moment, the number of papers I am asked to review remains reasonable. I read it through without interruptions (no phone, no open browser, no music or music without lyrics), taking notes on the side on all things that cross my mind. Notes are of different types: small mistakes, remarkable points, key information and the “interrogation queue”. This queue is inspired by developers’ code review and the most advanced metric found for it: An element is added in the queue when something is missing for my understanding here and has not been introduced upwards in the article. An element is removed from the queue when an explanation for it appears (so later in the article). Of course, any element remaining in the queue at the end of the manuscript is a problem: it is never explained, introduced properly. Two considerations play a role for the quality of the paper for its ease of understanding:\nHow long is the queue at any point in the paper? Does it introduce too much cognitive load? How long is the distance between the appearance of an element in the queue? (the interrogation moment) and its removal (the aha moment) The second point is easy to solve, just recommend introducing the concept before the place in the text where the interrogation appeared. The first point will require more work on the authors’ side to displace all explanations before the introduction of the concept/symbol, reducing the overall cognitive load at any moment for the reader.\nThematic read \u0026amp; writing the recommendations After the first reading round, I usually have some …","date":1549238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"0b3b6c2f96ae5e7f8715532968f1fe97","permalink":"https://matbesancon.xyz/post/2019-02-04-article-review/","publishdate":"2019-02-04T00:00:00Z","relpermalink":"/post/2019-02-04-article-review/","section":"post","summary":"Gathering some thoughts on what worked and what did not.\n","tags":["academia"],"title":"A naive and incomplete guide to peer-review","type":"post"},{"authors":["Mathieu Besançon","Miguel F. Anjos","Luce Brotcorne","Juan A. Gomez-Herrera"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"2a219b751b5bf0b055c57ac3aee3399f","permalink":"https://matbesancon.xyz/publication/conference/besancon-2018/","publishdate":"2023-12-22T16:16:30.909008Z","relpermalink":"/publication/conference/besancon-2018/","section":"publication","summary":"","tags":[],"title":"A Bilevel Framework for Optimal Price-Setting of Time-and-Level-of-Use Tariffs","type":"publication"},{"authors":["Mathieu Besançon"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"d5519464da48a55551c70f2748094830","permalink":"https://matbesancon.xyz/publication/journal/besanccon-2019-julia/","publishdate":"2023-12-22T16:16:30.033072Z","relpermalink":"/publication/journal/besanccon-2019-julia/","section":"publication","summary":"","tags":[],"title":"A Julia package for bilevel optimization problems","type":"publication"},{"authors":["Mathieu Besançon","Miguel F Anjos","Luce Brotcorne"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703357026,"objectID":"f14310ef9f5641976ab934edffef7a5e","permalink":"https://matbesancon.xyz/publication/preprint/besanccon-2019-near/","publishdate":"2023-12-22T16:16:31.515651Z","relpermalink":"/publication/preprint/besanccon-2019-near/","section":"publication","summary":"","tags":[],"title":"Near-optimal robust bilevel optimization","type":"publication"},{"authors":null,"categories":null,"content":"Enjoying the calm of the frozen eastern French countryside for the last week of 2018, I was struck by nostalgia while reading a SIAM news article [1] on a near-reversible heat exchange between two flows and decided to dust off my thermodynamics books (especially [2]).\nResearch in mathematical optimization was not the obvious path I was on a couple years ago. The joint bachelor-master’s program I followed in France was in process engineering, a discipline crossing transfer phenomena (heat exchange, fluid mechanics, thermodynamics), control, knowledge of the matter transformations at hand (chemical, biochemical, nuclear reactions) and industrial engineering (see note at the end of this page).\nHypotheses Throughout the article, we will use a set of flow hypotheses which build up the core of our model for heat exchange. These can seem odd but are pretty common in process engineering and realistic in many applications.\nThe two flows advance in successive “layers”. Each layer has a homogeneous temperature; we therefore ignore boundary layer effects. Successive layers do not exchange matter nor heat. The rationale behind this is that the temperature difference between fluids is significantly higher than between layers. Pressure losses in the exchanger does not release a significant heat compared to the fluid heat exchange. The fluid and wall properties are constant with temperature. Starting simple: parallel flow heat exchange In this model, both flows enter the exchanger on the same side, one at a hot temperature, the other at a cold temperature. Heat is exchanged along the exchanger wall, proportional at any point to the difference in temperature between the two fluids. We therefore study the evolution of two variables $u_1(x)$ and $u_2(x)$ in an interval $x \\in [0,L]$ with $L$ the length of the exchanger.\nIn any layer $[x, x + \\delta x]$, the heat exchange is equal to: $$\\delta \\dot{Q} = h \\cdot (u_2(x) - u_1(x)) \\cdot \\delta x$$ with $h$ a coefficient depending on the wall heat exchange properties.\nMoreover, the variation in internal energy of the hot flow is equal to $\\delta \\dot{Q}$ and is also expressed as:\n$$ c_2 \\cdot \\dot{m}_2 \\cdot (u_2(x+\\delta x) - u_2(x)) $$ $c_2$ is the calorific capacity of the hot flow and $\\dot{m}_2$ its mass flow rate. The you can check that the given expression is a power. The same expressions apply to the cold flow. Let us first assume the following:\n$$c_2 \\cdot \\dot{m}_2 = c_1 \\cdot \\dot{m}_1$$\nimport DifferentialEquations const DiffEq = DifferentialEquations using Plots function parallel_exchanger(du,u,p,x) h = p[1] # heat exchange coefficient Q = h * (u[1]-u[2]) du[1] = -Q du[2] = Q end function parallel_solution(L, p) problem = DiffEq.ODEProblem( parallel_exchanger, # function describing the dynamics of the system u₀, # initial conditions u0 (0., L), # region overwhich the solution is built, x ∈ [0,L] p, # parameters, here the aggregated transfer constant h ) return DiffEq.solve(problem, DiffEq.Tsit5()) end plot(parallel_solution([0.0,100.0], 50.0, (0.05))) $$ u_1(x) = T_{eq} \\cdot (1 - e^{-h\\cdot x}) $$ $$ u_2(x) = (100 - T_{eq}) \\cdot e^{-h\\cdot x} + T_{eq} $$\nWith $T_{eq}$ the limit temperature, trivially 50°C with equal flows.\n(Full disclaimer: I’m a bit rusty and had to double-check for errors)\nThis model is pretty simple, its performance is however low from a practical perspective. First on the purpose itself, we can compute for two fluids the equilibrium temperature. This temperature can be adjusted by the ratio of two mass flow rates but will remain a weighted average. Suppose the goal of the exchange is to heat the cold fluid, the necessary mass flow $\\dot{m}_2$ tends to $\\infty$ as the targeted temperature tends to $u_2(L)$, and this is independent of the performance of the heat exchanger itself, represented by the coefficient $h$. Here is the extended model using the flow rate ratio to adjust the temperature profiles:\nimport DifferentialEquations const DiffEq = DifferentialEquations function ratio_exchanger(du,u,p,x) h = p[1] # heat exchange coefficient r = p[2] # ratio of mass flow rate 2 / mass flow rate 1 Q = h * (u[1]-u[2]) du[1] = -Q du[2] = Q / r end function ratio_solution(u₀, L, p) problem = DiffEq.ODEProblem( ratio_exchanger, # function describing the dynamics of the system u₀, # initial conditions u0 (0., L), # region overwhich the solution is built, x ∈ [0,L] p, # parameters, here the aggregated transfer constant h ) return DiffEq.solve(problem, DiffEq.Tsit5()) end for (idx,r) in enumerate((1.0, 5.0, 10.0, 500.0)) plot(ratio_solution([0.0,100.0], 50.0, (0.05, r))) xlabel!(\u0026#34;x (m)\u0026#34;) ylabel!(\u0026#34;T °C\u0026#34;) title!(\u0026#34;Parallel flow with ratio $r\u0026#34;) savefig(\u0026#34;parallel_ratio_$(idx).pdf\u0026#34;) end This model has an analytical closed-form solution given by: $$ T_{eq} = \\frac{100\\cdot \\dot{m}_2}{\\dot{m}_1 + \\dot{m}_2} = 100\\cdot\\frac{r}{1+r} $$ $$ u_1(x) = T_{eq} \\cdot (1 - e^{-h\\cdot x}) $$ $$ u_2(x) = (100 - T_{eq}) \\cdot e^{-h\\cdot x \\cdot r} + T_{eq} $$\nOpposite flow model …","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"ca4d982c3fd583c60e36f0709b8f6687","permalink":"https://matbesancon.xyz/post/2018-12-27-heat-exchanger/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/post/2018-12-27-heat-exchanger/","section":"post","summary":"\n","tags":["engineering","julia"],"title":"Winter warm-up: toy models for heat exchangers","type":"post"},{"authors":null,"categories":null,"content":" So, it’s been a bit more than a year since I took a flight to Montréal to start a PhD on mathematical optimization \u0026amp; game theory for smart grids.\nAfter the rush of summer conferences, and my return to France for a part of my PhD at INRIA Lille, it is a good time to take a step back and think of what has been going on and what is coming next. I’ll also answer some common questions I had in a more thoughtful way than I can in a bar conversation/family dinner. Maybe this can also help other PhD students seeing we are not in the same boat, but they all still look alike.\nTL;DR: a PhD is not studies in the sense you think, and it’s not a job either, these mental models will not help much.\nSo, when are you going to finish? I don’t know, when are you going to finish your job? It doesn’t look like it’s been moving that much recently. Or when will this company you’re building be “finished”?\nThese questions are similar, really. A research subject is rarely isolated, don’t see this as emptying a 4-year bowl of soup. It’s more like picking berries: you grab one, then the next, which is close enough, oh and this one is nice, but a bit further, I’ll have to stretch my arm a bit more to reach it.\n[1]\nI had some interesting discussions in Montréal about when and how to know you should bring your PhD to a conclusion. And the answer should always be that it depends what your objectives are, if you want to include this last project in the PhD. So no, I don’t know when I will finish, because if every step was predictable in terms of duration and success, it would not be a PhD or even research, what I do know is that I don’t want to block interesting projects or leave only half-explored research trails because “3 years is plenty”.\nIt must feel weird, getting back to university It does, but not how you imagine. I was previously at a startup for a while. What I was used to is a great autonomy in execution. What the PhD is about is adding self-determination of the objectives, expected results, and means. It does not mean I’m working alone while I was in a team before, it means the degree of ownership of successes and failures is much higher, try to picture the three following sentences in a conversation:\n“I was at a startup before, it failed and I moved on to XYZ.” “I started a PhD but didn’t get through, then moved on to XYZ.” “I built a company, it failed, now I’m working on XYZ.” It depends on the relationship to failure the person has in front of you, but for those I know, (1) is just an external cause, while (2) and (3) are personal failures, that’s ownership.\nThe biggest conclusion I made roughly after 6 months in is that a single-founder startup is one of the closest mental models to keep during the PhD, which explains several things, like inability to explain exactly what you do to your family and friends, imposter syndrome or procrastination.\nSo you get paid enough to buy noodles? Yes, I’m living quite well thanks, I can even afford fancying my noodles, but let’s dig deeper on the matter of €/$/£.\nDisclaimer: my PhD is between applied maths \u0026amp; computer science, I know all majors are not that financially comfortable.\n[2]\nI also know it’s considered rude to talk about money in some cultures, including France, especially if you’re not complaining; so yes, I’ll be rude.\nWhen I’m in France, I’m paid slightly less than some engineers with the same level of qualification. The difference is higher if I’m comparing to what I would have had on Data Science, applied maths and software development positions. The difference between what I would earn and the scholarship is higher in Canada. Still, like I said, I can live without watching my bank account towards the end of the month.\nThe biggest danger of getting money monthly is thinking of it as a salary, meaning you’re thinking of the PhD as a job, meaning you’re thinking of yourself as an employee. On the paper, the money I get in France is a salary from my research institute, but one should keep in mind this is only on paper, the danger is to get the wrong mindset: think of yourself as a single person carrying a project, not an employee.\nPeople often argue that they have a research director, who is de facto their boss. I don’t think this is the case, directors choose project proposals and people to carry them out (the order of this choice varies). They choose to invest time, effort and money from their structure into this person+project pair, without dictating to the letter what the outcomes of the projects are. Their retribution for this exchange is a partial ownership in the outcomes of the project (publications, conferences, software). Sounds familiar? Yes I’m looking at the Wikipedia page on Venture Capital. Let’s dig deeper: thesis directors invest this time, money and energy in areas they are familiar with, they have worked in and/or have mentored other people on. This sounds like the VC firms’ investment theses. Read these two articles to see a more proper definition and …","date":1538006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"b38df74c3c3465b7fdaf47773d586230","permalink":"https://matbesancon.xyz/post/2018-09-27-year-in-phd/","publishdate":"2018-09-27T00:00:00Z","relpermalink":"/post/2018-09-27-year-in-phd/","section":"post","summary":"\n","tags":["phd","academia"],"title":"A year in PhD","type":"post"},{"authors":null,"categories":null,"content":" This is an adapted post on the talk we gave with James at JuliaCon 2018 in London. You can see the original slides, the video still requires a bit of post-processing.\nLast week JuliaCon in London was a great and very condensed experience. The two talks on LightGraphs.jl received a lot of positive feedback and more than that, we saw how people are using the library for a variety of use cases which is a great signal for the work on the JuliaGraphs ecosystem (see the lightning talk).\nI wanted to re-build the same graph for people who prefer a post version to my clumsy live explanations on a laptop not handling dual-screen well (those who prefer the latter are invited to see the live-stream of the talk).\nWhy abstractions? The LightGraphs library is built to contain as few elements as possible to get anyone going with graphs. This includes:\nThe interface a graph type has to comply with to be used Essential algorithms implemented by any graph respecting that interface A simple, battery-included implementation based on adjacency lists The thing is, if you design an abstraction which in fact has just one implementation, you’re doing abstraction wrong. This talks was also a reality-check for LightGraphs, are we as composable, extensible as we promised?\nThe reason for abstraction is also that minimalism has its price. The package was designed as the least amount of complexity required to get graphs working. When people started to use it, obviously they needed more features, some of which they could code themselves, some other required extensions built within LightGraphs. By getting the core abstractions right, you guarantee people will be able to use it and to build on top with minimal friction, while keeping it simple to read and contribute to.\nOur matrix graph type Let’s recall that a graph is a collection of nodes and a collection of edges between these nodes. To keep it simple, for a graph of $n$ edges, we will consider they are numbered from 1 to n. An edge connects a node $i$ to a node $j$, therefore all the information of a graph can be kept as an adjacency matrix $M_{ij}$ of size $n \\times n$:\n$$M_{ij} = \\begin{cases} 1, \u0026amp; \\mbox{if edge (i $\\rightarrow$ j) exists} \\\\ 0 \u0026amp; \\mbox{otherwise}\\end{cases}$$\nWe don’t know what the use cases for our type will be, and therefore, we will parametrize the graph type over the matrix type:\nimport LightGraphs; const lg = LightGraphs mutable struct MatrixDiGraph{MT \u0026lt;: AbstractMatrix{Bool}} \u0026lt;: lg.AbstractGraph{Int} matrix::MT end The edges are simply mapping an entry (i,j) to a boolean (whether there is an edge from i to j). Even though creating a graph type that can be directed or undirected depending on the situation is possible, we are creating a type that will be directed by default.\nImplementing the core interface We can now implement the core LightGraphs interface for this type, starting with methods defined over the type itself, of the form function(g::MyType)\nI’m not going to re-define each function here, their meaning can be found by checking the help in a Julia REPL: ?LightGraphs.vertices or on the documentation page.\nlg.is_directed(::MatrixDiGraph) = true lg.edgetype(::MatrixDiGraph) = lg.SimpleGraphs.SimpleEdge{Int} lg.ne(g::MatrixDiGraph) = sum(g.m) lg.nv(g::MatrixDiGraph) = size(g.m)[1] lg.vertices(g::MatrixDiGraph) = 1:nv(g) function lg.edges(g::MatrixDiGraph) n = lg.nv(g) return (lg.SimpleGraphs.SimpleEdge(i,j) for i in 1:n for j in 1:n if g.m[i,j]) end Note the last function edges, for which the documentation specifies that we need to return an iterator over edges. We don’t need to collect the comprehension in a Vector, returning a lazy generator.\nSome operations have to be defined on both the graph and a node, of the form function(g::MyType, node). lg.outneighbors(g::MatrixDiGraph, node) = [v for v in 1:lg.nv(g) if g.m[node, v]] lg.inneighbors(g::MatrixDiGraph, node) = [v for v in 1:lg.nv(g) if g.m[v, node]] lg.has_vertex(g::MatrixDiGraph, v::Integer) = v \u0026lt;= lg.nv(g) \u0026amp;\u0026amp; v \u0026gt; 0\nOut MatrixDiGraph type is pretty straight-forward to work with and all required methods are easy to relate to the way information is stored in the adjacency matrix.\nThe last step is implementing methods on both the graph and an edge of the form function(g::MatrixDiGraph,e). The only one we need here is: lg.has_edge(g::MatrixDiGraph,i,j) = g.m[i,j]\nOptional mutability Mutating methods were removed from the core interace some time ago, as they are not required to describe a graph-like behavior. The general behavior for operations mutating a graph is to return whether the operation succeded. They consist in adding or removing elements from either the edges or nodes.\nimport LightGraphs: rem_edge!, rem_vertex!, add_edge!, add_vertex! function add_edge!(g::MatrixDiGraph, e) has_edge(g,e) \u0026amp;\u0026amp; return false n = nv(g) (src(e) \u0026gt; n || dst(e) \u0026gt; n) \u0026amp;\u0026amp; return false g.m[src(e),dst(e)] = true end function rem_edge!(g::MatrixDiGraph,e) has_edge(g,e) || return false n = nv(g) (src(e) \u0026gt; n || dst(e) \u0026gt; n) \u0026amp;\u0026amp; …","date":1534464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"4ea0bcff6dde362d969de51cdf3b5768","permalink":"https://matbesancon.xyz/post/2018-08-17-abstract_graph/","publishdate":"2018-08-17T00:00:00Z","relpermalink":"/post/2018-08-17-abstract_graph/","section":"post","summary":"Who needs libraries when from scratch looks so good\n","tags":["julia","graph","package","interface"],"title":"Building our own graph type in Julia","type":"post"},{"authors":null,"categories":null,"content":" In the previous post, we explored a well-known integer optimization situation in manufacturing, the cutting stock problem. After some details on the decisions, constraints and objectives, we implemented a naive model in JuMP.\nOne key thing to notice is the explosion of number of variables and constraints and the fact that relaxed solutions (without constraining variables to be integers) are very far from actual feasible solutions.\nWe will now use an other way of formulating the problem, using a problem decomposition and an associated solution method (column generation).\nRe-stating the cutting stock problem Remember we used two decisions: $Y_i$ stating if the big roll $i$ is used and $X_{ij}$ expressing the number of cuts $j$ made in the roll $i$. To minimize the number of rolls, it makes sense to put as many small cuts as possible on a big roll. We could therefore identify saturating patterns, that is, a combination of small cuts fitting on a big roll, such that no additional cut can be placed, and then find the smallest combination of the pattern satisfying the demand.\nOne problem remains: it is impossible to compute, or even to store in memory all patterns, their number is exponentially big with the number of cuts, so we will try to find the best patterns and re-solve the problem, using the fact that not all possible patterns will be necessary.\nThis is exactly what the Dantzig-Wolfe decomposition does, it splits the problem into a Master Problem MP and a sub-problem SP.\nThe Master Problem, provided a set of patterns, will find the best combination satisfying the demand. The sub-problem, given an “importance” of each cut provided by the master problem, will find the best cuts to put on a new pattern. This is an iterative process, we can start with some naive patterns we can think of, compute an initial solution for the master problem, which will be feasible but not optimal, move on to the sub-problem to try to find a new pattern (or column in the optimization jargon, hence the term of column generation).\nHow do we define the “importance” of a cut $j$? The value of the dual variable associated with this constraint will tell us that. This is not a lecture in duality theory, math-eager readers can check out further documentation on the cutting stock problem and duality in linear optimization.\nMoreover, we are going to add one element to our model: excess cuts can be sold at a price $P_j$, so that we can optimize by minimizing the net cost (production cost of the big rolls minus the revenue from excess cuts).\nNew formulation Again, we are going to formulate first possible decisions and then constraints on these decisions for the new version of the problem.\nDecisions At the master problem level, given a pattern $p$, the decision will be $\\theta_p$ (theta, yes Greek letters are awesome), the number of big rolls which will be used with this pattern. $\\theta_p$ is a positive integer.\nThe decision at the sub-problem level will be to find how many of each cut $j$ to fit onto one big roll, $a_j$.\nFor a pattern $p$, the number of times a cut $j$ appears is given by $a_{jp}$.\nConstraints The big roll size constraint is kept in the sub-problem, a pattern built has to respect this constraint: $$ \\sum_j a_{j} \\cdot W_j \\leq L $$\nThe demand $D_j$ is met with all rolls of each pattern so it is kept at the master level. The number of cuts of type $j$ produced is the sum of the number of this cut on each patterns times the number of the pattern in a solution:\n$$ NumCuts_j = \\sum_p a_{jp} \\cdot \\theta_p \\geq D_j$$\nObjective formulation At the master problem, we minimize the number of rolls, which is simply: $$ \\sum_{p} \\theta_p $$\nAt the sub-problem, we are trying to maximize the gain associated with the need for the demand + the residual price of the cuts. If we can find a worth using producing compared to its production cost, it is added.\nImplementation As before, we will formulate the master and sub-problem using Julia with JuMP. Again, we use the Clp and Cbc open-source solvers. We read the problem data (prices, sizes, demand) from a JSON file.\nusing JuMP using Cbc: CbcSolver using Clp: ClpSolver import JSON const res = open(\u0026#34;data0.json\u0026#34;, \u0026#34;r\u0026#34;) do f data = readstring(f) JSON.Parser.parse(data) end const maxwidth = res[\u0026#34;maxwidth\u0026#34;] const cost = res[\u0026#34;cost\u0026#34;] const prices = Float64.(res[\u0026#34;prices\u0026#34;]) const widths = Float64.(res[\u0026#34;widths\u0026#34;]) const demand = Float64.(res[\u0026#34;demand\u0026#34;]) const nwidths = length(prices) cost is the production cost of a big roll.\nSub-problem The subproblem is a function taking reduced costs of each cut and maximizing the utility of the pattern it creates:\n\u0026#34;\u0026#34;\u0026#34; subproblem tries to find the best feasible pattern maximizing reduced cost and respecting max roll width corresponding to a multiple-item knapsack \u0026#34;\u0026#34;\u0026#34; function subproblem(reduced_costs, sizes, maxcapacity) submodel = Model(solver = CbcSolver()) n = length(reduced_costs) xs = @variable(submodel, xs[1:n] \u0026gt;= 0, Int) @constraint(submodel, sum(xs. * sizes) \u0026lt;= …","date":1527638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"7f6ad8e0dcd8dac56b21e206602010e6","permalink":"https://matbesancon.xyz/post/2018-05-25-colgen2/","publishdate":"2018-05-30T00:00:00Z","relpermalink":"/post/2018-05-25-colgen2/","section":"post","summary":"A column generation algorithm for the cutting width problem using Julia and JuMP\n","tags":["julia","optimization","integer-optimization","jump"],"title":"The cutting stock problem: part 2, solving with column generation","type":"post"},{"authors":null,"categories":null,"content":" Integer optimization often feels weird (at least to me). Simple reformulations of a (mixed) integer optimization problem (MIP) can make it way easier to solve. We’re going to explore one well-known example of such integer problem in two blog posts. This first part introduces the problem and develops a naive solution. We’re going to see why it’s complex to solve and why this formulation does not scale.\nIn a second post, we will see a reformulation of the problem which makes it easier to solve and scales to bigger instances.\nInteger optimization reminder An optimization problem takes three components: decisions variables $x$, a set of constraints telling you if a decision is feasible or not and a cost function $c(x)$ giving a total cost of a decision. Optimization is a domain of applied mathematics consisting in finding the best feasible decision for a problem. Lots of decision problems come with integrality constraints: if $x$ is the decision, then it can only take integer values 0,1,2… or even only binary values ${0,1}$. Think of problems involving number of units produced for a good, yes/no decisions, etc… If a problem has lots of variables, naive enumerations of feasible solutions becomes impossible: even problems with 50 variables can make your average laptop crash.\nThe cutting stock problem The problem is not new and has been given quite some thoughts because of its different industrial applications, it has been one of the first applications of the column generation method we are going to use. The key elements of the problems are: given some large rolls (metal, paper or other), we need to cut smaller portions of given lengths to satisfy a demand for the different small lengths. Find more details here. A small instance might be: given rolls of size $100cm$, we want to cut at least 7 rolls of size $12cm$ and 9 rolls of size $29cm$. The objective is to minimize the number of big rolls to satisfy this demand.\nHow do we formulate this mathematically?\nDecisions $Y_i$ is a binary decision indicating if we use the big roll number $i$. $X_{ij}$ is an integer giving the number of times we cut a small roll $j$ in the big roll $i$.\nConstraints $Y$ are binary variables, $X$ are integer. Now the less trivial constraints:\nDemand satisfaction constraint: the sum over all $i$ big rolls of the cut $j$ has to satisfy the demand for that cut: $$\\sum_{i} X_{ij} \\geq D_j $$ For the two-cut example with the demand of $7 \\times 12cm$ and $9 \\times 29cm$, let’s suppose we have 10 big rolls $i \\in {1…10}$, the demand for the first 12cm cut is 7 cuts, the number of cuts of this size produced is: $$ \\sum_i X_{i1} = X_{1,1} + X_{2,1} + … + X_{10,1}$$\nThis total must at least match the demand, so: $$ X_{1,1} + X_{2,1} + … + X_{10,1} \\geq 7 $$\nRoll size constraint: if a roll $i$ is used, we cannot fit more width onto it than its total width: $$\\sum_{j} X_{ij} \\cdot W_j \\leq L \\cdot Y_i $$ For the two-cut example with the demand of $7 \\times 12cm$ and $9 \\times 29cm$, let’s suppose we have one roll $i$:\nIf $Y_i = 0$, the roll size constraint becomes: $$ \\sum_{j} X_{ij} \\cdot W_j = 12 \\cdot X_{i1} + 29 \\cdot X_{i2} \\leq 0 $$\nThe only feasible solution for this roll $i$ is ($X_{i1} = 0,X_{i2} = 0$).\nIf $Y_i = 1$, the roll size constraint becomes: $$ 12 \\cdot X_{i1} + 29 \\cdot X_{i2} \\leq 100 $$ Which means we can fit as many cuts as the roll size allows for.\nA first naive implementation Let’s first import the necessary packages: we’re using JuMP as a modeling tool, which is an optimization-specific language embedded in Julia (compare it to AMPL, GAMS, Pyomo, PuLP). As I consider it an embedded language, I’ll do a full import into my namespace with using (unlike what I usually do with packages). We also use Cbc, an open-source solver for integer problems from the Coin-OR suite.\nusing JuMP using Cbc: CbcSolver We can define our optimization problem within a function taking the parameters of the cutting stock problem, namely a maxwidth of the big rolls, scalar assuming all of them have the same width, a widths vector, one element for each cut size $j$ and a demand vector, again, one for each cut size.\nfunction cutting_stock_model(maxwidth, widths, demand, N = sum(demand)) # Define the JuMP model m = Model(solver = CbcSolver()) # define the two groups of variables over their respective indices Y = @variable(m, Y[1:N],Bin) X = @variable(m, X[i=1:N,j=1:length(widths)],Int) # define both constraints and objective demand_satisfac = @constraint(m, [j=1:length(widths)], sum(X[i,j] for i in 1:N) \u0026gt;= demand[j] ) roll_size_const = @constraint(m, [i=1:N], sum(X[i,j] * widths[j] for j in 1:length(widths)) \u0026lt;= Y[i] * maxwidth ) @objective(m, Min, sum(Y[i] for i in 1:N)) # return the model formulation to solve later return m end Here $N$ has to be an upper bound on the number of big rolls to use, otherwise the problem will be infeasible (not enough big rolls to find a solution satisfying the demand). An initial naive value for this could be the total …","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"5d900ecf2db384340871f9103b8562ae","permalink":"https://matbesancon.xyz/post/2018-05-23-colgen/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/post/2018-05-23-colgen/","section":"post","summary":"Solving a cutting stock problem step by step using Julia and JuMP\n","tags":["julia","modeling","optimization","integer-optimization","jump"],"title":"Tackling the cutting stock problem: part 1, problem exploration","type":"post"},{"authors":null,"categories":null,"content":"With the end-of-year celebrations, we all had some expenses to manage, some of them shared with friends, and we all have this eternal problem of splitting them fairly.\nLes bons comptes font les bons amis. French wisdom\nApplications like Tricount or Splitwise became famous precisely by solving this problem for you: just enter the expenses one by one, with who owes whom and you’ll get the simplest transactions to balance the amounts at the end.\nIn this post, we’ll model the expense balancing problem from a graph perspective and see how to come up with a solution using Julia and the JuliaGraphs ecosystem [1].\nTable of Contents The expenses model Reducing expenses Breaking strongly connected components Expenses as a flow problem Conclusion, perspective and note on GPHC The expenses model Say that we have $n$ users involved in the expenses. An expense $\\delta$ is defined by an amount spent $\\sigma$, the user who paid the expense $p$ and a non-empty set of users who are accountable for this expense $a$.\n$\\delta = (\\sigma, p, a)$\nThe total of all expenses $\\Sigma$ can be though of as: for any two users $u_i$ and $u_j$, the total amount that $u_i$ spent for $u_j$. So the expenses are a vector of triplets (paid by, paid for, amount).\nAs an example, if I went out for pizza with Joe and paid 8GPHC for the two of us, the expense is modeled as:\n$\\delta = (\\sigma: 8GPHC, p: Mathieu, a: [Mathieu, Joe])$.\nNow considering I don’t keep track of money I owe myself, the sum of all expenses is the vector composed of one triplet:\n$\\Sigma = [(Mathieu, Joe, \\frac{8}{2} = 4)]$\nIn Julia, the expense information can be translated to a structure: const User = Int const GraphCoin = Float16 struct Expense payer::User amount::GraphCoin users::Set{User} end\nReducing expenses Now that we have a full representation of the expenses, the purpose of balancing is to find a vector of transactions which cancels out the expenses. A naive approach would be to use the transposed expense matrix as a transaction matrix. If $u_i$ paid $\\Sigma_{i,j}$ for $u_j$, then $u_j$ paying back that exact amount to $u_i$ will solve the problem. So we need in the worst case as many transactions after the trip as $|u| \\cdot (|u| - 1)$. For 5 users, that’s already 20 transactions, how can we improve it?\nBreaking strongly connected components Suppose that I paid the pizza slice to Joe for 4GPHC, but he bought me an ice cream for 2GPHC the day after. In the naive models, we would have two transactions after the trip: he give me 4GPHC and I would give him 2GPHC. That does not make any sense, he should simply pay the difference between what he owes me and what I owe him. For any pair of users, there should only be at most one transaction from the most in debt to the other, this result in the worst case of $\\frac{|u| \\cdot (|u| - 1)}{2}$ transactions, so 10 transactions for 5 people.\nNow imagine I still paid 4GPHC for Joe, who paid 2GPHC for Marie, who paid 4GPHC for me. In graph terminology, this is called a strongly connected component. The point here is that transactions will flow from one user to the next one, and back to the first.\nIf there is a cycle, we can find the minimal due sum within it. In our 3-people case, it is 2GPHC. That’s the amount which is just moving from hand to hand and back at the origin: it can be forgotten. This yields a new net debt: I paid 2GPHC for Joe, Marie paid 2GPHC for me. We reduced the number of transactions and the amount due thanks to this cycle reduction.\nExpenses as a flow problem To simplify the problem, we can notice we don’t actually care about who paid whom for what, a fair reimbursement plan only requires two conditions:\nAll people who are owed some money are given at least that amount People who owe money don’t pay more than the net amount they ought to pay We can define a directed flow network with users split in two sets of vertices, depending on whether they owe or are owed money. We call these two sets $V_1$ and $V_2$ respectively.\nThere is an edge from any node of $V_1$ to any node of $V_2$. We define a source noted $s$ connected to all vertices in $V_1$, the edge from $s$ to any node of $V_1$ has a capacity equal to what they owe. We define a sink noted $t$ to which all vertices in $V_2$ connect, with infinite capacity and a demand (the minimal flow that has to pass through) equal to what they are owed. With this model, GraphCoins will flow from user owing money to users who are owed money, see Wikipedia description of the flow problem.\nComputing net owed amount per user Given a vector of expenses, we should be able to build the matrix holding what is owed in net from a user to another:\n\u0026#34;\u0026#34;\u0026#34; Builds the matrix of net owed GraphCoins \u0026#34;\u0026#34;\u0026#34; function compute_net_owing(expenses::Vector{Expense}, nusers::Int) owing_matrix = zeros(GraphCoin, nusers, nusers) # row owes to column for expense in expenses for user in expense.users if user != expense.payer owing_matrix[user,expense.payer] += expense.amount / length(expense.users) end …","date":1515974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698166146,"objectID":"5dd7446f6cc4ddd5c173d6d1f089ef04","permalink":"https://matbesancon.xyz/post/2017-09-11-graph-theory-expenses-management/","publishdate":"2018-01-15T00:00:00Z","relpermalink":"/post/2017-09-11-graph-theory-expenses-management/","section":"post","summary":"Graph theory and Julia to solve the boring aspect of having friends\n","tags":["graph","julia"],"title":"Solving the group expenses headache with graphs","type":"post"},{"authors":null,"categories":null,"content":"In the last article, we explored different modeling options for a three-component systems which could represent the dynamics of a chemical reaction or a disease propagation in a population. Building on top of this model, we will formulate a desirable outcome and find a decision which maximizes this outcome.\nIn addition to the packages imported in the last post, we will also use BlackBoxOptim.jl:\nimport DifferentialEquations const DiffEq = DifferentialEquations import Plots import Optim The model The same chemical system with three components, A, B and R will be used: $$A + B → 2B$$ $$B → R$$\nThe reactor where the reaction occurs must remain active for one minute. Let’s imagine that $B$ is our valuable component while $R$ is a waste. We want to maximize the quantity of $B$ present within the system after one minute, that’s the objective function. For that purpose, we can choose to add a certain quantity of new $A$ within the reactor at any point. $$t_{inject} ∈ [0,t_{final}]$$.\nImplementing the injection There is one major feature of DifferentialEquations.jl we haven’t explored yet: the event handling system. This allows for the system state to change at a particular point in time, depending on conditions on the time, state, etc…\n# defining the problem const α = 0.8 const β = 3.0 diffeq = function(du, u, p, t) du[1] = - α * u[1] * u[2] du[2] = α * u[1] * u[2] - β * u[2] du[3] = β * u[2] end u0 = [49.0;1.0;0.0] tspan = (0.0, 1.0) prob = DiffEq.ODEProblem(diffeq, u0, tspan) const A_inj = 30 inject_new = function(t0) condition(u, t, integrator) = t0 - t affect! = function(integrator) integrator.u[1] = integrator.u[1] + A_inj end callback = DiffEq.ContinuousCallback(condition, affect!) sol = DiffEq.solve(prob, callback=callback) sol end # trying it out with an injection at t=0.4 sol = inject_new(0.4) Plots.plot(sol) The ContinuousCallback construct is the central element here, it takes as information:\nWhen to trigger the event, implemented as the condition function. It triggers when this function reaches 0, which is here the case when $t = t_0$. What to do with the state at that moment. The state is encapsulated within the integrator variable. In our case, we add 30 units to the concentration in A. As we can see on the plot, a discontinuity appears on the concentration in A at the injection time, the concentration in B restarts increasing.\nFinding the optimal injection time: visual approach From the previously built function, we can get the whole solution with a given injection time, and from that the final state of the system.\ntinj_span = 0.05:0.005:0.95 final_b = [inject_new(tinj).u[end][2] for tinj in tinj_span] Plots.plot(tinj_span, final_b) Using a plain for comprehension, we fetch the solution of the simulation for the callback built with each $t_{inject}$.\nInjecting $A$ too soon lets too much time for the created $B$ to turn into $R$, but injecting it too late does not let enough time for $B$ to be produced from the injected $A$. The optimum seems to be around ≈ 0.82,\nFinding the optimum using Optim.jl The package requires an objective function which takes a vector as input. In our case, the decision is modeled as a single variable (the injection time), it’s crucial to make the objective use a vector nonetheless, otherwise calling the solver will just explode with cryptic errors.\ncompute_finalb = tinj -\u0026gt; -1 * inject_new(tinj[1]).u[end][2] Optim.optimize(compute_finalb, 0.1, 0.9) We get a detailed result of the optimization including the method and iterations:\n* Algorithm: Brent\u0026#39;s Method * Search Interval: [0.100000, 0.900000] * Minimizer: 8.355578e-01 * Minimum: -2.403937e+01 * Iterations: 13 * Convergence: max(|x - x_upper|, |x - x_lower|) \u0026lt;= 2*(1.5e-08*|x|+2.2e-16): true * Objective Function Calls: 14 The function inject_new we defined above returns the complete solution of the simulation, we get the state matrix u, from which we extract the final state u[end], and then the second component, the concentration in B: u[end][2]. The optimization algorithm minimizes the objective, while we want to maximize the final concentration of B, hence the -1 multiplier used for\ncompute_finalb.\nWe can use the Optim.jl package because our function is twice differentiable, the best improvement direction is easy to compute.\nExtending the model The decision over one variable was pretty straightforward. We are going to extend it by changing how the $A$ component is added at $t_{inject}$. Instead of being completely dissolved, a part of the component will keep being poured in after $t_{inject}$. So the decision will be composed of two variables:\nThe time of the beginning of the injection The part of $A$ to inject directly and the part to inject in a continuous fashion. We will note the fraction injected directly $\\delta$. Given a fixed available quantity $A₀$ and a fraction to inject directly $\\delta$, the concentration in A is increased of $\\delta \\cdot A₀$ at time $t_{inject}$, after which the rate of change of the …","date":1513728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"0bb6cb3812eac858d8ee997b6b289fc4","permalink":"https://matbesancon.xyz/post/2017-12-20-diffeq-julia2/","publishdate":"2017-12-20T00:00:00Z","relpermalink":"/post/2017-12-20-diffeq-julia2/","section":"post","summary":"Now that we've built a model, let's use it to make the best decision\n","tags":["julia","modeling","numerical-techniques","applied-math","optimization"],"title":"DifferentialEquations.jl - part 2: decision from the model","type":"post"},{"authors":null,"categories":null,"content":"DifferentialEquations.jl came to be a key component of Julia’s scientific ecosystem. After checking the JuliaCon talk of its creator, I couldn’t wait to start building stuff with it, so I created and developed a simple example detailed in this blog post. Starting from a basic ordinary differential equation (ODE), we add noise, making it stochastic, and finally turn it into a discrete version.\nBefore running the code below, two imports will be used:\nimport DifferentialEquations const DiffEq = DifferentialEquations import Plots I tend to prefer explicit imports in Julia code, it helps to see from which part each function and type comes. As DifferentialEquations is longuish to write, we use an alias in the rest of the code.\nThe model We use a simple 3-element state in a differential equation. Depending on your background, pick the interpretation you prefer:\nAn SIR model, standing for susceptible, infected, and recovered, directly inspired by the talk and by the Gillespie.jl package. We have a total population with healthy people, infected people (after they catch the disease) and recovered (after they heal from the disease).\nA chemical system with three components, A, B and R. $$A + B → 2B$$ $$B → R$$\nAfter searching my memory for chemical engineering courses and the universal source of knowledge, I could confirm the first reaction is an autocatalysis, while the second is a simple reaction. An autocatalysis means that B molecules turn A molecules into B, without being consumed.\nThe first example is easier to represent as a discrete problem: finite populations make more sense when talking about people. However, it can be seen as getting closer to a continuous differential equation as the number of people get higher. The second model makes more sense in a continuous version as we are dealing with concentrations of chemical components.\nA first continuous model Following the tutorials from the official package website, we can build our system from:\nA system of differential equations: how does the system behave (dynamically) Initial conditions: where does the system start A time span: how long do we want to observe the system The system state can be written as: $$u(t) = \\begin{bmatrix} u₁(t) \\ u₂(t) \\ u₃(t)\n\\end{bmatrix}^T $$\nWith the behavior described as: $$ \\dot{u}(t) = f(u,t) $$ And the initial conditions $u(0) = u₀$.\nIn Julia with DifferentialEquations, this becomes: α = 0.8 β = 3.0 function diffeq(du, u, p, t) du[1] = - α * u[1] * u[2] du[2] = α * u[1] * u[2] - β * u[2] du[3] = β * u[2] end u₀ = [49.0;1.0;0.0] tspan = (0.0, 1.0)\ndiffeq models the dynamic behavior, u₀ the starting conditions and tspan the time range over which we observe the system evolution. Note that the diffeq function also take a p argument for parameters, in which we could have stored $\\alpha$ and $\\beta$.\nWe know that our equation is smooth, so we’ll let DifferentialEquations.jl figure out the solver. The general API of the package is built around two steps:\nBuilding a problem/model from behavior and initial conditions Solving the problem using a solver of our choice and providing additional information on how to solve it, yielding a solution. prob = DiffEq.ODEProblem(diffeq, u₀, tspan) sol = DiffEq.solve(prob) One very nice property of solutions produced by the package is that they contain a direct way to produce plots. This is fairly common in Julia to implement methods from other packages, here the ODESolution type implements Plots.plot:\nPlots.plot(sol) If we use the disease propagation example, $u₁(t)$ is the number of healthy people who haven’t been infected. It starts high, which makes the rate of infection by the diseased population moderate. As the number of sick people increases, the rate of infection increases: there are more and more possible contacts between healthy and sick people.\nAs the number of sick people increases, the recovery rate also increases, absorbing more sick people. So the “physics” behind the problem makes sense with what we observe on the curve.\nA key property to notice is the mass conservation: the sum of the three elements of the vector is constant (the total population in the health case). This makes sense from the point of view of the equations: $$\\frac{du₁}{dt} + \\frac{du₂}{dt} + \\frac{du_3}{dt} = 0$$\nAdding randomness: first attempt with a simple SDE The previous model works successfully, but remains naive. On small populations, the rate of contamination and recovery cannot be so smooth. What if some sick people isolate themselves from others for an hour or so, what there is a meeting organized, with higher chances of contacts? All these plausible events create different scenarios that are more or less likely to happen.\nTo represent this, the rate of change of the three variables of the system can be considered as composed of a deterministic part and of a random variation. One standard representation for this, as laid out in the package documentation is the following: $$ du = f(u,t) dt + ∑ gᵢ(u,t) dWᵢ …","date":1513209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"c5d2541b8ec153f68e77d373b80d0e1d","permalink":"https://matbesancon.xyz/post/2017-12-14-diffeq-julia/","publishdate":"2017-12-14T00:00:00Z","relpermalink":"/post/2017-12-14-diffeq-julia/","section":"post","summary":"Playing around with the differential equation solver turned simulation engine\n","tags":["julia","modeling","numerical-techniques","applied-math"],"title":"Getting started with DifferentialEquations.jl","type":"post"},{"authors":null,"categories":null,"content":"The start of my journey as a PhD student last September was a big step, but also an opportunity to review and improve my working habits. My day time had to be used properly, both for results’ sake and to be able to keep a balanced life.\nI had been introduced to the Pomodoro technique at Equisense (thanks Camille!) but remained skeptical as for its potential value within my work flow at the time.\nTo make it short, the technique consists in the following steps:\nDecide what task should be worked on. Allocate a given time to work (around 25 minutes) Set a timer and get to work When the time is up, make a short pause (~5 minutes), then repeat After 4 work sprints, take a longer break (~15-30 minutes) What was wrong with that? The development, test and operation phases were generally self-determining and lead to sprints from 20 to 120 minutes (that length isn’t surprising for some tasks and when highly focused). These were also often interrupted by team interactions (required concertation with members of the tech and product team, backend-specific collaborative problem-solving, …). The main point was that there are enough spontaneous interruptions of the work flow, no need to introduce an additional artificial one. As I look back, I still think this was a valid reason not to use this technique.\nWhat has changed? Time management as a grad student has to be un- and re-learned: rules are different, criteria for success change and so on.\nTime management seen by PhD comics [2]\nProblem structure: programming at a startup vs. applied math In my case, the major part of the workload switched from an implementation-heavy to a modeling-heavy context. As such, the work phases tend to be longer and with an heavier cognitive load. I am not saying that programming is easier, but I’m pretty sure mathematics almost always requires to keep more information in mind while working on a problem. Another opinion is that the part of instinct to find a path towards a solution is higher in mathematics.\nWhile programming, there are some key techniques that reduce the number of possible sources to a problem:\nGetting information on the state of the program at a given point (logging, debugging, printing to stdout) Testing the behavior of an isolated piece of the program with given input These techniques also work for scientific computing of course, but are harder to apply to both modeling and symbolic calculus, the different pieces of the problem have to be combined to find special structures which allow for a resolution. More solutions also tend to come while NOT looking at mathematical problem than for programming problems, where solutions come either in front of the code or when voluntarily thinking of the problem.\nTeam-dependent work vs. figure it out for yourself Most startups obviously value team work, it is one of the group skills that differentiate companies building great things from the ones stuck in an eternal early stage. This was even more true at Equisense where collaboration and product development were both very synchronous by modern standards. It had cons but ease two things:\nSpeed of product development. Lots of team under-estimate post-development coordination, the last meters of the sprint to have a feature ready Programming by constraints. Because of fast interactions between the people responsible for the different components, constraints from each one is quickly communicated and the modeling process is defined accounting for them right away. Now in research, especially in applied mathematics, the work is mostly independent, synchronization happens when working on a joined project for instance. This means that all the interruptions that were happening throughout the day are now gone! Nothing would stop you from working day and night without a break.\nConclusion Two key results of this change of work style are:\nWork sprints are not naturally bound anymore, obviously with decreasing efficiency Few to no interactions interrupt the sprints either My conclusion was the necessity of a time management technique and associated tools, with a low cognitive overhead and bringing as little distraction as possible.\nFrom these criteria, I rejected a mobile app, smartphones are great to bring different sources of information and communication channels together, not for remaining focused for hours, mobile apps are designed to catch and retain attention, that’s simply part of their business model. I also rejected web-based solutions for the constraint of firing up a browser, amongst the heaviest pieces of software on our modern desktops, just to start a working session.\nSo desktop GUI or CLI it is. Even though there is the gnomepomodoro project, it did not seem compatible with all Linux desktops. At that point, I realized the amount of work to build a Pomodoro was low, the requirements and constraints well known, I throw ideas together and start coding.\nI’ll explain the initial development and iterations of the app in Go in a second …","date":1508371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"f7564da8cfdbdb8403a8b8800b323b74","permalink":"https://matbesancon.xyz/post/2017-10-19-tomate-cli/","publishdate":"2017-10-19T00:00:00Z","relpermalink":"/post/2017-10-19-tomate-cli/","section":"post","summary":"Switching from data scientist to graduate student is not\nonly a variation in tasks, but also in success criteria and work flow\n","tags":["golang","phd","productivity"],"title":"Switching my work flow to Pomodoro for grad studies - part I: motivation  ","type":"post"},{"authors":null,"categories":null,"content":"Last weeks have been pretty intense. I officially left Equisense and started a joined PhD project between INRIA Lille and École Polytechnique Montreal. I had been preparing for this fresh start for several months and also wanted to evolve in my content creation process.\nA journey from plain markdown to Hugo I started writing articles to keep track of my learning paths on various topics, including numerical techniques, data analysis and programming. My first articles were either hand-written or RMarkdown-generated Markdown files on a GitHub repository.\nAs I was slowly moving from R to Python, Jupyter notebook became pretty handy to mix paragraphs, code snippets, results and charts. It also forced me to quit declaring and modifying variables, an annoying habit got from always having a REPL and text editor bundled in most scientific computing IDEs (Matlab, Scilab, RStudio, Jupyter).\nGreat, the articles were not centralized though but split into their GitHub repositories, you have to admit this is not the most user-friendly browsing experience. I found several blogs running on Jekyll and I decided to give it a try. For someone who is not fond of struggles with front-end side layout issues, this was a true gift, I could easily reuse templates from front-end developers and designers (special thanks for the awesome Gravity project) without much struggle and focus on what I liked: building projects and writing content.\nSwitching to THE writing platform I kept maintaining the Jekyll blog until almost exactly one year ago. During that time, I was mostly writing in the context of a side-project or thinking on our journey at Equisense. This raised new requirements for the writing process such as collaborative writing, review from friends, seeing the overall picture we were sending as a team from the sum of our articles.\nFor these reasons, my articles gradually switched to Medium, first published as an individual, then on the Equisense page. This was a very productive time for writing as we encouraged one another and had a direct impact on the way we presented the team, how we work and our learning path: an invaluable tool to help candidates decide whether the company was a fit for them and to ease the onboading.\nIf Medium works, why would anyone go back to writing everything from scratch? I really enjoy the writing experience on Medium, with some drawbacks. Medium’s design is very opinionated, that’s a part of what makes it a experience. However, leaving some choices on key topics is essential (at least to me) on the content-creation side. I believe this should be the case on any two-sided platform: be opinionated on the user-side, leave flexibility on the creator side.\nThe perfect example is the bright screen. It ensures the Medium experience is consistent with the unique font, background etc… But writing on a dark screen is a lot more comfortable, especially when you’re used to it or when your eyes are more light-sensitive: writing late in the evening or early in the morning was not conceivable to me on the Medium interface. The hack I used was to write everything on Atom, then paste everything to Medium once the first draft was ready, still a bit of a pain.\nThis might seem minor as a reason to switch, but the root behind it is more essential: Medium is a platform, you’re therefore a user, not an owner. Despite its global success, the company is still young and looking for the right model. Nothing tells me the model they choose tomorrow will be one I want to contribute to (how paid content will be managed for instance). Switching platforms for written content is a lot more tedious than choosing well at the beginning. This new step in my professional and personal life is the perfect occasion to rethink that commitment, I will still be re-publishing content to Medium, but not as the primary source.\n","date":1504569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"995a003b8692a04fd7cb9fc1e8fac62f","permalink":"https://matbesancon.xyz/post/2017-09-05-moving-content-hugo/","publishdate":"2017-09-05T00:00:00Z","relpermalink":"/post/2017-09-05-moving-content-hugo/","section":"post","summary":"New steps, new platform.\n","tags":["jekyll","blog","hugo","writing","medium"],"title":"Moving my content creation to a home-made Hugo site","type":"post"},{"authors":null,"categories":null,"content":"Two weeks ago, the Equisense team spent a couple days together near the ocean in northern France. The goal was to get to know each other better and sit down to (re)think on the way we work, where we are heading and try to put some words on who we are and how we plan on moving forward.\nYou named it, we tried to define one of the obsessions in the startup culture: company values. There are many pitfalls to avoid while doing so and any of them can make the company look as a cliché, under-ambitious or reject some of the stakeholders. Keep in mind that these values are publicly displayed or at least talked about, which means your employees, investors and clients could all feel disconnected from them. At best, their reaction would be “whatever, I guess everybody says so” and see you as another cliché.\nAt worst, they can reject the way you define yourself and consider the advantages of working with you are not worth it. But is the second scenario really worse? The scope of such problem is a lot wider than your external communication and the “values” tab of your website.\nUsing Paul Graham’s definition, a startup is all about growth, hence moving fast. Quick decisions and executions cannot come without breaking things and building tension on the way. Trying to define values because they make you sound confident while being agreeable by everyone cannot yield any outcome. No great result comes out of willing to please everyone, and the corollary is that every successful person and organization has haters. Given this statement, the best any entity can do is to choose who is likely not to stick with them and why.\nWhy values anyway? This question is totally legit, especially in countries like France where the startup culture is still not familiar to a significant part of the population or like Germany where pragmatism is king in both what you do and how you communicate it. Traditional organizations have always had a simple deal on the recruitment side: lend me your skills and I’ll pay you decently. On the clients’ side, the “values” displayed were traditionally associated with product differentiation (“at XYZ, our obsession is to bring the best product to [insert target] at unbeatable prices”). The shareholders’ side did not even need big words, a couple KPIs from the last quarter would do. As Nicolas Colin phrases in several Medium posts, keeping what is being said to each of these stakeholders completely separated is not possible anymore in the digital age. That’s partly why company’s culture and values became central topics.\nI’ve seen two cases at traditional companies. The first situation is ignorance or non-existence of company values, maybe some of these will sound familiar: “Values? No this is the communication staff’s business” “We’re not here to dream, we’re paid to get the job done”\nThe second attitude is picking up “values” corresponding to trends but completely disconnected from the organization’s reality. Just visit the Corporate bullshit generator and you will know what I’m referring to: We’ve all seen extremely conservative companies call themselves “innovators” or some historical monopolists call themselves “disruptive”.\nSo what changed with startups? Simple, you can redefine the way your market works (and sometimes create it in the process) and become the central element for it. That’s an idea you can genuinely believe in, work for and something you can promise employees, shareholders and clients. Now where are your values in this? Simply where you want the market to go and the path you’re ready to take to go there.\nThere are obviously other factors at stake here, the fact that millennials need to define themselves through what they do for instance and cannot just work for a decent paycheck in the same polite colleague relation for 40+ years. More than ever, our generation needs some sense in what we do at the moment and not in a 40-year projection.\nWriting down values is about choosing a path and sticking to it I’d say this is the main highlight I got from hours of thoughts about the topic of company values. Choosing values is not about sounding cool, not about the image you want for the organization’s recruitments.\nSomething concrete? Don’t define yourself as hackers if you spend at least as much time planning as actually trying and building stuff. Don’t pretend to disrupt anything if you’re playing the old rules within a known and stable market. Don’t define yourself as transparent if you’re not obsessed with everyone knowing both how you think and operate.\nDefining yourself through the most trendy terms of the year has lead to a feeling of emptiness in the messages carried in the startup ecosystems and has even become one of its clichés or even jokes.\nHow to define your company in a non-cliché way I won’t have the pretension to set the rules on this point, simply to match some great frameworks from people smarter than me who gave this point years of thoughts (conference in French by Oussama Ammar) …","date":1475798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505181809,"objectID":"79f7d2b1346de2e5935d516b80f443e1","permalink":"https://matbesancon.xyz/post/2016-10-7-company-values/","publishdate":"2016-10-07T00:00:00Z","relpermalink":"/post/2016-10-7-company-values/","section":"post","summary":"And how to avoid becoming one of the biggest clichés of startup culture\n","tags":["startup"],"title":"Company values: stop trying to dictate who you are","type":"post"},{"authors":null,"categories":null,"content":"When I came back to Equisense, I was surprised and intrigued by many things. But there was one element of the job in particular I had not planned: coming back to low level and embedded programming from higher abstractions I was used to. No OS, no libraries, no smooth write-and-test work-flow, just brutal and bare metal. I clearly needed to blow some steam off with something closer to what I usually do (or did), a data-driven and functional project using nice techs.\nWhy yet another PageRank? The time came to find a new side project and I was just finishing the lectures of Parallel Programming, which I recommend if you’re already at ease with Scala and its environment (IDEs, SBT). I wanted to apply the concepts on a project built from scratch. One day, while neglectfully scrolling through another blog post showing the basic concepts of the PageRank computation, I thought this would make a “okay” project. But wait, interesting elements here:\nThe model behind the PageRank computation is a Markov Chain, with which I have been working a lot with at Siemens. Iterating until stability of the ranks is basically a linear flow, easily performed by tail call recursion which is optimized to avoid stack-overflowing the JVM by behaving like a while loop. Computing the rank of each site is independent of the other computations, parallelizing the tasks is a piece of cake So we’re all set up for a purely functional and parallel PageRank.\nThe PageRank model We’re gonna go through the basic implementation of the algorithm. What fascinates me is the two-sided view of the algorithm: the intuitive version can be explained to a 5-year-old (or to your boss) while the maths behind it relies on the interpretation of matrix eigenvalues and on a computation of the stationary distribution of the Markov model.\nThe intuitive version Imagine you’re surfing on the web like any productive Sunday evening. On a given page, there is an equal probability to click on any link present on the page. There is also a probability that you get tired of the current series of pages and randomly go back to any page of the network.\nLet’s try to visualize the two extremes of this “random switch” usually called damping factor d. If we set d=0, the transition to any page is equally probable, since the surfer will always switch to choosing a page at random. This means that the links going out of the page they’re currently on don’t influence the probability distribution of the next page.\nOn the other end of the spectrum if the damping factor d=1, the surfer will always look for its next page in the outgoing links of her current page (this raises an issue for pages without any links). An usual value for the factor is d=0.85which keeps the probability of long sequences of related pages likely to happen, but allows for random switch.\nKey elements of the algorithm The algorithm uses the matrix of links: an entry (i,j) is 1 if there is a link on the page j to the page i and 0 otherwise (note that this notation is opposite to the common convention for Markov transition matrices, where the line is the origin state and the column the destination). The other element is a rank vector which is updated until a convergence criterion is met.\nTypes of the different structures Since we want to be able to perform some computations in parallel, most functions will manipulate Scala’s Generic data structures. Let’s start with the link matrix. It is a sparse structure: instead of representing all entries of the matrix in a vector of vectors, just non-empty elements and there corresponding column and line indexes are stored.\n// defining a dense matrix of Ints as a sequence of sequence type DenseMatrix = GenSeq[GenSeq[Int]] // SparseMatrix: tuple (line, column, value) type SparseMatrix = GenSeq[(Int,Int,Int)] However, the values of our link matrix only contains zeros and ones, so the entries present in the structure all have one as value, so we just need to keep rows and columns:\ntype LinkMat = GenSeq[(Int,Int)] The ranks are stored in a simple generic float sequence:\nR: GenSeq[Float] We also need a few utility functions. sumElements takes the matrix, the rank vector and an integer to find all links for which the outgoing page is j.\ndef sumElements(R: GenSeq[Float], A: LinkMat, j: Int): Float = { // sums all PageRanks / number of links for a column j val totalLinks = A.filter{tup =\u0026gt; tup._2 == j} if (totalLinks.isEmpty) sys.error(\u0026#34;No link in the page \u0026#34; + j + \u0026#34; at sumElements\u0026#34;) else R(j)/totalLinks.size } Note This implementation of the function is not purely functional since an imperative system error is raised if no index i is found. A better solution here would have been to wrap the value in an Option[Float], return None if no index has been found and Some(x) in case of success.\nWe also need to find all pages pointing to a given page i. This might be a bit compact, but keep in mind that the matrix is simply a pair of page indexes. So we find all pages where the first element is i …","date":1473724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532551968,"objectID":"e712cb4b7772f47874f38482b57d6fb0","permalink":"https://matbesancon.xyz/post/2016-09-13-page-rank/","publishdate":"2016-09-13T00:00:00Z","relpermalink":"/post/2016-09-13-page-rank/","section":"post","summary":"The logic and implementation of one of the first algorithms to power the modern web\n","tags":["data-science","algorithm","functional"],"title":"Functional and parallel PageRank implementation in Scala","type":"post"},{"authors":null,"categories":null,"content":"In politics, a government’s first decisions and actions are often reviewed and assessed after the famous first 100 days. According to Wikipedia, the term was coined by F. Roosevelt himself. I wanted to throw the first thoughts on my comeback at Equisense, on culture and operations and the difference between the two stages.\nStartups are in a continuous headlong rush I’ll not wait the 100 days to write this post, because in a startup things move faster, so why wait another month? At the same time you have to remain focused for a lot longer than three months or the consequences to expect are worse than skeptical articles or plunging opinion polls. Nobody could judge actions or decisions drawn after a year in business based on one or even three months.\nAnd this adrenalined marathon does not (or should not) stop with a first product release, or worse with a nice fund-raising round. This stops with the company running out of business or the market saying “I love you, let’s stay forever until death do us apart”.\nAn early insight before the long break I had already worked with the team in June, July and August 2015 during their three-founders-and-laptops stage. The focus was on first studies of acceleration signals and feasibility of some features given technological and product choices. Those first graphs and explanations thrown on a small Tex reports were later improved and re-written by the team as it designed Motion. After these few months, I left the startup for Montréal, where I spent my last semester as a student, after which I carried out my Master project in Germany. When came the time to think about what came next, we were still in contact with Camille, Idriss and Benoit and the conversation shifted to how the project had evolved and the thousand cool things coming in the next months and could become a “real job” this time. After a couple more one-to-one, a few visits in Lille and a hackathon, I was officially back on the adventure.\nGetting back and the baby is all grown up! In the meantime, Equisense launched a successful Kickstarter, reinforcing the feeling that horse-riders value what the product can bring to their experience. The company moved from its first office in Compiègne Innovation Center to the impressive Euratechnologies center in Lille.\n[1]\nMore than this, the team also got bigger to face the challenges of this new stage. The goal is no longer to hack horses acceleration signals until something comes out of it, but to build a reliable and intuitive product from the scientific findings and technological pieces brought together.\nReally getting on the market requires a deeper bond with horse-riders and as much feedback as possible. The new team embodies these changes: a diversity of mindsets, experiences and backgrounds to handle all upcoming challenges with the same care for clients. The hacker spirit isn’t gone in this new version of the company, but it isn’t the major pillar of the culture anymore. A culture is hard to put into words, but I’d say this second face of our Janus startup is a combination of a genuine care for horses wellness and of a passion for horse-riding.\n[2]\nThe fact is that while I was admiring their external successes from my Canadian home, (the Kickstarter campaign, new partnerships, features of the product getting out of the ground one by one…), the most impressive achievement was being joined by so many diverse profiles while building a working environment at the image of their horse-riders’ mindset.\nThe founders already had this care and passion in their DNA, they just succeeded in transforming it into a full culture and transmitting it into an obsession within the team.\n[2]\nWhere are we in the lifetime? From a clumsy foal to the great stallion Other than the culture, the operations changed from the clumsy foal learning to stay up on its hoofs to the stallion swiftly jumping and anticipating all obstacles. [3]\nThe seed stage is about using all assets that are or can easily be at your disposal to run tests and confirm hypotheses. Unlike lots of web startups, uncertainty does not come only from the market, but also from the technological bricks: hardware is today where the web was about ten years ago.\nMoving forwards meant iterating while building knowledge on both fields of uncertainty. This meant at the same time assessing if some measured signals could detect a jump during a training session, and if the customers actually had an interest in the feature. Working for nothing can be frustrating, but ignoring something horse-riders want would have been much worse!\n[5]\nIn the new phase, uncertainty radically changed. In the first stage, we saw how it was caused by a lack of knowledge on both the technology and the market. Of course the knowledge the team has on these topics is still not complete, but is enough to make intelligent decisions and move on. However, a new source of uncertainty has replaced the two previous:\nOrganization and processes are obvious when 4 people …","date":1470873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"c02032f6b34d7f7c4cc2a8c514b44a99","permalink":"https://matbesancon.xyz/post/2016-08-11-back-to-startup/","publishdate":"2016-08-11T00:00:00Z","relpermalink":"/post/2016-08-11-back-to-startup/","section":"post","summary":"100 days after a come-back.\n","tags":["startup","career"],"title":"Back to startup life: thoughts after the first days","type":"post"},{"authors":null,"categories":null,"content":"It’s monday morning and I’m waking up with a hangover sensation, not from partying in the streets of Lille as one could think at the beginning of the week, but from the last intense 48 hours. I took part in the so-called “creathon” organized by the Switch up challenge and MakeSense. The special touch on this event was the social impact the projects were meant to have. The result could even not be a company but a non-profit organization.\nI’m going to generalize my thoughts as much as possible, but to give a concrete view of what is going on, I’m going to use the example of Quare, the project we worked on. It is a device quantifying your stress and the quality of your current working environment thanks to different sensors and individually adjusted algorithms.\nBuilding a company from the ground up: just let me finish this Creating the whole concept for a scalable, repeatable and profitable business based on an issue you want to tackle takes time, focus and creativity. The goal is not to come up with the most spectacular way to solve the problem and build your product, but to find the simplest way allowing your business to grow without limits. You’re not building the Empire State, you’re planting an acorn to grow an oak.\nStop organizing everything, mess is a bless People coming from a business and management background tend to think of activities in terms of processes, so a structured way in which things are handled and operated. When designing the concept for a startup, just don’t. The reason is, your team is going through several critical phases where any constraint in the reasoning would hinder the ability to rephrase the problem you’re focused on, or come up with a radical new way to address it. Just let the mess happen and collectively feel when it is time to move on to a next step. If a team member feels like you missed something, simply let them gather their thoughts and arguments, then try to build on them and see how they change the concept. However, not everything must be kept if you don’t want an over-generalized problem.\nFocus is key to deliver This part is a bit tricky and you won’t find any silver bullet, even less in this blog post. By focused, understand both being focus as state-of-mind and keeping your business idea focused on what matters the most to the issue you address: this is about making real choices which will never please everyone. Keeping focused and moving fast during the week-end has two effects. If you play well, you’ll deliver a lot more by the time of the final pitch, and that’s an awesome point if you have some of the following objectives:\nConvince and impress specific people in the room: potential partners, investors, accelerators Check how comfortable team members are with each other, if you align on your ambition, your definition and perception of the problem, the way you work and react under pressure To relate things to our situation this weekend, we were an initial team of two engineers working for a middle-stage startup and two designers at ease with both product and graphical design. A highly motivated business school student joined the team a bit later in the weekend. We had both a creative and chaotic enthusiasm to explore the problems related to stress and lack of focus at work (the issue we decided to tackle) and some ability to focus and to move fast based on our diverse experiences and skills.\nWe addressed the issue of focus at work which is our direct concern while working in an open-space and dealing with creative work requiring long, uninterrupted sessions to make the best out of a day. Even though we spent a great deal of time defining the scope of the problem and primary targets, we all had this get things done attitude and were eager to start realizing something concrete from the beginning. So not only did we all have a strong link to the problem, we also had a common view on it, it allowed the team to have a sharp vision and homogeneous of work and focus.\nBut there are also several not-so-good reasons to move fast during a startup weekend:\nValidate technical choices (Should my device communicate through Wifi or Bluetooth? What information do we need to store from our clients? Any back-end related question) Precise quantitative estimates are pointless. Yup, business plans don’t have their place at a startup weekend, if they have a place at all for early stage of uncertain and radically new businesses. Try to simply get some rough ideas about the size of the problem, how much would the device cost in production. Again, to relate this to our situation, we could make a pretty good estimate of the Bill of Materials based on the sensors we wanted to use and on our experience with Equisense (what components were needed, the type of micro-controller, etc…). We could come out with different shapes for the device and test it thanks to different people giving feedback which allowed us to iterate, but that was is. We did not go any further on the economic projections, …","date":1468195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505181809,"objectID":"18e5ce04e0e549ce3d11ee554ca428f1","permalink":"https://matbesancon.xyz/post/2016-07-11-hackathon/","publishdate":"2016-07-11T00:00:00Z","relpermalink":"/post/2016-07-11-hackathon/","section":"post","summary":"Feedback after a weekend re-discovering the early stage feelings.\n","tags":["startup"],"title":"On startup weekends and getting things done","type":"post"},{"authors":null,"categories":null,"content":"The goal of this article is to present couple challenges waiting the industrial data scientist or industrial data science teams, the deep reasons I believe are the root of this inertia, based on my experience (in both data science and engineering projects) and exchanges with engineers and data scientists. The last part introduces some suggestions to make the collaboration richer for both sides.\nWhy isn’t data science already everywhere in engineering? It is surprising that this transition hasn’t been so spontaneous. Indeed, one could think that engineers, belonging to the “STEM family” (people studying or working in fields related to Science, Technology, Engineering and Mathematics) would easily embrace the concepts and methods of data science and moreover be able to identify the potential gains, savings and improvements to carry out complex projects in a more effective manner.\nSilo thinking in STEM That’s not the case, most engineers I’ve been discussing and working with never considered these techniques as relevant to their current tasks. So why so little enthusiasm? A recurrent problem I noticed is the silo thinking of disciplines created by strong and early specializations, along with natural distaste and reduction of unknown fields.\nWe’re not Google, deal with it So when someone will first pitch machine learning to an engineer, I would often observe reactions of “it’s not relevant to my field/work/issues” because they don’t consider being in a “tech” industry. This is the same reaction type observed in companies facing digital disruption (see the excellent article of Nicolas Colin here).\nAs a personal example, as I was talking to a production manager about the impact advanced predictive analytics could have on machine reliability and availability, she advanced the “non-tech” argument, to which I answered with examples of traditional manufacturing companies already using these techniques, including General Electric for turbine monitoring (what they refer to as the Industrial Internet). His last point was “Well sure but… we’re not GE”, which I understood as “I’m not able to learn from nor to work in that field totally out of my comfort zone”. Although, her discomfort with the methods involved is easily understandable since it requires key concepts in mathematics, statistics and algorithm thinking which would often be considered as theory unusable in their “real life”.\nMy subject is so complex The other reaction one would observe is linked to an interesting thinking process: People always tend to reduce the breadth of subjects they don’t know, and to emphasize (not to say oversize) the width and complexity of their own domain. I recently read a “conversation hack” to make a conversation pleasant to someone, in three steps:\nAsk them what they do for a living Ask them some more details about how they manage things Look impressed, add “Wow, that sounds very complex” People don’t feel at ease with the introduction of quantitative, rational methods and analytics for decision-making in their daily work because this implies that a rather “simple” model can generate better decisions than them. It revives this old phobia of losing their job to a machine.\nBut still… why particularly engineers? We didn’t address this question yet, and it still sounds counter-intuitive, given our first statements. From my personal experience studying and working with both junior and senior engineers, and relatively to business or social science background, there is a stronger will to “master the model” and understand most key aspects of the system they work on.\nBank managers, marketing leaders or finance analysts totally feel comfortable with the use of data base systems and business intelligence tools, even statistical analyses or predictive modeling tools they can perfectly leverage, but not often understand on the technical parts. They would just need to be able to read, use and trust the results. Engineers, on the other hand don’t feel legitimate when using tools they don’t master they feel the need of understanding and controlling what’s going on under the hood.\nThere is a common vision of the engineers in several cultures, they are the handy people, able to answer most of your questions, master all techniques from nuclear power generation to bio-technologies. They are all supposed to be Tony Stark (or Elon Musk in a more realistic way). So their secret fear is not about being afraid of getting their job “automated” but more about a situation where they cannot handle their system anymore because a part of the decisions taken is not under their control anymore.\nWhat to do about it? What data science can bring to their organization Proving the utility of data science is the easy part, the process is actually almost identical to bringing data science to any other industry. The potential users should be shown what pain points this new field would address in their business, how similar businesses have already applied machine learning …","date":1463788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557403969,"objectID":"3ad58ffddbed1f414f5b67f43a86ec22","permalink":"https://matbesancon.xyz/post/2016-05-21-bringing-data-science-engineers/","publishdate":"2016-05-21T00:00:00Z","relpermalink":"/post/2016-05-21-bringing-data-science-engineers/","section":"post","summary":"Thoughts as an engineer-by-training evolving towards data skills in a\nmanufacturing context.\n","tags":["data-science","engineering"],"title":"Bringing data science to engineers","type":"post"},{"authors":null,"categories":null,"content":"[1]\nPart III: Model development\nTo follow the following article without any trouble, I would recommend to start with the beginning.\nHow does predictive modeling work Keep the terminology in mind This is important to understand the principles and sub-disciplines of machine learning. We are trying to predict a specific output, our information of interest, which is the category of bank note we observe (genuine or forged). This task is therefore labeled as supervised learning, as opposed to unsupervised learning which consists of finding patterns or groups from data without a priori identification of those groups.\nSupervised learning can further be labeled as classification or regression, depending on the nature of the outcome, respectively categorical or numerical. It is essential to know because the two disciplines don’t involve the same models. Some models work in both cases but their expected behavior and performance would be different. In our case, the outcome is categorical with two levels.\nHow does classification work? Based on a subset of the data, we train a model, so we tune it to minimize its error on these data. To make a parallel with Object-Oriented Programming, the model is an instance of the class which defines how it works. The attributes would be its parameters and it would always have two methods (functions usable only from the object):\ntrain the model from a set of observations (composed of predictive variables and of the outcome) predict the outcome given some new observations Another optional method would be adapt which takes new training data and adjusts/corrects the parameters. A brute-force way to perform this is to call the train method on both the old and new data, but for some models a more efficient technique exists. Independent evaluation A last significant element: we mentioned using only a subset of the data to train the model. The reason is that the performance of the model has to be evaluated, but if we compute the error on the training data, the result will be biased because the model was precisely trained to minimize the error on this training set. So the evaluation has to be done on a separated subset of the data, this is called cross validation.\nOur model: logistic regression This model was chosen mostly because it is visually and intuitively easy to understand and simple to implement from scratch. Plus, it covers a central topic in data science, optimization. The underlying reasoning is the following: The logit function of the probability of a level of the classes is linearly dependent on the predictors. This can be written as:\nnp.log(p/(1-p)) = beta0 + beta[0] * x[0] + beta[1] * x[1] + ... Why do we need the logit function here? Well technically, a linear regression could be fitted with the class as output (encoded as 0/1) and the features as predictive variables. However, for some values of the predictors, the model would yield outputs below 0 or above 1. The logistic function equation yields an output between 0 and 1 and is therefore well suited to model a probability.\nYou can noticed a decision boundary, which is the limit between the region where the model yields a prediction “0” and a prediction “1”. The output of the model is a probability of the class “1”, the forged bank notes, so the decision boundary can be put at p=0.5, which would be our “best guess” for the transition between the two regions.\nRequired parameters As you noticed in the previous explanation, the model takes a vector of parameters which correspond to the weights of the different variables. The intercept \\beta_0 places the location of the point at which p=0.5, it shifts the curve to the right or the left. The coefficients of the variables correspond to the sharpness of the transition.\nLearning process Parameters identification issue Unlike linear regression, the learning process for logistic regression is not a straight-forward computation of the parameters through simple linear algebra operations. The criterion to optimize is the likelihood, or equivalently, the log-likelihood of the parameters:\nL(beta|(X,z)) = f(X,z) Parameters update The best parameters in the sense of the log-likelihood are therefore found where this function reaches its maximum. For the logistic regression problem, there is only one critical point, which is also the only maximum of the log-likelihood. So the overall process is to start from a random set of parameters and to update it in the direction that increases the log-likelihood the most. This precise direction is given by the gradient of the log-likelihood. The updated weights at each iteration can be written as:\nbeta = beta + gamma* gradient_log_likelihood(beta) Several criteria can be used to determine if a given set of parameters is an acceptable solution. A solution will be considered acceptable when the difference between two iterations is low enough.\nOptimal learning rate The coefficient gamma is called the learning rate. Higher values lead to quicker variations …","date":1452643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"3606db920cd652843023a1d68df3e5ec","permalink":"https://matbesancon.xyz/post/2016-01-13-fraud-detection3/","publishdate":"2016-01-13T00:00:00Z","relpermalink":"/post/2016-01-13-fraud-detection3/","section":"post","summary":"Learning by doing: predicting the outcome.\n","tags":["data-science","python","classification"],"title":"A Pythonic data science project: Part III","type":"post"},{"authors":null,"categories":null,"content":"[1]\nPart II: Feature engineering\nWhat is feature engineering? It could be describe as the transformation of raw data to produce a model input which will have better performance. The features are the new variables created in the process. It is often described as based on domain knowledge and more of an art than of a science. Therefore, it requires a great attention and a more “manual” process than the rest of data science projects.\nFeature engineering tends to be heavier when raw data are far from the expected input format of our learning models (images or text for instance). It can be noticed that some feature engineering was already performed on our data, since banknotes were registered as images taken from a digital camera, and we only received 5 features for each image.\nCorrelated variables Simple linear and polynomial regression We noticed some strong dependencies between variables thanks to the scatter plot. Those can deter the performance and robustness of several machine learning models. Skewness and kurtosis seem to be somehow related. A regression line can be fitted with the skewness as explanatory variable:\na, b = stats.linregress(data0[\u0026#34;skew\u0026#34;],data0[\u0026#34;kurtosis\u0026#34;])[:2] plt.plot(data0[\u0026#34;skew\u0026#34;],data0[\u0026#34;kurtosis\u0026#34;],\u0026#39;g+\u0026#39;) plt.plot(np.arange(-2.5,2.5,0.05) ,b+a*np.arange(-2.5,2.5,0.05),\u0026#39;r\u0026#39;) plt.title(\u0026#39;Simple linear regression\u0026#39;) plt.xlabel(\u0026#39;Skewness\u0026#39;) plt.ylabel(\u0026#39;Kurtosis\u0026#39;) plt.show() The following result highlights a lack in the model. The slope and intercept seem to be biased by a dense cluster of points with the skewness between 1 and 2. The points with a low skewness are under-represented in the model and do not follow the trend of the regression line. A robust regression technique could correct this bias, but a polynomial regression is the most straight-forward method to capture a higher part of the variance here. The second-degree polynomial model can be written as:\ny_hat = a*np.square(x) + b*x + c and its coefficients can be determined through the minimization of least-square error in numpy:\na, b, c = np.polyfit(data0[\u0026#34;skew\u0026#34;],data0[\u0026#34;kurtosis\u0026#34;],deg=2) plt.plot(data0[\u0026#34;skew\u0026#34;],data0[\u0026#34;kurtosis\u0026#34;],\u0026#39;+\u0026#39;) plt.plot(np.arange(-15,15,.5),a*np.arange(-15,15,.5) * np.arange(-15,15,.5)+b*np.arange(-15,15,.5)+c,\u0026#39;r\u0026#39;) plt.title(\u0026#39;2nd degree polynomial regression\u0026#39;) plt.xlabel(\u0026#39;Skewness\u0026#39;) plt.ylabel(\u0026#39;Kurtosis\u0026#39;) A polynomial regression yields a much better output with balanced residuals. The p-value for all coefficients is below the 1% confidence criterion. One strong drawback can however be noticed: the polynomial model predicts an increase in the kurtosis for skewness superior to 2, but there is no evidence for this statement in our data, so the model could lead to stronger errors.\nThe regression does not capture all the variance (and does not explain all underlying phenomena) of the Kurtosis, so a transformed variable has to be kept, which should be independent from the skewness. The most obvious value is the residual of the polynomial regression we performed.\nWe can can represent this residual versus the explanatory variable to be assured that:\nThe residuals are centered around 0 The variance of the residuals is approximately constant with the skewness There are still patterns in the Kurtosis: the residuals are not just noise p0 = plt.scatter(d0[\u0026#39;skew\u0026#39;],c+b*d0[\u0026#34;skew\u0026#34;] +a*d0[\u0026#34;skew\u0026#34;]* d0[\u0026#34;skew\u0026#34;]-d0[\u0026#34;kurtosis\u0026#34;],c=\u0026#39;b\u0026#39;,marker=\u0026#39;+\u0026#39;,label=\u0026#34;0\u0026#34;) p0 = plt.scatter(d1[\u0026#39;skew\u0026#39;],c+b*d1[\u0026#34;skew\u0026#34;] +a*d1[\u0026#34;skew\u0026#34;]* d1[\u0026#34;skew\u0026#34;]-d1[\u0026#34;kurtosis\u0026#34;],c=\u0026#39;r\u0026#39;,marker=\u0026#39;+\u0026#39;,label=\u0026#34;1\u0026#34;) plt.title(\u0026#39;Explanatory variable vs Regression residuals\u0026#39;) plt.xlabel(\u0026#39;Skewness\u0026#39;) plt.ylabel(\u0026#39;Residuals\u0026#39;) plt.legend([\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;]) plt.show() The data is now much more uncorrelated, so the feature of interest is the residual of the regression which will replace the kurtosis in the data.\nClass-dependent regression We can try and repeat the same process for the entropy and skewness, which also seem to be related to each other. p0 = plt.scatter(d0[\u0026#39;skew\u0026#39;],c+b*d0[\u0026#34;skew\u0026#34;] +a*d0[\u0026#34;skew\u0026#34;]* d0[\u0026#34;skew\u0026#34;]-d0[\u0026#34;kurtosis\u0026#34;],c=\u0026#39;b\u0026#39;,marker=\u0026#39;+\u0026#39;,label=\u0026#34;0\u0026#34;) p0 = plt.scatter(d1[\u0026#39;skew\u0026#39;],c+b*d1[\u0026#34;skew\u0026#34;] +a*d1[\u0026#34;skew\u0026#34;]* d1[\u0026#34;skew\u0026#34;]-d1[\u0026#34;kurtosis\u0026#34;],c=\u0026#39;r\u0026#39;,marker=\u0026#39;+\u0026#39;,label=\u0026#34;1\u0026#34;) plt.title(\u0026#39;Explanatory variable vs Regression residuals\u0026#39;) plt.xlabel(\u0026#39;Skewness\u0026#39;) plt.ylabel(\u0026#39;Residuals\u0026#39;) plt.legend([\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;]) plt.show() plt.plot(d0[\u0026#34;skew\u0026#34;],d0[\u0026#34;entropy\u0026#34;],\u0026#39;+\u0026#39;,label=\u0026#34;Class 0\u0026#34;) plt.plot(d1[\u0026#34;skew\u0026#34;],d1[\u0026#34;entropy\u0026#34;],\u0026#39;r+\u0026#39;,label=\u0026#34;Class 1\u0026#34;) plt.xlabel(\u0026#34;Skewness\u0026#34;) plt.ylabel(\u0026#34;Entropy\u0026#34;) plt.grid() plt.legend() plt.show()\nWe can try can fit a 2nd-degree polynomial function:\nft = np.polyfit(data0[\u0026#34;skew\u0026#34;],data0[\u0026#34;entropy\u0026#34;],deg=2) plt.plot(d0[\u0026#34;skew\u0026#34;],d0[\u0026#34;entropy\u0026#34;],\u0026#39;+\u0026#39;,label=\u0026#34;Class 0\u0026#34;) plt.plot(d1[\u0026#34;skew\u0026#34;],d1[\u0026#34;entropy\u0026#34;],\u0026#39;r+\u0026#39;,label=\u0026#34;Class 1\u0026#34;) plt.plot(np.arange(-15,14.5,.5), ft[0]*np.arange(-15,14.5,.5)*np.arange(-15,14.5,.5)+ft[1]* np.arange(-15,14.5,.5)+ft[2],\u0026#39;-\u0026#39;,linewidth=2 , label=\u0026#34;Fitted polynom\u0026#34;) plt.xlabel(\u0026#34;Skewness\u0026#34;) plt.ylabel(\u0026#34;Entropy\u0026#34;) plt.grid() plt.legend(loc=\u0026#34;bottom center\u0026#34;) plt.show() However, it seems that the model does not fit …","date":1452556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"f5eee2f82ae9dc6d372cabbc1bff550c","permalink":"https://matbesancon.xyz/post/2016-01-12-fraud-detection2/","publishdate":"2016-01-12T00:00:00Z","relpermalink":"/post/2016-01-12-fraud-detection2/","section":"post","summary":"Learning by doing: the feature engineering step.\n","tags":["data-science","python","classification"],"title":"A Pythonic data science project: Part II","type":"post"},{"authors":null,"categories":null,"content":"A complete predictive modeling project in Python Part I: Preprocessing and exploratory analysis\nOne of the amazing things with data science is the ability to tackle complex problems involving hidden parallel phenomena interacting with each other, just from the data they produce.\nAs an example, we will use data extracted from images of forged and genuine banknotes. The distinction between the two categories would be thought to require a deep domain expertise, which limits the ability to check more than a few banknotes at a time. An automated and trustable test would be of interest for many businesses, governments and organizations.\nStarting from the data provided by H. Dörsken and Volker Lohweg, from the University of Applied Science of Ostwestfalen-Lippe, Germany on the UCI Machine Learning Repository, we will follow key steps of a data science project to build a performant, yet scalable classifier.\nThe dataset was built by applying a wavelet transform on images of banknotes to extract 4 features:\nVariance, skewness, kurtosis of the wavelet transform (respectively second, third and fourth moment of the distribution).\nEntropy of the image, which can be interpreted as the amount of information or randomness (which is represented by how different adjacent pixels are).\nYou can find further information on Wavelet on Wikipedia or ask Quora. An explanation of entropy as meant in the image processing context can be found here.\nTo get a better understanding of the way the algorithms works, the full model will be built from scratch or almost (not using a machine learning library like scikit-learn on Python or caret on R).\nBasic statistic notions (variance, linear regression) and some basic python knowledge is recommended to follow through the three articles.\nProgramming choices and libraries Language and environment Python, which is a great compromise between practicality (with handy data format and manipulation) and scalability (much easier to implement for large scale, automated computation than R, Octave or Matlab). More precisely, Python 3.5.1 with the Anaconda distribution 2.4.0, I personally use the Spyder environment but feel free to keep your favorite tools.\nLibraries Collections (built-in) for occurrence counting numpy 1.10.1, providing key data format, mathematical manipulation techniques. scipy 0.16.0, imported here for the distance matrix computation and the stat submodule for Quantile-Quantile plots. pandas 0.17.1 for advanced data format, high-level manipulation and visualization pyplot from matplotlib 1.5.0 for basic visualization ggplot 0.6.8, which I think is a much improved way to visualize data urllib3 to parse the data directly from the repository (no manual download) So our first lines of code (once you placed your data in the proper repository) should look like this:\nimport numpy as np import pandas as pd import ggplot from matplotlib import pyplot as plt import scipy.stats as stats import scipy.spatial.distance from collections import Counter import urllib3 Source files The source files will be available on the corresponding Github repository. These include:\npreprocess.py to load the data and libraries exploratory.py for preliminary visualization feature_eng.py where the data will be transformed to boost the model performance model_GLM.py where we define key functions and build our model model.py where we will visualize characteristics of the model Dataset overview and exploratory analysis Understanding intuitive phenomena in the data and test its underlying structure are the objectives for this first (usually long) phase of a data science project, especially if you were not involved in the data collection process.\nData parsing Instead of manually downloading the data and placing it in our project repository, we will download using the urllib3 library.\nurl = \u0026#34;https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\u0026#34; http = urllib3.PoolManager() r = http.request(\u0026#39;GET\u0026#39;,url) with open(\u0026#34;data_banknote_authentication.txt\u0026#34;,\u0026#39;wb\u0026#39;) as f: f.write(r.data) r.release_conn() data0 = pd.read_csv(\u0026#34;data_banknote_authentication.txt\u0026#34;, names=[\u0026#34;vari\u0026#34;,\u0026#34;skew\u0026#34;,\u0026#34;kurtosis\u0026#34;,\u0026#34;entropy\u0026#34;,\u0026#34;class\u0026#34;]) Key statistics and overview Since the data were loaded using pandas, key methods of the DataFrame object can be used to find some key information in the data.\ndata0.describe() vari skew kurtosis entropy class count 1372.000000 1372.000000 1372.000000 1372.000000 1372.000000 mean 0.433735 1.922353 1.397627 -1.191657 0.444606 std 2.842763 5.869047 4.310030 2.101013 0.497103 min -7.042100 -13.773100 -5.286100 -8.548200 0.000000 25% -1.773000 -1.708200 -1.574975 -2.413450 0.000000 50% 0.496180 2.319650 0.616630 -0.586650 0.000000 75% 2.821475 6.814625 3.179250 0.394810 1.000000 max 6.824800 12.951600 17.927400 2.449500 1.000000 Negative values can be noticed in the variance and entropy, whereas it is theoretically impossible, so it can be deduced that some preprocessing operations were already …","date":1452470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697888534,"objectID":"0b5db556eb4114109debcb050cd6f379","permalink":"https://matbesancon.xyz/post/2016-01-11-fraud-detection/","publishdate":"2016-01-11T00:00:00Z","relpermalink":"/post/2016-01-11-fraud-detection/","section":"post","summary":"Learning by doing: detecting fraud on bank notes using Python in 3 steps.\r\n","tags":["data-science","python","classification"],"title":"A Pythonic data science project: Part I","type":"post"}]