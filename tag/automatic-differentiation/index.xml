<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>automatic-differentiation | μβ</title>
    <link>https://matbesancon.xyz/tag/automatic-differentiation/</link>
      <atom:link href="https://matbesancon.xyz/tag/automatic-differentiation/index.xml" rel="self" type="application/rss+xml" />
    <description>automatic-differentiation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 24 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matbesancon.xyz/media/icon_hu7565e292f0a230f950fabd03a1d7dda9_12642_512x512_fill_lanczos_center_3.png</url>
      <title>automatic-differentiation</title>
      <link>https://matbesancon.xyz/tag/automatic-differentiation/</link>
    </image>
    
    <item>
      <title>Sets, chains and rules - part II</title>
      <link>https://matbesancon.xyz/post/2020-12-24-chains_sets2/</link>
      <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://matbesancon.xyz/post/2020-12-24-chains_sets2/</guid>
      <description>&lt;p&gt;In a previous post, I detailed some of the features of
&lt;a href=&#34;https://github.com/matbesancon/MathOptSetDistances.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MathOptSetDistances.jl&lt;/a&gt;
and the evolution of the idea behind it. This is part II focusing on derivatives.&lt;/p&gt;


&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&#34;#meet-chainrulesjl&#34;&gt;Meet ChainRules.jl&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#projection-derivative&#34;&gt;Projection derivative&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#example-on-the-nonnegative-orthant&#34;&gt;Example on the nonnegative orthant&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#forward-rule&#34;&gt;Forward rule&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#reverse-rules&#34;&gt;Reverse rules&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#notes&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;The most interesting part of the packages is the projection onto a set.
For some applications, what we need is not only the projection but also the
&lt;strong&gt;derivative&lt;/strong&gt; of this projection.&lt;/p&gt;
&lt;p&gt;One answer here would be to let Automatic Differentiation (AD) do the work.
However:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Just like there are closed-form expressions for the projection, many sets admit closed-form projection derivatives that can be computed cheaply,&lt;/li&gt;
&lt;li&gt;Some projections may require to perform steps impossible or expensive with AD, as a root-finding procedure&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; or an eigendecomposition&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;;&lt;/li&gt;
&lt;li&gt;Some functions might make calls into deeper water. JuMP for instance supports a lot of optimization solvers implemented in C and called as shared libraries. AD will not propagate through these calls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For these reasons, AD systems often let users implement some derivatives themselves,
but as a library developer, I do not want to depend on a full AD package
(and force downstream users to do so).&lt;/p&gt;
&lt;h1 id=&#34;meet-chainrulesjl&#34;&gt;Meet ChainRules.jl&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JuliaDiff/ChainRules.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChainRules.jl&lt;/a&gt; is a Julia package
addressing exactly the issue mentioned above: it defines a set of primitives
to talk about derivatives in Julia.
Library developers can implement custom derivatives for their own functions and types.
Finally, AD library developers can leverage ChainRules.jl to obtain derivatives
from functions when available, and otherwise use AD mechanisms to obtain them from
more elementary functions.&lt;/p&gt;
&lt;p&gt;The logic and motivation is explained in more details in &lt;a href=&#34;https://www.youtube.com/watch?v=B4NfkkkJ7rs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frame&amp;rsquo;s talk&lt;/a&gt;
at JuliaCon 2020 and the package &lt;a href=&#34;https://www.juliadiff.org/ChainRulesCore.jl/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;
which is very instructive on AD in general.&lt;/p&gt;
&lt;h1 id=&#34;projection-derivative&#34;&gt;Projection derivative&lt;/h1&gt;
&lt;p&gt;We are interested in computing
$D\Pi_{\mathcal{S}}(v)$, the derivative of the projection with respect to the
initial point. As a refresher, if $\Pi_s(\cdot)$ is a function from $V$ onto itself,
and if $V$ then the derivative $D\Pi$ maps a point in $V$ onto a linear map
from the &lt;em&gt;tangent space&lt;/em&gt; of $V$ onto itself.
The tangent space of $V$ is roughly speaking the space where differences of
values in $V$ live. If $V$ corresponds to real numbers, then the tangent space
will also be real numbers, but if $V$ is a space of time/dates, then the tangent
space is a duration/time period. See here&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; for more references.
Again, roughly speaking, this linear map takes perturbations of the input $\Delta v$
and maps them to perturbation of the projected point $\Delta v_p$.&lt;/p&gt;
&lt;p&gt;As an example warm-up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is the whole domain of $v$ $\Rightarrow$ the projection is $v$ itself, $D\Pi_{\mathcal{S}}(v)$ is the identity operator.&lt;/li&gt;
&lt;li&gt;$S$ is $\{0\}^n$ $\Rightarrow$ the projection is always $\{0\}^n$, $D\Pi_{\mathcal{S}}(v)$ maps every $Δv$ to a zero vector: perturbations in the input do not change the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$D\Pi_{\mathcal{S}}(v)$ is a linear map from $\mathcal{V}$ to $\mathcal{V}$.
If $v \in \mathbb{R}^n$, it can be represented as a
$n\times n$ matrix.
There are several ways of representing linear maps, see the &lt;a href=&#34;https://github.com/JuliaSmoothOptimizers/LinearOperators.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinearOperators.jl&lt;/a&gt;
package for some insight. Two approaches (for now) are implemented for set distances:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Matrix approach&lt;/strong&gt;: given $v \in \mathbb{R}^n$, return the linear operator as an $n\times n$ matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward mode&lt;/strong&gt;: given $v$ and a direction $\Delta v$, provide the directional derivative $D\Pi_{\mathcal{S}}(v) \Delta v$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reverse mode&lt;/strong&gt;: given $v$, provide a closure corresponding to the adjoint of the derivative.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(1) has been implemented by &lt;a href=&#34;https://github.com/AKS1996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Akshay&lt;/a&gt; for many sets
during his GSoC this summer, along with the projections themselves.&lt;/p&gt;
&lt;p&gt;(1) corresponds to computing the derivative eagerly as a full matrix, thus
paying storage and computation cost upfront. The advantage is the simplicity for standard vectors,
take &lt;code&gt;v, s&lt;/code&gt;, build and return the matrix.
(2) is the building block for forward-mode differentiation:
given a point $v$ and an input perturbation $\Delta v$, compute the output perturbation.
(3) corresponds to a building block for reverse-mode differentiation.
An aspect of the matrix approach is that it works well for 1-D arrays
but gets complex quite quickly for other structures, including multi-argument
functions or matrices. Concatenating everything into a vector is too rigid.&lt;/p&gt;
&lt;h1 id=&#34;example-on-the-nonnegative-orthant&#34;&gt;Example on the nonnegative orthant&lt;/h1&gt;
&lt;p&gt;The nonnegative orthant cone is the set $\mathbb{R}^n_+$; it is represented in MOI
as &lt;code&gt;MOI.Nonnegatives(n)&lt;/code&gt; with &lt;code&gt;n&lt;/code&gt; the dimension.
The projection is simple because it can be done elementwise:
$$
(\Pi_S(v))_i = max(v_i, 0) \,\,\forall i.
$$&lt;/p&gt;
&lt;p&gt;In other terms, any non-diagonal term of the gradient matrix is 0 for any $v$.
Here is a visualization made with haste for $n=2$ using the very promising &lt;a href=&#34;https://github.com/Wikunia/Javis.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Javis.jl&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://matbesancon.xyz/img/posts/projections/projection.gif&#34; alt=&#34;Projection&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The red circle is a vector in the plane and the blue square its projection.&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The Julia implementation follows the same idea, here in a simplified version:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; projection_on_set(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;MOI&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Nonnegatives) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(v, zero(T))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For each component $i \in 1..n$, there are two cases to compute its derivative, either
the constraint is active or not.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
v_i &amp;lt; 0 &amp;amp; \Rightarrow \frac{\partial \Pi_i}{\partial v_i}(v) = 0\\
v_i &amp;gt; 0 &amp;amp; \Rightarrow \frac{\partial \Pi_i}{\partial v_i}(v) = 1.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The projection is not differentiable on points where one of the components is 0.
The convention usually taken is to return any quantity on such point
(to the best of my knowledge, no system guarantees a subgradient).
The Julia implementation holds on two lines:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; projection_gradient_on_set(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}, &lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;MOI&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Nonnegatives) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (sign&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(v) &lt;span style=&#34;color:#f92672&#34;&gt;.+&lt;/span&gt; one(T)) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; LinearAlgebra&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Diagonal(y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;First the diagonal of the matrix is computed using broadcasting and the sign function.
Then a &lt;code&gt;LinearAlgebra.Diagonal&lt;/code&gt; matrix is constructed. This matrix type is sparsity-aware,
in the sense that it encodes the information of having only non-zero entries on
the diagonal. We save on space, using $O(n)$ memory instead of $O(n^2)$ for a
full matrix, and can benefit from specialized methods down the line.&lt;/p&gt;
&lt;p&gt;We implemented the matrix approach from scratch. Even though we materialize the
derivative as a diagonal matrix, it still costs storage, which will become a
burden when we compose this projection with other functions and compute derivatives
on the composition.&lt;/p&gt;
&lt;h1 id=&#34;forward-rule&#34;&gt;Forward rule&lt;/h1&gt;
&lt;p&gt;For a function &lt;code&gt;f&lt;/code&gt;, value &lt;code&gt;v&lt;/code&gt; and tangent &lt;code&gt;Δv&lt;/code&gt;, the forward rule, or &lt;code&gt;frule&lt;/code&gt;
in ChainRules.jl does two things at once:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute the function value &lt;code&gt;y = f(v)&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;Compute the directional derivative &lt;code&gt;∂y = Df(v) Δv&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The motivation for computing the two values at once is detailed in the
&lt;a href=&#34;https://www.juliadiff.org/ChainRulesCore.jl/v0.9/#frule-and-rrule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.
Quite often, computing the derivative will require computing &lt;code&gt;f(v)&lt;/code&gt; itself
so it is likely to be interesting to return it anyway instead of forcing the user
to call the function again.&lt;/p&gt;
&lt;p&gt;The exact signature of &lt;code&gt;ChainRulesCore.frule&lt;/code&gt; involves some details we want to
ignore for now, but the essence is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; frule((Δself, v&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;), &lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;typeof(f), v&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;; kwargs&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y, ∂y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;∂Y&lt;/code&gt; is the directional derivative using the direction &lt;code&gt;Δx&lt;/code&gt;. Note here the variadic
&lt;code&gt;Δx&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;, since we do not want to impose a rigid, single-argument structure
to functions. The &lt;code&gt;Δself&lt;/code&gt; argument is out of scope for this post but you can read
on its use &lt;a href=&#34;https://www.juliadiff.org/ChainRulesCore.jl/v0.9/#Self-derivative-%ce%94self,-self,-self,-%e1%b9%a1elf-etc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our set projection, it may look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; ChainRulesCore&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;frule(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (_, Δv, _),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;typeof(projection_on_set),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;MOI&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Nonnegatives) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    vproj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; projection_on_set(v, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ∂vproj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Δv &lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt; (v &lt;span style=&#34;color:#f92672&#34;&gt;.&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vproj, ∂vproj
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The last computation line leverages broadcast to express elementwise the
multiplication of &lt;code&gt;Δv&lt;/code&gt; with the indicator of &lt;code&gt;v[i]&lt;/code&gt; being nonnegative.
The important thing to note here is that we never build the derivative as a data
structure. Instead, we implement it as a function. An equivalent using our
&lt;code&gt;projection_gradient_on_set&lt;/code&gt; would be:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; projection_directional_derivative(v, Δv, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    vproj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; projection_on_set(v, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    DΠ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; projection_gradient_on_set(v, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ∂vproj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DΠ &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; Δv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vproj, ∂vproj
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice the additional allocation and matrix-vector product.&lt;/p&gt;
&lt;h1 id=&#34;reverse-rules&#34;&gt;Reverse rules&lt;/h1&gt;
&lt;p&gt;The forward mode is fairly intuitive, the backward mode less so.
The motivation for using it, and the reason it is the favoured one for several
important fields using AD, is that it can differentiate a composition of functions
with only matrix-vector products, instead of requiring matrix-matrix products.
What it computed is, given a perturbation in the output (or &lt;em&gt;seed&lt;/em&gt;), provide the
corresponding perturbation in the input.
There are great resources online which will explain it in better terms than I could
so we will leave it at that.&lt;/p&gt;
&lt;p&gt;Looking at the &lt;code&gt;rrule&lt;/code&gt; signature from ChainRules.jl:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;function rrule(::typeof(f), x...; kwargs...)
    y = f(x...)
    function pullback_f(Δy)
        # implement the pullback here
        return ∂self, ∂x
    end
    return y, pullback_f
end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is a bit denser. &lt;code&gt;rrule&lt;/code&gt; takes the function as input and its arguments.
So far so good. It returns two things, the value &lt;code&gt;y&lt;/code&gt; of the function, similalry to &lt;code&gt;frule&lt;/code&gt;
and a &lt;em&gt;pullback&lt;/em&gt;. This term comes from differential geometry and in the context
of AD, is also referred to as a backpropagator. Again, the ChainRules
&lt;a href=&#34;https://www.juliadiff.org/ChainRulesCore.jl/dev/#The-propagators:-pushforward-and-pullback&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docs&lt;/a&gt;
got your back with great explanations.&lt;/p&gt;
&lt;p&gt;It also corresponds to the Jacobian-transpose vector product if you prefer the term.
In the body of &lt;code&gt;pullback_f&lt;/code&gt;, we compute the variation of the output with respect to each input.
If we give the pullback a 1 or 1-like as input, we compute the gradient,
the partial derivative of &lt;code&gt;f&lt;/code&gt; with respect to each input &lt;code&gt;x[i]&lt;/code&gt; evaluated at the
point &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is the result for our positive orthant (again, simplified for conciseness):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; ChainRulesCore&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rrule(&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;typeof(projection_on_set), v, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;MOI&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Nonnegatives)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    vproj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; projection_on_set(v, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; pullback(Δvproj)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; length(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v̄ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zeros(eltype(Δvproj), n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; vproj[i] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; v[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                v̄[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Δvproj[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (ChainRulesCore&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;NO_FIELDS, v̄, ChainRulesCore&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DoesNotExist())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (vproj, pullback)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first step is computing the projection, here we do not bother with saving
for loops and just call the projection function.
For each index &lt;code&gt;i&lt;/code&gt; of the vector, if the i-th projection component is equal to
the i-th initial point, $v_i$ is in the positive orthant and variations of
the output are directly equal to variations of the input. Otherwise,
this means the non-negativity constraint is tight, the projection lies on
the boundary &lt;code&gt;vproj[i] = 0&lt;/code&gt;, and output variations are not propagated to the input
since the partial derivative is zero.&lt;/p&gt;
&lt;p&gt;We see here that a tuple of 3 elements is returned. The first corresponds to
&lt;code&gt;∂self&lt;/code&gt;, out of the scope for this package. The second is the interesting one,
&lt;code&gt;v̄&lt;/code&gt;, the derivative with respect to the input point.
The last one &lt;code&gt;ChainRulesCore.DoesNotExist()&lt;/code&gt; indicates that there is no derivative
with respect to the last argument of &lt;code&gt;projection_on_set&lt;/code&gt;, namely the set &lt;code&gt;s&lt;/code&gt;.
This makes sense because there is nothing to differentiate in the set.&lt;/p&gt;
&lt;p&gt;An interesting point to notice is that the implementation, not the types defines the derivatives.
A non-trivial example would be a floating-point argument &lt;code&gt;p&lt;/code&gt; only used to extract
the sign bit. This means it would &lt;strong&gt;not&lt;/strong&gt; have a notion of local perturbation.
The type (a floating-point) would be interpreted as differentiable.
To my understanding, &lt;a href=&#34;https://github.com/tensorflow/swift/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swift for Tensorflow&lt;/a&gt; uses
a type-first approach, where types indicate what field gets differentiated.&lt;/p&gt;
&lt;p&gt;If you imagine using this in practice, in an AD library for instance,
one would first call &lt;code&gt;rrule&lt;/code&gt; forward, computing primal values and collecting the
successive pullbacks. Once we arrive at the end of our chain of functions,
we could backpropagate from $\Delta Y_{final} = 1$, walking our way back to
the primary input parameters.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This post comes after a few weeks of work on &lt;a href=&#34;https://github.com/matbesancon/MathOptSetDistances.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MathOptSetDistances.jl&lt;/a&gt;,
the package with the actual implementation of the presented features.
There is still a lot to learn and do on the topic, including solutions to more
projections and derivatives thereof, but also interesting things to build upon.
Defining derivatives and projections is after all a foundation for greater things to
happen.&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;See H. Friberg&amp;rsquo;s talk on exponential cone projection in Mosek at &lt;a href=&#34;https://docs.mosek.com/slides/2018/ismp2018/ismp-friberg.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISMP 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;An example case for the projection onto the Positive Semidefinite cone&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;If like me you haven&amp;rsquo;t spent much time lying around differential geometry books,
the &lt;a href=&#34;https://www.juliadiff.org/ChainRulesCore.jl/dev/#Differentials&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChainRules.jl&lt;/a&gt;
documentation has a great developer-oriented explanation.
For more visual explanations, Keno Fischer had a recent talk on
&lt;a href=&#34;https://www.youtube.com/watch?v=mQnSRfseu0c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the topic&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;See the source code &lt;a href=&#34;https://gist.github.com/matbesancon/80aa961e5c01fa6c426426083c684d84&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sets, chains and rules - part I</title>
      <link>https://matbesancon.xyz/post/2020-12-23-chains_sets/</link>
      <pubDate>Wed, 23 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://matbesancon.xyz/post/2020-12-23-chains_sets/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&#34;#mathoptinterface-and-the-motivation&#34;&gt;MathOptInterface and the motivation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#set-projections&#34;&gt;Set projections&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#user-defined-distance-notions&#34;&gt;User-defined distance notions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#bonus&#34;&gt;Bonus&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;In this post, I will develop the process through which the
&lt;a href=&#34;https://github.com/matbesancon/MathOptSetDistances.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MathOptSetDistances.jl&lt;/a&gt;
package has been created and evolved. In the second one, I will go over the differentiation part.&lt;/p&gt;
&lt;h1 id=&#34;mathoptinterface-and-the-motivation&#34;&gt;MathOptInterface and the motivation&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://jump.dev/MathOptInterface.jl/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MathOptInterface.jl&lt;/a&gt; or MOI
for short is a Julia package to unify &lt;em&gt;structured constrained&lt;/em&gt; optimization problems.
The abstract representation of problems MOI addresses is as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\min_{x}\,\, &amp;amp; F(x) \\\\
\text{s.t.}\,\, &amp;amp; G_k(x) \in \mathcal{S}_k \,\, \forall k \\\\
&amp;amp; x \in \mathcal{X}.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;$\mathcal{X}$ is the domain of the decision variables,
$F$ is the objective function, mapping values of the variables to the real line.
The constrained aspect comes from the constraints $G_k(x) \in \mathcal{S}_k$,
some mappings of the variables $G_k$ have to belong to a certain set $\mathcal{S}_k$.
See this &lt;a href=&#34;https://arxiv.org/abs/2002.03447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt; on MOI for more information
on this representation.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;structured&lt;/strong&gt; aspect comes from the fact that a specific form of $F$, $G$
and $\mathcal{S}$ is known in advance by the modeller. In other words, MOI
does not deal with arbitrary unknown functions or black-box sets.
For such cases, other tools are more adapted.&lt;/p&gt;
&lt;p&gt;From a given problem in this representation, two operations can be of interest
within a solution algorithm or from a user perspective:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given a value for $x$, evaluating a function $F(x)$ or $G(x)$,&lt;/li&gt;
&lt;li&gt;Given a value $v$ in the co-domain of $G_k$, asserting whether $v \in S_k$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first point is addressed by the function &lt;code&gt;eval_variables&lt;/code&gt; in the &lt;code&gt;MOI.Utilities&lt;/code&gt; submodule
(&lt;a href=&#34;https://jump.dev/MathOptInterface.jl/v0.9/apireference/#MathOptInterface.Utilities.eval_variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The second point appears as simple (or at least it did to me) but is trickier.
What tolerance should be set?
Most solvers include a numerical tolerance on constraint violations, should this
be propagated from user choices, and how?&lt;/p&gt;
&lt;p&gt;The deceivingly simple feature ended up opening one of the
&lt;a href=&#34;https://github.com/jump-dev/MathOptInterface.jl/pull/1023&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;longest discussions&lt;/a&gt;
in the MOI repository.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fairly straightforward[&amp;hellip;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Optimistic me, beginning of the PR, February 2020&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A more meaningful query for solvers is, given a value $v$, what is the
&lt;strong&gt;distance&lt;/strong&gt; from $v$ to the set $\mathcal{S}$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
(\text{δ(v, s)})\,\,\min_{v_p}\,\, &amp;amp; \text{dist}(v_p, v) \\\\
\text{s.t.}\,\, &amp;amp; v_p \in \mathcal{S}.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The optimal value of the problem above noted $δ(v, s)$ depends on the
notion of the distance taken between two values in the domain $\mathcal{V}$,
noted $dist(\cdot,\cdot)$ here.
In terms of implementation, the signature is roughly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;distance_to_set(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;V&lt;/span&gt;, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;S&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Real&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;Aside:&lt;/em&gt;
this is an example where multiple dispatch brings great value to the design:
the implementation of &lt;code&gt;distance_to_set&lt;/code&gt; depends on both the value type &lt;code&gt;V&lt;/code&gt;
and the type of set &lt;code&gt;S&lt;/code&gt;. See why it&amp;rsquo;s useful in the
&lt;a href=&#34;# Bonus&#34;&gt;Bonus section&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If $\mathcal{S}$ was a generic set, computing this distance would be as hard as
solving an optimization problem with constraints $v \in \mathcal{S}$ but
since we are dealing with structured optimization, many particular sets have
closed-form solutions for the problem above.&lt;/p&gt;
&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;
&lt;p&gt;$\|\cdot\|$ will denote the $l_2-$norm if not specified.&lt;/p&gt;
&lt;p&gt;The distance computation problem defined by the following data:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; v \in \mathcal{V} = \mathbb{R}^n,\\
&amp;amp; \mathcal{S} = \mathbb{Z}^n,\\
&amp;amp; dist(a, b) = \|a - b\|
\end{align}
$$&lt;/p&gt;
&lt;p&gt;consists of rounding element-wise to the closest integer.&lt;/p&gt;
&lt;p&gt;The following data:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; v \in \mathcal{V} = \mathbb{R}^n,\\
&amp;amp; \mathcal{S} = \mathbb{R}^n_+,\\
&amp;amp; dist(a, b) = \|a - b\|
\end{align}
$$&lt;/p&gt;
&lt;p&gt;find the closest point in the positive orthant, with a result:&lt;/p&gt;
&lt;p&gt;$$
v_{p}\left[i\right] = \text{max}(v\left[i\right], 0) \,\, \forall i \in \{1..n\}.
$$&lt;/p&gt;
&lt;h1 id=&#34;set-projections&#34;&gt;Set projections&lt;/h1&gt;
&lt;p&gt;The distance from a point to a set tells us how far a given candidate is from
respecting a constraint. But for many algorithms, the quantity of interest is
the projection itself:&lt;/p&gt;
&lt;p&gt;$$
\Pi_{\mathcal{S}}(v) \equiv \text{arg} \min_{v_p \in \mathcal{S}}  \text{dist}(v, v_p).
$$&lt;/p&gt;
&lt;p&gt;Like the optimal distance, the best projection onto a set can often be defined
in closed form i.e. without using generic optimization methods.&lt;/p&gt;
&lt;p&gt;We also keep the convention that the projection of a point already in the set is
always itself:
$$
δ(v, \mathcal{S}) = 0 \,\, \Leftrightarrow \,\, v \in \mathcal{S} \,\, \Leftrightarrow \,\, \Pi_{\mathcal{S}}(v) = v.
$$&lt;/p&gt;
&lt;p&gt;The interesting thing about projections is that once obtained, a distance
can be computed easily, although only computing the distance can be slightly
more efficient, since we do not need to allocate the projected point.&lt;/p&gt;
&lt;h1 id=&#34;user-defined-distance-notions&#34;&gt;User-defined distance notions&lt;/h1&gt;
&lt;p&gt;Imagine a set defined using two functions:
$$
\begin{align}
\mathcal{S} = \{v \in \mathcal{V}\,|\, f(v) \leq 0, g(v)\leq 0 \}.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The distance must be evaluated with respect to two values:
$$
(max(f(v), 0), max(g(v), 0)).
$$&lt;/p&gt;
&lt;p&gt;Here, the choice boils down to a norm, but hard-coding it seems harsh and rigid for users.
Even if we plan correctly and add most norms people would expect, someone will
end up with new exotic problems on &lt;a href=&#34;https://github.com/blegat/SetProg.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sets&lt;/a&gt;,
&lt;a href=&#34;https://github.com/jump-dev/ComplexOptInterface.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;complex numbers&lt;/a&gt; or function spaces.&lt;/p&gt;
&lt;p&gt;The solution that came up after discussions is adding a type to dispatch on,
specifying the notion of distance used:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; distance_to_set(d&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;D&lt;/span&gt;, v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;V&lt;/span&gt;, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;S&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;D&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractDistance&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;V&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;S&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;MOI&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractSet&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;which can for instance encode a p-norm or anything else.
In many cases, there is no ambiguity, and the package defines &lt;code&gt;DefaultDistance()&lt;/code&gt;
exactly for this.&lt;/p&gt;
&lt;h1 id=&#34;bonus&#34;&gt;Bonus&lt;/h1&gt;
&lt;p&gt;If you are coming from a class-based object-oriented background, a common
design choice is to define a &lt;code&gt;Set&lt;/code&gt; abstract class with a method &lt;code&gt;project_on_set(v::V)&lt;/code&gt; to implement.
This would work for most situations, since a set often implies a domain &lt;code&gt;V&lt;/code&gt;.
What about the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Projecting onto the reals (no-op)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;project_on_set(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Reals&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Real&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Projecting onto the reals (actual work)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;project_on_set(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}, s&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Reals&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Complex&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Which &amp;ldquo;class&amp;rdquo; should own the implementation in that case?
From what I observed, libraries end up with either an enumeration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; typeof(v) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Reals&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;elseif&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or when the number of possible domains is expected to be low, with several methods:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# in the set class Reals&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; project_real(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Real&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; project_complex(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;AbstractVector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;}) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Complex&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; project_scalar(v&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; {&lt;span style=&#34;color:#66d9ef&#34;&gt;T&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Real&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As a last remark, one may wonder why would one define trivial sets as the &lt;code&gt;MOI.Reals&lt;/code&gt;
or the &lt;code&gt;MOI.Zeros&lt;/code&gt;. A good example where this is needed is the polyhedral cone:
$$
A x = 0
$$
with $x$ a vector. This makes more sense to define $Ax$ as the function and&lt;br&gt;
&lt;code&gt;MOI.Zeros&lt;/code&gt; as the set.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Differentiating the discrete: Automatic Differentiation meets Integer Optimization</title>
      <link>https://matbesancon.xyz/post/2020-01-23-discrete-diff/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://matbesancon.xyz/post/2020-01-23-discrete-diff/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://matbesancon.xyz/img/posts/diff_discrete/graph1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;


&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&#34;#automatic-differentiation&#34;&gt;Automatic Differentiation&lt;/a&gt;
      &lt;ol&gt;
        &lt;li&gt;&lt;a href=&#34;#automatic-differentiation-on-a-pure-julia-solver&#34;&gt;Automatic differentiation on a pure-Julia solver&lt;/a&gt;&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#example-problem-weighted-independent-set&#34;&gt;Example problem: weighted independent set&lt;/a&gt;
      &lt;ol&gt;
        &lt;li&gt;&lt;a href=&#34;#optimization-model-of-the-weighted-independent-set&#34;&gt;Optimization model of the weighted independent set&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a-julia-implementation&#34;&gt;A Julia implementation&lt;/a&gt;&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#why-not-reverse-mode&#34;&gt;Why not reverse-mode?&lt;/a&gt;
      &lt;ol&gt;
        &lt;li&gt;&lt;a href=&#34;#giving-reverse-with-zygote-a-shot&#34;&gt;Giving reverse with Zygote a shot&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#how-could-this-be-improved&#34;&gt;How could this be improved?&lt;/a&gt;&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion-speculation-prospect&#34;&gt;Conclusion, speculation, prospect&lt;/a&gt;
      &lt;ol&gt;
        &lt;li&gt;&lt;a href=&#34;#special-mentions&#34;&gt;Special mentions&lt;/a&gt;&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;In continuous convex optimization, duality is often the theoretical foundation for
computing the sensibility of the optimal value of a problem to
one of its parameters. In the non-linear domain, it is fairly standard to assume
one can compute at any point of the domain the function $f(x)$ and gradient
$\nabla f(x)$.&lt;/p&gt;
&lt;p&gt;What about discrete optimization?&lt;br&gt;
The first thought would be that differentiating
the resolution of a discrete problem does not make sense, the information it yields
since infinitesimal variations in the domain of the variables do not make sense.&lt;/p&gt;
&lt;p&gt;However, three cases come to mind for which asking for gradients makes perfect sense:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In mixed-integer linear problems, some variables take continuous values.
All linear expressions are differentiable, and every constraint coefficient,
right-hand-side and objective coefficient can have an attached partial derivative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even in pure-integer problems, the objective value will be a continuous
function of the coefficients, possibly locally smooth, for which one can get
the partial derivative associated with each weight.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We might be interested in computing the derivative of &lt;strong&gt;some&lt;/strong&gt; expression
of the variables with respect to some parameters, without this expression
being the objective.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For these points, some duality-based techniques and reformulations can be used,
sometimes very expensive when the input size grows.
One common approach is to first
solve the problem, then fixing the integer variables and re-solving the
continuous part of the problem to compute the dual values associated with
each constraint, and the reduced cost coefficients.
This leads to solving a NP-hard problem, followed by a second solution from
scratch of a linear optimization problem, still, it somehow works.&lt;/p&gt;
&lt;p&gt;More than just solving the model and computing results, one major use case
is embarking the result of an optimization problem into another more complete
program. The tricks developed above cannot be integrated with an automated way
of computing derivatives.&lt;/p&gt;
&lt;h1 id=&#34;automatic-differentiation&#34;&gt;Automatic Differentiation&lt;/h1&gt;
&lt;p&gt;Automatic Differentiation is far from new, but has known a gain in attention
in the last decade with its used in ML, increasing the usability of the available
libraries. It consists in getting an augmented information out of a function.&lt;/p&gt;
&lt;p&gt;If a function has a type signature &lt;code&gt;f: a -&amp;gt; b&lt;/code&gt;, the goal is, without modifying
the function, to compute a derivative, which is also a function, which to every
point in the domain, yields a linear map from domain to co-domain &lt;code&gt;df: a -&amp;gt; (a -o b)&lt;/code&gt;,
where &lt;code&gt;a -o b&lt;/code&gt; denotes a linear map, regardless of underlying representation (matrix, function, &amp;hellip;).
See the talk and paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; for a type-based formalism of AD if you are ok with programming language formalism.&lt;/p&gt;
&lt;h2 id=&#34;automatic-differentiation-on-a-pure-julia-solver&#34;&gt;Automatic differentiation on a pure-Julia solver&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Wikunia/ConstraintSolver.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ConstraintSolver.jl&lt;/a&gt; is a recent
project by &lt;a href=&#34;https://github.com/Wikunia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikunia&lt;/a&gt;. As the name indicates, it is a
&lt;a href=&#34;https://en.wikipedia.org/wiki/Constraint_programming&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;constraint programming&lt;/a&gt;
solver, a more Computer-Science-flavoured approach to integer optimization.
As a Julia solver, it can leverage both multiple dispatch and the type system
to benefit from some features for free. One example of such
feature is automatic differentiation: if your function is generic enough
(not relying on a specific implementation of number types, such as &lt;code&gt;Float64&lt;/code&gt;),
gradients with respect to some parameters can be computed by calling the function
just once (forward-mode automatic differentiation).&lt;/p&gt;
&lt;h1 id=&#34;example-problem-weighted-independent-set&#34;&gt;Example problem: weighted independent set&lt;/h1&gt;
&lt;p&gt;Let us consider a classical problem in combinatorial optimization, given an undirected graph
$G = (V, E)$, finding a subset of the vertices, such that no two vertices in the
subset are connected by an edge, and that the total weight of the chosen vertices
is maximized.&lt;/p&gt;
&lt;h2 id=&#34;optimization-model-of-the-weighted-independent-set&#34;&gt;Optimization model of the weighted independent set&lt;/h2&gt;
&lt;p&gt;Formulated as an optimization problem, it looks as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
(\mathcal{P}): \max_{x} &amp;amp; \sum_{i \in V} w_i x_i \\\\
\text{s.t.} \\\\
&amp;amp; x_i + x_j \leq 1 \,\, \forall (i,j) \in E \\\\
&amp;amp; x \in \mathbb{B}^{|V|}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Translated to English, this would be maximizing the weighted sum of picked
vertices, which are decisions living in the $|V|$-th dimensional binary space,
such that for each edge, no two vertices can be chosen.
The differentiable function here is the objective value of such optimization
problem, and the parameters we differentiate with respect to are the weights
attached to each vertex $w_i$. We will denote it $f(w) = \max_x (\mathcal{P}_w)$.&lt;/p&gt;
&lt;p&gt;If a vertex $i$ is not chosen in a solution, there are two cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the vertex has the same weight as at least one other, say $j$, such that
swapping $i$ and $j$ in the selected subset does not change the optimal value.
of $\mathcal{P}$.
In that case, there is a kink in the function, a discontinuity of the derivative,
which may not be computed correctly by automatic differentiation.
This is related to the phenomenon of degeneracy in the simplex algorithm,
multiple variables could be chosen equivalently to enter the base.&lt;/li&gt;
&lt;li&gt;there is no other vertex with the same weight, such that swapping the two
maintains the same objective value. In that case, the derivative is $0$,
small enough variations of the weight does not change the solution nor the objective.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If a vertex $i$ is chosen in a solution, then $x_i = 1$, and the corresponding
partial derivative of the weight is $\frac{\partial f(w)}{\partial w_i} = 1$.&lt;/p&gt;
&lt;h2 id=&#34;a-julia-implementation&#34;&gt;A Julia implementation&lt;/h2&gt;
&lt;p&gt;We will import a few packages, mostly MathOptInterface.jl (MOI), the foundation for
constrained optimization, the solver itself, the Test standard lib, and ForwardDiff.jl
for automatic differentiation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Test
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; ConstraintSolver
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; CS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ConstraintSolver
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; MathOptInterface
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; MOI &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MathOptInterface
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; ForwardDiff
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let us first write an implementation for the max-weight independent set problem.
We will use a 4-vertex graph, looking as such:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://matbesancon.xyz/img/posts/diff_discrete/graph2.svg&#34; alt=&#34;Weighted graph&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The optimal answer here is to pick vertices 1 and 4 (in orange).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max independent set MOI&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Optimizer()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constrained_variable(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ZeroOne()) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, j &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; matrix[i,j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            (z, _) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constrained_variable(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;GreaterThan(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constraint(model, z, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Integer&lt;/span&gt;())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constraint(model, z, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LessThan(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineFunction(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, x[i][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, x[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, z),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                ], &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constraint(model, f, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;EqualTo(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    weights &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    terms &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(weights[i], x[i][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; eachindex(x)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    objective &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineFunction(terms, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ObjectiveFunction&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;typeof&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;objective&lt;/span&gt;)}(), objective)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ObjectiveSense(), MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MAX_SENSE)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optimize!(model)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# add some tests&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Why the additional code with&lt;code&gt;(z, _) = MOI.add_constrained_variable(model, MOI.GreaterThan(0.0))&lt;/code&gt;?
&lt;em&gt;ConstraintSolver.jl&lt;/em&gt; does not yet support constraints of the type &lt;code&gt;a x + b y &amp;lt;= c&lt;/code&gt;,
but linear equality constraints are fine, so we can derive equivalent formulations by adding a
slack variable &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this problem, the tests could be on both the solution and objective value, as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;VariablePrimal(), x[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;VariablePrimal(), x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ObjectiveValue()) &lt;span style=&#34;color:#f92672&#34;&gt;≈&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An equivalent JuMP version would look look this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Model(with_optimizer(CS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Optimizer))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;@variable&lt;/span&gt;(m, x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;], Bin)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, j &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; matrix[i,j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        zcomp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;@variable&lt;/span&gt;(m)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        JuMP&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_binary(zcomp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;@constraint&lt;/span&gt;(m, x[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; zcomp &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@objective&lt;/span&gt;(m, Max, dot(w, x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;optimize!(m)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Why are we not using JuMP, which is much more concise and closer to the
mathematical formulation?&lt;/p&gt;
&lt;p&gt;JuMP uses &lt;code&gt;Float64&lt;/code&gt; for all value types, which means we do not get the benefit of
generic types, while &lt;code&gt;MathOptInterface&lt;/code&gt; types are parameterized by the numeric type used.
To be fair, maintaining type genericity on a project as large as JuMP
is hard without making performance compromises. JuMP is not built of functions, but
of a model object which contains a mutable state of the problem being constructed,
and building an Algebraic Modelling Language without this incremental build of the
model has not proved successful till now. One day, we may get a powerful declarative
DSL for mathematical optimization, but it has not come yet.&lt;/p&gt;
&lt;p&gt;Back to our problem, we now have a way to compute the optimal value and solution.
Let us implement our function $f(w)$:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; weighted_stable_set(w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Optimizer(solution_type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Real&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constrained_variable(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ZeroOne()) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, j &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; matrix[i,j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            (z, _) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constrained_variable(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;GreaterThan(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constraint(model, z, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Integer&lt;/span&gt;())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constraint(model, z, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LessThan(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineFunction(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, x[i][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, x[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, z),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                ], &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_constraint(model, f, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;EqualTo(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    terms &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineTerm(w[i], x[i][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; eachindex(x)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    objective &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScalarAffineFunction(terms, zero(eltype(w)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ObjectiveFunction&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;typeof&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;objective&lt;/span&gt;)}(), objective)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ObjectiveSense(), MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MAX_SENSE)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optimize!(model)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(model, MOI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ObjectiveValue())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can now compute the gradient in one function call with ForwardDiff:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Differentiating stable set&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    weights &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ∇w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ForwardDiff&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gradient(weighted_stable_set, weights)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt; ∇w[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;≈&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt; ∇w[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;≈&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt; ∇w[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;≈&lt;/span&gt; ∇w[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;≈&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To understand how this derivative computation can work with just few
function calls (proportional to the size of the input), one must dig
a bit deeper in &lt;a href=&#34;https://en.wikipedia.org/wiki/Dual_number&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dual Numbers&lt;/a&gt;.
I will shamelessly refer to &lt;a href=&#34;https://matbesancon.xyz/slides/ad4dev#/12&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my slides&lt;/a&gt;
at the Lambda Lille meetup for an example implementation in Haskell.&lt;/p&gt;
&lt;h1 id=&#34;why-not-reverse-mode&#34;&gt;Why not reverse-mode?&lt;/h1&gt;
&lt;p&gt;I mentioned that the cost of computing the value &amp;amp; derivatives is proportional
to the size of the input, which can increase rapidly for real-world problems.
This is specific to so-called &lt;em&gt;forward mode&lt;/em&gt; automatic differentiation.
We will not go over the inner details of forward versus reverse.
As a rule of thumb, forward-mode has less overhead, and is better when the
dimension of the output far exceeds the dimension of the input, while
reverse-mode is better when the dimension of the input exceeds the one
of the output.&lt;/p&gt;
&lt;h2 id=&#34;giving-reverse-with-zygote-a-shot&#34;&gt;Giving reverse with Zygote a shot&lt;/h2&gt;
&lt;p&gt;Getting back to our question, the answer is rather down-to-earth,
the reverse-mode I tried simply did not work there.
Reverse-mode requires tracing the normal function call, building a
&amp;ldquo;tape&amp;rdquo;, this means that it needs a representation of the function
(as a graph or other).
I gave &lt;a href=&#34;https://github.com/FluxML/Zygote.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zygote.jl&lt;/a&gt;
a try, which can be done by replacing &lt;code&gt;ForwardDiff.gradient(f,x)&lt;/code&gt; with
&lt;code&gt;Zygote.gradient(f, x)&lt;/code&gt; in the snippet above.
Building a representation of the function means &lt;em&gt;Zygote&lt;/em&gt; must have a
representation of all operations performed. For the moment,
this is still restricted to a subset of the Julia language
(which is far more complex than commonly encountered mathematical functions
built as a single expression). This subset still excludes throwing and
handling exceptions, which is quite present in both ConstraintSolver.jl
and MathOptInterface.&lt;/p&gt;
&lt;p&gt;I have not tried the other reverse tools for the sake of conciseness (and time),
so feel free to check out &lt;a href=&#34;https://github.com/invenia/Nabla.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nabla.jl&lt;/a&gt;,
&lt;a href=&#34;https://github.com/JuliaDiff/ReverseDiff.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ReverseDiff.jl&lt;/a&gt;
and &lt;a href=&#34;https://github.com/FluxML/Tracker.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tracker.jl&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-could-this-be-improved&#34;&gt;How could this be improved?&lt;/h2&gt;
&lt;p&gt;A first solution could be to move the idiom of Julia from &lt;code&gt;throw/try/catch&lt;/code&gt;
to handling errors as values, using something like the &lt;code&gt;Result/Either&lt;/code&gt; type
in Scala / Haskell / Rust and &lt;a href=&#34;https://github.com/iamed2/ResultTypes.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;corresponding libraries&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another alternative, currently happening is to keep pushing Zygote to support
more features from Julia, going in the direction of supporting differentiation
of any program, as dynamic as it gets.&lt;/p&gt;
&lt;p&gt;One last option for the particular problem of exception handling would be
to be able to opt-out of input validation, with some &lt;code&gt;@validate expr&lt;/code&gt;,
with &lt;code&gt;expr&lt;/code&gt; potentially throwing or handling an error, and a &lt;code&gt;@nocheck&lt;/code&gt;
or &lt;code&gt;@nothrows&lt;/code&gt; macro in front of the function call, considering the function
will remain on the happy path and not guaranteeing validity or error messages
otherwise. This works exactly like the &lt;code&gt;@boundscheck&lt;/code&gt;, &lt;code&gt;@inbounds&lt;/code&gt; pair for
index validation.&lt;/p&gt;
&lt;h1 id=&#34;conclusion-speculation-prospect&#34;&gt;Conclusion, speculation, prospect&lt;/h1&gt;
&lt;p&gt;This post is already too long so we&amp;rsquo;ll stop there.
The biggest highlights here are that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In discrete problems, we also have some continuous parts.&lt;/li&gt;
&lt;li&gt;Julia&amp;rsquo;s type system allows AD to work almost out of the box in most cases.&lt;/li&gt;
&lt;li&gt;With JuMP and MOI, solving optimization problems is just another algorithmic building block in your Julia program, spitting out results, and derivatives if you make them.&lt;/li&gt;
&lt;li&gt;I believe that&amp;rsquo;s why plugging in solvers developed in C/C++ is fine, but not always what we want. I would be ready to take a performance hit on the computation time of my algorithms to have some hackable, type-generic MILP solver in pure Julia.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;special-mentions&#34;&gt;Special mentions&lt;/h2&gt;
&lt;p&gt;Thanks a lot to &lt;a href=&#34;https://github.com/Wikunia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikunia&lt;/a&gt;, first for developing ConstraintSolver.jl,
without which none of this would have been possible, and for the open discussion on the multiple
issues I posted. Don&amp;rsquo;t hesitate to check out his &lt;a href=&#34;https://opensourc.es/blog/constraint-solver-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt;,
where the whole journey from 0 to a constraint solver is documented.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://conal.net/papers/essence-of-ad/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The simple essence of automatic differentiation&lt;/a&gt;, Conal Elliott, Proceedings of the ACM on Programming Languages (ICFP), 2018&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;I believe a pure-Julia solver could be made as fast as a C/C++ solver, but developing solvers is an enormous amount of work and micro-optimizations, tests on industrial cases. The new &lt;a href=&#34;https://highs.dev&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HiGHS&lt;/a&gt; solver however shows that one can get pretty good results by developing a linear solver from scratch with all modern techniques already baked in.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
